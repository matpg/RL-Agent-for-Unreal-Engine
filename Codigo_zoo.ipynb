{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codigo zoo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpg/RL-Agent-for-Unreal-Engine-4/blob/main/Codigo_zoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy9QoDC7XA7"
      },
      "source": [
        "# RL Baselines Zoo: Training in Colab\n",
        "\n",
        "\n",
        "\n",
        "Github Repo: [https://github.com/araffin/rl-baselines-zoo](https://github.com/araffin/rl-baselines-zoo)\n",
        "\n",
        "Stable-Baselines Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
        "\n",
        "# Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXVDDlTn02M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076162e5-266e-48c4-d303-eed1af464b21"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt-get update\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg freeglut3-dev xvfb\n",
        "!pip install stable-baselines[mpi] --upgrade\n",
        "!pip install pybullet\n",
        "!pip install box2d box2d-kengz pyyaml pytablewriter optuna scikit-optimize\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [506 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.3 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,372 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,699 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,136 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [237 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,816 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [266 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,244 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [53.8 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Fetched 11.6 MB in 4s (3,058 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0 xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 1,884 kB of archives.\n",
            "After this operation, 8,093 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 1,884 kB in 1s (1,361 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines[mpi]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl (240kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.15.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.1\n",
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/d9/756b8fe29c574b34e3a60fd777688f8aaacb7eae37fcd1b5983ec415646d/pybullet-3.0.7-cp36-cp36m-manylinux1_x86_64.whl (87.5MB)\n",
            "\u001b[K     |████████████████████████████████| 87.5MB 55kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.0.7\n",
            "Collecting box2d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 6.8MB/s \n",
            "\u001b[?25hCollecting box2d-kengz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Collecting pytablewriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.6MB/s \n",
            "\u001b[?25hCollecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 26.0MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.4MB/s \n",
            "\u001b[?25hCollecting tcolorpy<1,>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/02/51/bbb0cc7f30771c285c354634bf83653a2871d58c6923bd29bfddeb9c9cb1/tcolorpy-0.0.8-py3-none-any.whl\n",
            "Collecting typepy[datetime]<2,>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/09/ba18047bc809e1622ca62c7b7848f027d7d41543098282242192e56885da/typepy-1.1.2-py3-none-any.whl\n",
            "Collecting mbstrdecoder<2,>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/57/3bb55beafe0a5e9883621f01a560d16bcef6d4f844dc2dd40caa0a8d9182/mbstrdecoder-1.0.0-py3-none-any.whl\n",
            "Collecting tabledata<2,>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/df/b2/264d9707502f0259a3eb82ec48064df98b1735d5a5f315b6a1d7105263f4/tabledata-1.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.6/dist-packages (from pytablewriter) (50.3.2)\n",
            "Collecting msgfy<1,>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl\n",
            "Collecting DataProperty<2,>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/2d/e5413965af992f4e489b6f5eebf52db9c17953c772962d1223d434b05cef/DataProperty-0.50.0-py3-none-any.whl\n",
            "Collecting pathvalidate<3,>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/99/5f/6076a8ec6cefe1b7ef725ebb32ebfa19559a5d171992153af23db6d38d9f/pathvalidate-2.3.1-py3-none-any.whl\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/3c/06c76ec8b54b9b1fad7f35e903fd25010fe3e0d41bd94cea5e6f12e0d651/cmaes-0.7.0-py3-none-any.whl\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/61/5b64d73b01c1218f55c894b5ec0fb89b32c6960b7f7b3ad9f5ac0c373b9d/cliff-3.5.0-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.7)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c8/c16d30bbed11a1722060014c246d124582d1f781b26f5859d8dacc3e08e1/colorlog-4.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 31.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.17.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.20)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2018.9; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.4 in /usr/local/lib/python3.6/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter) (3.0.4)\n",
            "Collecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/54/af6e2703f064485d717cb311d3f9440cd302a823ba6d80a020b59eae166d/cmd2-1.4.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 30.1MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 34.2MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from stevedore>=2.0.1->cliff->optuna) (3.1.1)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=2.0.1->cliff->optuna) (3.4.0)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp36-none-any.whl size=359761 sha256=d28cfff5fac377dd56a943d7a56a17ea195a827890ca47a34121764904425573\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: box2d-kengz, PrettyTable, pyperclip\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp36-cp36m-linux_x86_64.whl size=2034945 sha256=6fc09878b160159d32605222ebe0facab586c366bd9578338e671c9367ee1815\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-cp36-none-any.whl size=13700 sha256=db2ed0ef3f902d1aced430de553620192aa564353ccd55a1699c1518f12f76d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11119 sha256=5abcfc2ec3e43a86db53e026d5c1a70cec00c581d9e668d25c522360ae476109\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built box2d-kengz PrettyTable pyperclip\n",
            "Installing collected packages: box2d, box2d-kengz, tcolorpy, mbstrdecoder, typepy, DataProperty, tabledata, msgfy, pathvalidate, pytablewriter, cmaes, PrettyTable, pbr, stevedore, colorama, pyperclip, cmd2, cliff, colorlog, Mako, python-editor, alembic, optuna, pyaml, scikit-optimize\n",
            "  Found existing installation: prettytable 2.0.0\n",
            "    Uninstalling prettytable-2.0.0:\n",
            "      Successfully uninstalled prettytable-2.0.0\n",
            "Successfully installed DataProperty-0.50.0 Mako-1.1.3 PrettyTable-0.7.2 alembic-1.4.3 box2d-2.3.10 box2d-kengz-2.3.3 cliff-3.5.0 cmaes-0.7.0 cmd2-1.4.0 colorama-0.4.4 colorlog-4.6.2 mbstrdecoder-1.0.0 msgfy-0.1.0 optuna-2.3.0 pathvalidate-2.3.1 pbr-5.5.1 pyaml-20.4.0 pyperclip-1.8.1 pytablewriter-0.58.0 python-editor-1.0.4 scikit-optimize-0.8.1 stevedore-3.3.0 tabledata-1.1.3 tcolorpy-0.0.8 typepy-1.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjF3qRg7oGH"
      },
      "source": [
        "## Clone RL Baselines Zoo Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjGikdT1DFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f227d338-3a89-4f6a-af2b-3c684f0bed97"
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines-zoo'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 1840 (delta 20), reused 26 (delta 12), pack-reused 1796\u001b[K\n",
            "Receiving objects: 100% (1840/1840), 375.67 MiB | 39.53 MiB/s, done.\n",
            "Resolving deltas: 100% (1085/1085), done.\n",
            "Checking out files: 100% (333/333), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REMQlh-ezyVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97847e6-0e45-4db3-b2b1-3b01a1ebec80"
      },
      "source": [
        "cd rl-baselines-zoo/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rl-baselines-zoo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSTHgpbhb1op",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c304d417-05a2-4178-cf04-30801e0687ee"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/rl-baselines-zoo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ-pAbF7zRZ"
      },
      "source": [
        "## Train an RL Agent\n",
        "\n",
        "\n",
        "The train agent can be found in the `logs/` folder.\n",
        "\n",
        "Here we will train A2C on CartPole-v1 environment for 100 000 steps. \n",
        "\n",
        "\n",
        "To train it on Pong (Atari), you just have to pass `--env PongNoFrameskip-v4`\n",
        "\n",
        "Note: You need to update `hyperparams/algo.yml` to support new environments. You can access it in the side panel of Google Colab. (see https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34lXM8ZfMQfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e51c439-5ce2-491c-bad9-2668ae9db068"
      },
      "source": [
        "!pip install gym-minigrid\n",
        "# go to gym-minigrid in \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs\" and replace the modeled crossing env"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-minigrid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/57/15171eff6222dd012cc89c001f5c50ad9e11b8cef385873db3b4c0d89aff/gym_minigrid-1.0.1-py3-none-any.whl (47kB)\n",
            "\r\u001b[K     |███████                         | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 20kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 30kB 13.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 40kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.9.6->gym-minigrid) (0.16.0)\n",
            "Installing collected packages: gym-minigrid\n",
            "Successfully installed gym-minigrid-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmsGh9A8p4NO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c8eb1c2-5aaa-4bf7-ab74-9fa4e7754da3"
      },
      "source": [
        "# MONTANDO FOLDER COMPARTIDO DE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npzEtYM3Ovsm"
      },
      "source": [
        "# moving the mod crossing file (U MAZE MODELED) to the destination path\r\n",
        "%rm \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs/crossing.py\"\r\n",
        "%cp \"/content/drive/MyDrive/Colab Notebooks/modelo ts agentnet/ajustes de entorno y modelo/crossing.py\" \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs\"\r\n",
        "\r\n",
        "#SIMULTANEAMENTE SE DEBE REEMPLAZAR LA INFORMACIÓN DEL ENTORNO EN LOS HIPERPARAMETROS PPO2 DEL PAQUETE ZOO\r\n",
        "%rm \"/content/rl-baselines-zoo/hyperparams/ppo2.yml\"\r\n",
        "%cp \"/content/drive/MyDrive/Colab Notebooks/modelo ts agentnet/ajustes de entorno y modelo/ppo2.yml\" \"/content/rl-baselines-zoo/hyperparams\"\r\n",
        "\r\n",
        "#tambien, actualizar el archivo train.py con cambios en los env wrappers.\r\n",
        "%rm \"/content/rl-baselines-zoo/train.py\"\r\n",
        "%cp \"/content/drive/MyDrive/Colab Notebooks/modelo ts agentnet/ajustes de entorno y modelo/train.py\" \"/content/rl-baselines-zoo\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sC22eGHTH-"
      },
      "source": [
        "#ENTRENAMIENTO PARA LA RED CON ALGORITMO PPO2 PARA EL ENTORNO DE PUZZLES CLASICOS\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --gym-packages gym_minigrid\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bIR_N7R11XI"
      },
      "source": [
        "#ENTRENAMIENTO PARA LA RED CON ALGORITMO PPO2 PARA EL ENTORNO DE PUZZLES DE UE4\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingEnvUmaze-v0 --gym-packages gym_minigrid\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1wkomTewzb"
      },
      "source": [
        "#ENTRENAMIENTO PARA LA RED CON ALGORITMO PPO2 PARA EL ENTORNO GRANDES DE PUZZLES CLASICOS\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingS11N5-v0 --gym-packages gym_minigrid\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiVaxzni5Vx3",
        "outputId": "f83656f2-7bb4-46f8-ce49-0d43e6d4e650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#ENTRENAMIENTO PARA LA RED CON ALGORITMO PPO2 PARA EL ENTORNO GRANDE DE PUZZLES DE UE4\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N3-v0 --gym-packages gym_minigrid\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03056475   |\n",
            "| clipfrac           | 0.12036133   |\n",
            "| ep_len_mean        | 19.2         |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.849        |\n",
            "| fps                | 404          |\n",
            "| n_updates          | 418          |\n",
            "| policy_entropy     | 0.25692943   |\n",
            "| policy_loss        | -0.022709994 |\n",
            "| serial_timesteps   | 214016       |\n",
            "| time_elapsed       | 4.71e+03     |\n",
            "| total_timesteps    | 1712128      |\n",
            "| value_loss         | 0.14205325   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.9327175    |\n",
            "| clipfrac           | 0.23791504   |\n",
            "| ep_len_mean        | 22.6         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.905        |\n",
            "| fps                | 420          |\n",
            "| n_updates          | 419          |\n",
            "| policy_entropy     | 0.45456535   |\n",
            "| policy_loss        | -0.053295087 |\n",
            "| serial_timesteps   | 214528       |\n",
            "| time_elapsed       | 4.72e+03     |\n",
            "| total_timesteps    | 1716224      |\n",
            "| value_loss         | 0.15245739   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1720000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.60 +/- 122.71\n",
            "-------------------------------------\n",
            "| approxkl           | 0.10566926   |\n",
            "| clipfrac           | 0.17651367   |\n",
            "| ep_len_mean        | 20.7         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.952        |\n",
            "| fps                | 362          |\n",
            "| n_updates          | 420          |\n",
            "| policy_entropy     | 0.47731528   |\n",
            "| policy_loss        | -0.031631704 |\n",
            "| serial_timesteps   | 215040       |\n",
            "| time_elapsed       | 4.73e+03     |\n",
            "| total_timesteps    | 1720320      |\n",
            "| value_loss         | 0.10141665   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.047554564  |\n",
            "| clipfrac           | 0.09450684   |\n",
            "| ep_len_mean        | 17.8         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.936        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 421          |\n",
            "| policy_entropy     | 0.18669184   |\n",
            "| policy_loss        | -0.023143256 |\n",
            "| serial_timesteps   | 215552       |\n",
            "| time_elapsed       | 4.74e+03     |\n",
            "| total_timesteps    | 1724416      |\n",
            "| value_loss         | 0.058861453  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022588855  |\n",
            "| clipfrac           | 0.09831543   |\n",
            "| ep_len_mean        | 20.6         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.909        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 422          |\n",
            "| policy_entropy     | 0.19222693   |\n",
            "| policy_loss        | -0.035511713 |\n",
            "| serial_timesteps   | 216064       |\n",
            "| time_elapsed       | 4.75e+03     |\n",
            "| total_timesteps    | 1728512      |\n",
            "| value_loss         | 0.06193896   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1730000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.00 +/- 2.90\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0545549    |\n",
            "| clipfrac           | 0.08630371   |\n",
            "| ep_len_mean        | 19.8         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.844        |\n",
            "| fps                | 407          |\n",
            "| n_updates          | 423          |\n",
            "| policy_entropy     | 0.15372978   |\n",
            "| policy_loss        | -0.022240676 |\n",
            "| serial_timesteps   | 216576       |\n",
            "| time_elapsed       | 4.76e+03     |\n",
            "| total_timesteps    | 1732608      |\n",
            "| value_loss         | 0.04817403   |\n",
            "-------------------------------------\n",
            "-----------------------------------\n",
            "| approxkl           | 0.43752822 |\n",
            "| clipfrac           | 0.20134278 |\n",
            "| ep_len_mean        | 25.4       |\n",
            "| ep_reward_mean     | 0.928      |\n",
            "| explained_variance | 0.471      |\n",
            "| fps                | 415        |\n",
            "| n_updates          | 424        |\n",
            "| policy_entropy     | 0.21376336 |\n",
            "| policy_loss        | 0.29065818 |\n",
            "| serial_timesteps   | 217088     |\n",
            "| time_elapsed       | 4.77e+03   |\n",
            "| total_timesteps    | 1736704    |\n",
            "| value_loss         | 0.26486567 |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=1740000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 1.83\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022237128  |\n",
            "| clipfrac           | 0.11015625   |\n",
            "| ep_len_mean        | 19.5         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.83         |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 425          |\n",
            "| policy_entropy     | 0.23695998   |\n",
            "| policy_loss        | -0.033457406 |\n",
            "| serial_timesteps   | 217600       |\n",
            "| time_elapsed       | 4.78e+03     |\n",
            "| total_timesteps    | 1740800      |\n",
            "| value_loss         | 0.103920996  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.10648258    |\n",
            "| clipfrac           | 0.03227539    |\n",
            "| ep_len_mean        | 16.7          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.903         |\n",
            "| fps                | 421           |\n",
            "| n_updates          | 426           |\n",
            "| policy_entropy     | 0.0354489     |\n",
            "| policy_loss        | -0.0061536347 |\n",
            "| serial_timesteps   | 218112        |\n",
            "| time_elapsed       | 4.79e+03      |\n",
            "| total_timesteps    | 1744896       |\n",
            "| value_loss         | 0.005672586   |\n",
            "--------------------------------------\n",
            "-----------------------------------\n",
            "| approxkl           | 1.3602383  |\n",
            "| clipfrac           | 0.29331055 |\n",
            "| ep_len_mean        | 25.9       |\n",
            "| ep_reward_mean     | 0.927      |\n",
            "| explained_variance | 0.252      |\n",
            "| fps                | 422        |\n",
            "| n_updates          | 427        |\n",
            "| policy_entropy     | 0.18550475 |\n",
            "| policy_loss        | 0.3659237  |\n",
            "| serial_timesteps   | 218624     |\n",
            "| time_elapsed       | 4.8e+03    |\n",
            "| total_timesteps    | 1748992    |\n",
            "| value_loss         | 0.33622718 |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=1750000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.60 +/- 2.24\n",
            "-------------------------------------\n",
            "| approxkl           | 0.3558659    |\n",
            "| clipfrac           | 0.14360352   |\n",
            "| ep_len_mean        | 19.5         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.449        |\n",
            "| fps                | 413          |\n",
            "| n_updates          | 428          |\n",
            "| policy_entropy     | 0.11149578   |\n",
            "| policy_loss        | -0.024698108 |\n",
            "| serial_timesteps   | 219136       |\n",
            "| time_elapsed       | 4.81e+03     |\n",
            "| total_timesteps    | 1753088      |\n",
            "| value_loss         | 0.1382439    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.30453393   |\n",
            "| clipfrac           | 0.16643067   |\n",
            "| ep_len_mean        | 20.6         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.719        |\n",
            "| fps                | 424          |\n",
            "| n_updates          | 429          |\n",
            "| policy_entropy     | 0.1860334    |\n",
            "| policy_loss        | -0.030862585 |\n",
            "| serial_timesteps   | 219648       |\n",
            "| time_elapsed       | 4.82e+03     |\n",
            "| total_timesteps    | 1757184      |\n",
            "| value_loss         | 0.092634596  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1760000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.00 +/- 2.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.057541896  |\n",
            "| clipfrac           | 0.10275879   |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.844        |\n",
            "| fps                | 402          |\n",
            "| n_updates          | 430          |\n",
            "| policy_entropy     | 0.1960937    |\n",
            "| policy_loss        | -0.026055077 |\n",
            "| serial_timesteps   | 220160       |\n",
            "| time_elapsed       | 4.83e+03     |\n",
            "| total_timesteps    | 1761280      |\n",
            "| value_loss         | 0.062402703  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.063239105 |\n",
            "| clipfrac           | 0.17814942  |\n",
            "| ep_len_mean        | 20          |\n",
            "| ep_reward_mean     | 0.944       |\n",
            "| explained_variance | 0.854       |\n",
            "| fps                | 418         |\n",
            "| n_updates          | 431         |\n",
            "| policy_entropy     | 0.33173758  |\n",
            "| policy_loss        | -0.03897942 |\n",
            "| serial_timesteps   | 220672      |\n",
            "| time_elapsed       | 4.84e+03    |\n",
            "| total_timesteps    | 1765376     |\n",
            "| value_loss         | 0.076444075 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.053500645  |\n",
            "| clipfrac           | 0.2128418    |\n",
            "| ep_len_mean        | 25.3         |\n",
            "| ep_reward_mean     | 0.929        |\n",
            "| explained_variance | 0.81         |\n",
            "| fps                | 424          |\n",
            "| n_updates          | 432          |\n",
            "| policy_entropy     | 0.45331565   |\n",
            "| policy_loss        | -0.044226967 |\n",
            "| serial_timesteps   | 221184       |\n",
            "| time_elapsed       | 4.85e+03     |\n",
            "| total_timesteps    | 1769472      |\n",
            "| value_loss         | 0.15701288   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1770000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.40 +/- 4.08\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04751495   |\n",
            "| clipfrac           | 0.18835449   |\n",
            "| ep_len_mean        | 25.3         |\n",
            "| ep_reward_mean     | 0.93         |\n",
            "| explained_variance | 0.711        |\n",
            "| fps                | 406          |\n",
            "| n_updates          | 433          |\n",
            "| policy_entropy     | 0.33099517   |\n",
            "| policy_loss        | -0.001375217 |\n",
            "| serial_timesteps   | 221696       |\n",
            "| time_elapsed       | 4.86e+03     |\n",
            "| total_timesteps    | 1773568      |\n",
            "| value_loss         | 0.15508585   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.02330446  |\n",
            "| clipfrac           | 0.16838379  |\n",
            "| ep_len_mean        | 26.3        |\n",
            "| ep_reward_mean     | 0.925       |\n",
            "| explained_variance | 0.862       |\n",
            "| fps                | 423         |\n",
            "| n_updates          | 434         |\n",
            "| policy_entropy     | 0.45143253  |\n",
            "| policy_loss        | -0.03250773 |\n",
            "| serial_timesteps   | 222208      |\n",
            "| time_elapsed       | 4.87e+03    |\n",
            "| total_timesteps    | 1777664     |\n",
            "| value_loss         | 0.14125082  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=1780000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.00 +/- 1.10\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04787239   |\n",
            "| clipfrac           | 0.14177246   |\n",
            "| ep_len_mean        | 20.1         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.694        |\n",
            "| fps                | 402          |\n",
            "| n_updates          | 435          |\n",
            "| policy_entropy     | 0.30293503   |\n",
            "| policy_loss        | 0.0012019796 |\n",
            "| serial_timesteps   | 222720       |\n",
            "| time_elapsed       | 4.88e+03     |\n",
            "| total_timesteps    | 1781760      |\n",
            "| value_loss         | 0.13030824   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.025426477 |\n",
            "| clipfrac           | 0.112011716 |\n",
            "| ep_len_mean        | 19.7        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.903       |\n",
            "| fps                | 417         |\n",
            "| n_updates          | 436         |\n",
            "| policy_entropy     | 0.24810381  |\n",
            "| policy_loss        | -0.03227476 |\n",
            "| serial_timesteps   | 223232      |\n",
            "| time_elapsed       | 4.89e+03    |\n",
            "| total_timesteps    | 1785856     |\n",
            "| value_loss         | 0.037620004 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03781397   |\n",
            "| clipfrac           | 0.053979494  |\n",
            "| ep_len_mean        | 18.2         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.892        |\n",
            "| fps                | 426          |\n",
            "| n_updates          | 437          |\n",
            "| policy_entropy     | 0.084657006  |\n",
            "| policy_loss        | -0.018290743 |\n",
            "| serial_timesteps   | 223744       |\n",
            "| time_elapsed       | 4.9e+03      |\n",
            "| total_timesteps    | 1789952      |\n",
            "| value_loss         | 0.012510391  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1790000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 5.15\n",
            "-------------------------------------\n",
            "| approxkl           | 0.06242018   |\n",
            "| clipfrac           | 0.20263672   |\n",
            "| ep_len_mean        | 26.4         |\n",
            "| ep_reward_mean     | 0.924        |\n",
            "| explained_variance | 0.85         |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 438          |\n",
            "| policy_entropy     | 0.4031593    |\n",
            "| policy_loss        | -0.009168248 |\n",
            "| serial_timesteps   | 224256       |\n",
            "| time_elapsed       | 4.91e+03     |\n",
            "| total_timesteps    | 1794048      |\n",
            "| value_loss         | 0.18058392   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.027210662  |\n",
            "| clipfrac           | 0.16364746   |\n",
            "| ep_len_mean        | 22.6         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.884        |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 439          |\n",
            "| policy_entropy     | 0.39644593   |\n",
            "| policy_loss        | -0.036141995 |\n",
            "| serial_timesteps   | 224768       |\n",
            "| time_elapsed       | 4.92e+03     |\n",
            "| total_timesteps    | 1798144      |\n",
            "| value_loss         | 0.09411851   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1800000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 0.98\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02388835   |\n",
            "| clipfrac           | 0.11586914   |\n",
            "| ep_len_mean        | 21.3         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.889        |\n",
            "| fps                | 407          |\n",
            "| n_updates          | 440          |\n",
            "| policy_entropy     | 0.26759744   |\n",
            "| policy_loss        | -0.023839701 |\n",
            "| serial_timesteps   | 225280       |\n",
            "| time_elapsed       | 4.93e+03     |\n",
            "| total_timesteps    | 1802240      |\n",
            "| value_loss         | 0.07407707   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03874278   |\n",
            "| clipfrac           | 0.16875      |\n",
            "| ep_len_mean        | 22.8         |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.845        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 441          |\n",
            "| policy_entropy     | 0.28095108   |\n",
            "| policy_loss        | -0.021118172 |\n",
            "| serial_timesteps   | 225792       |\n",
            "| time_elapsed       | 4.94e+03     |\n",
            "| total_timesteps    | 1806336      |\n",
            "| value_loss         | 0.103838444  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1810000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 1.20\n",
            "------------------------------------\n",
            "| approxkl           | 0.024698239 |\n",
            "| clipfrac           | 0.15651855  |\n",
            "| ep_len_mean        | 26.1        |\n",
            "| ep_reward_mean     | 0.926       |\n",
            "| explained_variance | 0.909       |\n",
            "| fps                | 415         |\n",
            "| n_updates          | 442         |\n",
            "| policy_entropy     | 0.5404869   |\n",
            "| policy_loss        | -0.02848491 |\n",
            "| serial_timesteps   | 226304      |\n",
            "| time_elapsed       | 4.95e+03    |\n",
            "| total_timesteps    | 1810432     |\n",
            "| value_loss         | 0.12979361  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019964144  |\n",
            "| clipfrac           | 0.14035645   |\n",
            "| ep_len_mean        | 24.9         |\n",
            "| ep_reward_mean     | 0.93         |\n",
            "| explained_variance | 0.917        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 443          |\n",
            "| policy_entropy     | 0.47990045   |\n",
            "| policy_loss        | -0.030356267 |\n",
            "| serial_timesteps   | 226816       |\n",
            "| time_elapsed       | 4.96e+03     |\n",
            "| total_timesteps    | 1814528      |\n",
            "| value_loss         | 0.108788624  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 1.5068276    |\n",
            "| clipfrac           | 0.16745606   |\n",
            "| ep_len_mean        | 23.2         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.901        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 444          |\n",
            "| policy_entropy     | 0.31906956   |\n",
            "| policy_loss        | -0.047373928 |\n",
            "| serial_timesteps   | 227328       |\n",
            "| time_elapsed       | 4.97e+03     |\n",
            "| total_timesteps    | 1818624      |\n",
            "| value_loss         | 0.1254455    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1820000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 3.19\n",
            "------------------------------------\n",
            "| approxkl           | 0.13590828  |\n",
            "| clipfrac           | 0.16455078  |\n",
            "| ep_len_mean        | 21.6        |\n",
            "| ep_reward_mean     | 0.939       |\n",
            "| explained_variance | 0.958       |\n",
            "| fps                | 404         |\n",
            "| n_updates          | 445         |\n",
            "| policy_entropy     | 0.33042997  |\n",
            "| policy_loss        | -0.04334258 |\n",
            "| serial_timesteps   | 227840      |\n",
            "| time_elapsed       | 4.98e+03    |\n",
            "| total_timesteps    | 1822720     |\n",
            "| value_loss         | 0.049231    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.028417299  |\n",
            "| clipfrac           | 0.085668944  |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.927        |\n",
            "| fps                | 424          |\n",
            "| n_updates          | 446          |\n",
            "| policy_entropy     | 0.18504892   |\n",
            "| policy_loss        | -0.015078738 |\n",
            "| serial_timesteps   | 228352       |\n",
            "| time_elapsed       | 4.99e+03     |\n",
            "| total_timesteps    | 1826816      |\n",
            "| value_loss         | 0.042314455  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1830000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.00 +/- 2.28\n",
            "-------------------------------------\n",
            "| approxkl           | 0.088386066  |\n",
            "| clipfrac           | 0.07961426   |\n",
            "| ep_len_mean        | 18           |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.904        |\n",
            "| fps                | 405          |\n",
            "| n_updates          | 447          |\n",
            "| policy_entropy     | 0.12667035   |\n",
            "| policy_loss        | -0.027199382 |\n",
            "| serial_timesteps   | 228864       |\n",
            "| time_elapsed       | 5e+03        |\n",
            "| total_timesteps    | 1830912      |\n",
            "| value_loss         | 0.025890846  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.029231463  |\n",
            "| clipfrac           | 0.11711426   |\n",
            "| ep_len_mean        | 20.4         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.957        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 448          |\n",
            "| policy_entropy     | 0.26999217   |\n",
            "| policy_loss        | -0.032641664 |\n",
            "| serial_timesteps   | 229376       |\n",
            "| time_elapsed       | 5.01e+03     |\n",
            "| total_timesteps    | 1835008      |\n",
            "| value_loss         | 0.027901918  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.13386175   |\n",
            "| clipfrac           | 0.07600097   |\n",
            "| ep_len_mean        | 19.6         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.931        |\n",
            "| fps                | 428          |\n",
            "| n_updates          | 449          |\n",
            "| policy_entropy     | 0.12917528   |\n",
            "| policy_loss        | -0.027486268 |\n",
            "| serial_timesteps   | 229888       |\n",
            "| time_elapsed       | 5.02e+03     |\n",
            "| total_timesteps    | 1839104      |\n",
            "| value_loss         | 0.025642643  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1840000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 19.00 +/- 3.74\n",
            "-------------------------------------\n",
            "| approxkl           | 2.4816227    |\n",
            "| clipfrac           | 0.15168457   |\n",
            "| ep_len_mean        | 17           |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.695        |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 450          |\n",
            "| policy_entropy     | 0.055883873  |\n",
            "| policy_loss        | -0.059565514 |\n",
            "| serial_timesteps   | 230400       |\n",
            "| time_elapsed       | 5.03e+03     |\n",
            "| total_timesteps    | 1843200      |\n",
            "| value_loss         | 0.06395249   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.09641541   |\n",
            "| clipfrac           | 0.04753418   |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.639        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 451          |\n",
            "| policy_entropy     | 0.05004556   |\n",
            "| policy_loss        | -0.021869902 |\n",
            "| serial_timesteps   | 230912       |\n",
            "| time_elapsed       | 5.04e+03     |\n",
            "| total_timesteps    | 1847296      |\n",
            "| value_loss         | 0.10344676   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1850000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.00 +/- 1.10\n",
            "------------------------------------\n",
            "| approxkl           | 0.07998736  |\n",
            "| clipfrac           | 0.06801758  |\n",
            "| ep_len_mean        | 18.2        |\n",
            "| ep_reward_mean     | 0.949       |\n",
            "| explained_variance | 0.86        |\n",
            "| fps                | 409         |\n",
            "| n_updates          | 452         |\n",
            "| policy_entropy     | 0.09223111  |\n",
            "| policy_loss        | -0.01737275 |\n",
            "| serial_timesteps   | 231424      |\n",
            "| time_elapsed       | 5.05e+03    |\n",
            "| total_timesteps    | 1851392     |\n",
            "| value_loss         | 0.017621696 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.070496485  |\n",
            "| clipfrac           | 0.14250489   |\n",
            "| ep_len_mean        | 19.4         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.746        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 453          |\n",
            "| policy_entropy     | 0.20214379   |\n",
            "| policy_loss        | -0.028208885 |\n",
            "| serial_timesteps   | 231936       |\n",
            "| time_elapsed       | 5.06e+03     |\n",
            "| total_timesteps    | 1855488      |\n",
            "| value_loss         | 0.05785774   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.035467763  |\n",
            "| clipfrac           | 0.17143555   |\n",
            "| ep_len_mean        | 20.3         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.692        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 454          |\n",
            "| policy_entropy     | 0.3720943    |\n",
            "| policy_loss        | -0.022497017 |\n",
            "| serial_timesteps   | 232448       |\n",
            "| time_elapsed       | 5.07e+03     |\n",
            "| total_timesteps    | 1859584      |\n",
            "| value_loss         | 0.19766179   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1860000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.20 +/- 122.96\n",
            "------------------------------------\n",
            "| approxkl           | 0.05939638  |\n",
            "| clipfrac           | 0.22009277  |\n",
            "| ep_len_mean        | 24.9        |\n",
            "| ep_reward_mean     | 0.929       |\n",
            "| explained_variance | 0.837       |\n",
            "| fps                | 369         |\n",
            "| n_updates          | 455         |\n",
            "| policy_entropy     | 0.59007394  |\n",
            "| policy_loss        | -0.04256528 |\n",
            "| serial_timesteps   | 232960      |\n",
            "| time_elapsed       | 5.08e+03    |\n",
            "| total_timesteps    | 1863680     |\n",
            "| value_loss         | 0.16855034  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02714245   |\n",
            "| clipfrac           | 0.21369629   |\n",
            "| ep_len_mean        | 28.9         |\n",
            "| ep_reward_mean     | 0.917        |\n",
            "| explained_variance | 0.759        |\n",
            "| fps                | 424          |\n",
            "| n_updates          | 456          |\n",
            "| policy_entropy     | 0.6447141    |\n",
            "| policy_loss        | -0.035320822 |\n",
            "| serial_timesteps   | 233472       |\n",
            "| time_elapsed       | 5.09e+03     |\n",
            "| total_timesteps    | 1867776      |\n",
            "| value_loss         | 0.28847164   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1870000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.40 +/- 122.45\n",
            "-------------------------------------\n",
            "| approxkl           | 0.120601974  |\n",
            "| clipfrac           | 0.2584961    |\n",
            "| ep_len_mean        | 28.6         |\n",
            "| ep_reward_mean     | 0.918        |\n",
            "| explained_variance | 0.814        |\n",
            "| fps                | 358          |\n",
            "| n_updates          | 457          |\n",
            "| policy_entropy     | 0.54751855   |\n",
            "| policy_loss        | -0.043971952 |\n",
            "| serial_timesteps   | 233984       |\n",
            "| time_elapsed       | 5.1e+03      |\n",
            "| total_timesteps    | 1871872      |\n",
            "| value_loss         | 0.21150109   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 2.229504    |\n",
            "| clipfrac           | 0.15668945  |\n",
            "| ep_len_mean        | 26.1        |\n",
            "| ep_reward_mean     | 0.925       |\n",
            "| explained_variance | 0.875       |\n",
            "| fps                | 415         |\n",
            "| n_updates          | 458         |\n",
            "| policy_entropy     | 0.37992567  |\n",
            "| policy_loss        | -0.02244911 |\n",
            "| serial_timesteps   | 234496      |\n",
            "| time_elapsed       | 5.11e+03    |\n",
            "| total_timesteps    | 1875968     |\n",
            "| value_loss         | 0.19730869  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=1880000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.47\n",
            "------------------------------------\n",
            "| approxkl           | 0.019341085 |\n",
            "| clipfrac           | 0.14023438  |\n",
            "| ep_len_mean        | 18.5        |\n",
            "| ep_reward_mean     | 0.949       |\n",
            "| explained_variance | 0.961       |\n",
            "| fps                | 405         |\n",
            "| n_updates          | 459         |\n",
            "| policy_entropy     | 0.43117613  |\n",
            "| policy_loss        | -0.03679841 |\n",
            "| serial_timesteps   | 235008      |\n",
            "| time_elapsed       | 5.12e+03    |\n",
            "| total_timesteps    | 1880064     |\n",
            "| value_loss         | 0.040245444 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.096225195  |\n",
            "| clipfrac           | 0.1689209    |\n",
            "| ep_len_mean        | 23.5         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.882        |\n",
            "| fps                | 415          |\n",
            "| n_updates          | 460          |\n",
            "| policy_entropy     | 0.4168343    |\n",
            "| policy_loss        | -0.020810563 |\n",
            "| serial_timesteps   | 235520       |\n",
            "| time_elapsed       | 5.13e+03     |\n",
            "| total_timesteps    | 1884160      |\n",
            "| value_loss         | 0.12431886   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023888955  |\n",
            "| clipfrac           | 0.13190918   |\n",
            "| ep_len_mean        | 21           |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.961        |\n",
            "| fps                | 427          |\n",
            "| n_updates          | 461          |\n",
            "| policy_entropy     | 0.42296058   |\n",
            "| policy_loss        | -0.033290055 |\n",
            "| serial_timesteps   | 236032       |\n",
            "| time_elapsed       | 5.14e+03     |\n",
            "| total_timesteps    | 1888256      |\n",
            "| value_loss         | 0.053020287  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1890000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 1.67\n",
            "------------------------------------\n",
            "| approxkl           | 1.6562265   |\n",
            "| clipfrac           | 0.12321778  |\n",
            "| ep_len_mean        | 20          |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.898       |\n",
            "| fps                | 410         |\n",
            "| n_updates          | 462         |\n",
            "| policy_entropy     | 0.22996485  |\n",
            "| policy_loss        | 0.08037649  |\n",
            "| serial_timesteps   | 236544      |\n",
            "| time_elapsed       | 5.15e+03    |\n",
            "| total_timesteps    | 1892352     |\n",
            "| value_loss         | 0.043473065 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.026761776 |\n",
            "| clipfrac           | 0.112939455 |\n",
            "| ep_len_mean        | 18.3        |\n",
            "| ep_reward_mean     | 0.949       |\n",
            "| explained_variance | 0.98        |\n",
            "| fps                | 417         |\n",
            "| n_updates          | 463         |\n",
            "| policy_entropy     | 0.3432049   |\n",
            "| policy_loss        | -0.03030785 |\n",
            "| serial_timesteps   | 237056      |\n",
            "| time_elapsed       | 5.16e+03    |\n",
            "| total_timesteps    | 1896448     |\n",
            "| value_loss         | 0.021044822 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=1900000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 16.60 +/- 0.80\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0188946    |\n",
            "| clipfrac           | 0.1305664    |\n",
            "| ep_len_mean        | 19.9         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.949        |\n",
            "| fps                | 405          |\n",
            "| n_updates          | 464          |\n",
            "| policy_entropy     | 0.38227522   |\n",
            "| policy_loss        | -0.031684324 |\n",
            "| serial_timesteps   | 237568       |\n",
            "| time_elapsed       | 5.17e+03     |\n",
            "| total_timesteps    | 1900544      |\n",
            "| value_loss         | 0.06329052   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015624344  |\n",
            "| clipfrac           | 0.071948245  |\n",
            "| ep_len_mean        | 16.9         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.93         |\n",
            "| fps                | 427          |\n",
            "| n_updates          | 465          |\n",
            "| policy_entropy     | 0.20420654   |\n",
            "| policy_loss        | -0.022188123 |\n",
            "| serial_timesteps   | 238080       |\n",
            "| time_elapsed       | 5.18e+03     |\n",
            "| total_timesteps    | 1904640      |\n",
            "| value_loss         | 0.04357811   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03616719   |\n",
            "| clipfrac           | 0.13930663   |\n",
            "| ep_len_mean        | 22.8         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.972        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 466          |\n",
            "| policy_entropy     | 0.42436713   |\n",
            "| policy_loss        | -0.034964792 |\n",
            "| serial_timesteps   | 238592       |\n",
            "| time_elapsed       | 5.19e+03     |\n",
            "| total_timesteps    | 1908736      |\n",
            "| value_loss         | 0.035509504  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1910000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.20 +/- 3.31\n",
            "------------------------------------\n",
            "| approxkl           | 3.6652522   |\n",
            "| clipfrac           | 0.16503906  |\n",
            "| ep_len_mean        | 21.9        |\n",
            "| ep_reward_mean     | 0.938       |\n",
            "| explained_variance | 0.888       |\n",
            "| fps                | 409         |\n",
            "| n_updates          | 467         |\n",
            "| policy_entropy     | 0.26521108  |\n",
            "| policy_loss        | -0.0518473  |\n",
            "| serial_timesteps   | 239104      |\n",
            "| time_elapsed       | 5.2e+03     |\n",
            "| total_timesteps    | 1912832     |\n",
            "| value_loss         | 0.105925344 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.053432096  |\n",
            "| clipfrac           | 0.11936035   |\n",
            "| ep_len_mean        | 20.2         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.928        |\n",
            "| fps                | 420          |\n",
            "| n_updates          | 468          |\n",
            "| policy_entropy     | 0.31101722   |\n",
            "| policy_loss        | -0.019859742 |\n",
            "| serial_timesteps   | 239616       |\n",
            "| time_elapsed       | 5.21e+03     |\n",
            "| total_timesteps    | 1916928      |\n",
            "| value_loss         | 0.07003025   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1920000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.40 +/- 3.38\n",
            "------------------------------------\n",
            "| approxkl           | 0.09153809  |\n",
            "| clipfrac           | 0.14685059  |\n",
            "| ep_len_mean        | 24.5        |\n",
            "| ep_reward_mean     | 0.93        |\n",
            "| explained_variance | 0.965       |\n",
            "| fps                | 404         |\n",
            "| n_updates          | 469         |\n",
            "| policy_entropy     | 0.43582582  |\n",
            "| policy_loss        | -0.03587457 |\n",
            "| serial_timesteps   | 240128      |\n",
            "| time_elapsed       | 5.22e+03    |\n",
            "| total_timesteps    | 1921024     |\n",
            "| value_loss         | 0.055426747 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.6569287    |\n",
            "| clipfrac           | 0.11652832   |\n",
            "| ep_len_mean        | 23.4         |\n",
            "| ep_reward_mean     | 0.933        |\n",
            "| explained_variance | 0.901        |\n",
            "| fps                | 424          |\n",
            "| n_updates          | 470          |\n",
            "| policy_entropy     | 0.21533962   |\n",
            "| policy_loss        | -0.045089934 |\n",
            "| serial_timesteps   | 240640       |\n",
            "| time_elapsed       | 5.23e+03     |\n",
            "| total_timesteps    | 1925120      |\n",
            "| value_loss         | 0.094162986  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.1426184    |\n",
            "| clipfrac           | 0.09611817   |\n",
            "| ep_len_mean        | 20.3         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.99         |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 471          |\n",
            "| policy_entropy     | 0.33587438   |\n",
            "| policy_loss        | -0.023792634 |\n",
            "| serial_timesteps   | 241152       |\n",
            "| time_elapsed       | 5.24e+03     |\n",
            "| total_timesteps    | 1929216      |\n",
            "| value_loss         | 0.013482852  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1930000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 4.66\n",
            "-----------------------------------\n",
            "| approxkl           | 4.4235654  |\n",
            "| clipfrac           | 0.28061524 |\n",
            "| ep_len_mean        | 32.4       |\n",
            "| ep_reward_mean     | 0.906      |\n",
            "| explained_variance | 0.56       |\n",
            "| fps                | 406        |\n",
            "| n_updates          | 472        |\n",
            "| policy_entropy     | 0.14152426 |\n",
            "| policy_loss        | 0.04889553 |\n",
            "| serial_timesteps   | 241664     |\n",
            "| time_elapsed       | 5.25e+03   |\n",
            "| total_timesteps    | 1933312    |\n",
            "| value_loss         | 0.3696537  |\n",
            "-----------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 3.8093395   |\n",
            "| clipfrac           | 0.14338379  |\n",
            "| ep_len_mean        | 23          |\n",
            "| ep_reward_mean     | 0.934       |\n",
            "| explained_variance | 0.47        |\n",
            "| fps                | 416         |\n",
            "| n_updates          | 473         |\n",
            "| policy_entropy     | 0.015030531 |\n",
            "| policy_loss        | -0.04512363 |\n",
            "| serial_timesteps   | 242176      |\n",
            "| time_elapsed       | 5.26e+03    |\n",
            "| total_timesteps    | 1937408     |\n",
            "| value_loss         | 0.18518876  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=1940000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.20 +/- 1.17\n",
            "-------------------------------------\n",
            "| approxkl           | 1.422683     |\n",
            "| clipfrac           | 0.19853516   |\n",
            "| ep_len_mean        | 22.8         |\n",
            "| ep_reward_mean     | 0.935        |\n",
            "| explained_variance | 0.535        |\n",
            "| fps                | 411          |\n",
            "| n_updates          | 474          |\n",
            "| policy_entropy     | 0.11482004   |\n",
            "| policy_loss        | -0.055685036 |\n",
            "| serial_timesteps   | 242688       |\n",
            "| time_elapsed       | 5.27e+03     |\n",
            "| total_timesteps    | 1941504      |\n",
            "| value_loss         | 0.18639827   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.15545654   |\n",
            "| clipfrac           | 0.048535157  |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.804        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 475          |\n",
            "| policy_entropy     | 0.041752286  |\n",
            "| policy_loss        | -0.012137766 |\n",
            "| serial_timesteps   | 243200       |\n",
            "| time_elapsed       | 5.28e+03     |\n",
            "| total_timesteps    | 1945600      |\n",
            "| value_loss         | 0.023269292  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 1.2872579    |\n",
            "| clipfrac           | 0.11835937   |\n",
            "| ep_len_mean        | 18.1         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.706        |\n",
            "| fps                | 413          |\n",
            "| n_updates          | 476          |\n",
            "| policy_entropy     | 0.052802812  |\n",
            "| policy_loss        | -0.034498904 |\n",
            "| serial_timesteps   | 243712       |\n",
            "| time_elapsed       | 5.29e+03     |\n",
            "| total_timesteps    | 1949696      |\n",
            "| value_loss         | 0.06084581   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1950000, episode_reward=0.75 +/- 0.38\n",
            "Episode length: 82.60 +/- 120.83\n",
            "------------------------------------\n",
            "| approxkl           | 0.4944149   |\n",
            "| clipfrac           | 0.04851074  |\n",
            "| ep_len_mean        | 17.1        |\n",
            "| ep_reward_mean     | 0.952       |\n",
            "| explained_variance | 0.848       |\n",
            "| fps                | 361         |\n",
            "| n_updates          | 477         |\n",
            "| policy_entropy     | 0.027510187 |\n",
            "| policy_loss        | -0.01683381 |\n",
            "| serial_timesteps   | 244224      |\n",
            "| time_elapsed       | 5.3e+03     |\n",
            "| total_timesteps    | 1953792     |\n",
            "| value_loss         | 0.009709066 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.53455436   |\n",
            "| clipfrac           | 0.20715332   |\n",
            "| ep_len_mean        | 26.2         |\n",
            "| ep_reward_mean     | 0.925        |\n",
            "| explained_variance | 0.507        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 478          |\n",
            "| policy_entropy     | 0.16235214   |\n",
            "| policy_loss        | -0.017038543 |\n",
            "| serial_timesteps   | 244736       |\n",
            "| time_elapsed       | 5.31e+03     |\n",
            "| total_timesteps    | 1957888      |\n",
            "| value_loss         | 0.21650799   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1960000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 5.23\n",
            "-------------------------------------\n",
            "| approxkl           | 0.13496646   |\n",
            "| clipfrac           | 0.17297363   |\n",
            "| ep_len_mean        | 22           |\n",
            "| ep_reward_mean     | 0.938        |\n",
            "| explained_variance | 0.667        |\n",
            "| fps                | 403          |\n",
            "| n_updates          | 479          |\n",
            "| policy_entropy     | 0.19130793   |\n",
            "| policy_loss        | -0.013044943 |\n",
            "| serial_timesteps   | 245248       |\n",
            "| time_elapsed       | 5.32e+03     |\n",
            "| total_timesteps    | 1961984      |\n",
            "| value_loss         | 0.12183609   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.13585892   |\n",
            "| clipfrac           | 0.06716309   |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.662        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 480          |\n",
            "| policy_entropy     | 0.08065088   |\n",
            "| policy_loss        | -0.009968149 |\n",
            "| serial_timesteps   | 245760       |\n",
            "| time_elapsed       | 5.33e+03     |\n",
            "| total_timesteps    | 1966080      |\n",
            "| value_loss         | 0.047494534  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1970000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.60 +/- 1.85\n",
            "-------------------------------------\n",
            "| approxkl           | 0.10095098   |\n",
            "| clipfrac           | 0.082250975  |\n",
            "| ep_len_mean        | 16.7         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.867        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 481          |\n",
            "| policy_entropy     | 0.12862399   |\n",
            "| policy_loss        | -0.028634941 |\n",
            "| serial_timesteps   | 246272       |\n",
            "| time_elapsed       | 5.34e+03     |\n",
            "| total_timesteps    | 1970176      |\n",
            "| value_loss         | 0.0114206355 |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.06704022  |\n",
            "| clipfrac           | 0.18276367  |\n",
            "| ep_len_mean        | 22.3        |\n",
            "| ep_reward_mean     | 0.938       |\n",
            "| explained_variance | 0.68        |\n",
            "| fps                | 416         |\n",
            "| n_updates          | 482         |\n",
            "| policy_entropy     | 0.3050794   |\n",
            "| policy_loss        | -0.04110416 |\n",
            "| serial_timesteps   | 246784      |\n",
            "| time_elapsed       | 5.35e+03    |\n",
            "| total_timesteps    | 1974272     |\n",
            "| value_loss         | 0.10109439  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04169514   |\n",
            "| clipfrac           | 0.20014648   |\n",
            "| ep_len_mean        | 24.3         |\n",
            "| ep_reward_mean     | 0.933        |\n",
            "| explained_variance | 0.803        |\n",
            "| fps                | 424          |\n",
            "| n_updates          | 483          |\n",
            "| policy_entropy     | 0.4594202    |\n",
            "| policy_loss        | -0.044184536 |\n",
            "| serial_timesteps   | 247296       |\n",
            "| time_elapsed       | 5.36e+03     |\n",
            "| total_timesteps    | 1978368      |\n",
            "| value_loss         | 0.11230172   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1980000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.00 +/- 123.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024239395  |\n",
            "| clipfrac           | 0.18586425   |\n",
            "| ep_len_mean        | 24           |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.869        |\n",
            "| fps                | 364          |\n",
            "| n_updates          | 484          |\n",
            "| policy_entropy     | 0.66280097   |\n",
            "| policy_loss        | -0.035441283 |\n",
            "| serial_timesteps   | 247808       |\n",
            "| time_elapsed       | 5.37e+03     |\n",
            "| total_timesteps    | 1982464      |\n",
            "| value_loss         | 0.1680846    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03337897   |\n",
            "| clipfrac           | 0.22470704   |\n",
            "| ep_len_mean        | 27.6         |\n",
            "| ep_reward_mean     | 0.92         |\n",
            "| explained_variance | 0.76         |\n",
            "| fps                | 416          |\n",
            "| n_updates          | 485          |\n",
            "| policy_entropy     | 0.5960821    |\n",
            "| policy_loss        | -0.035794705 |\n",
            "| serial_timesteps   | 248320       |\n",
            "| time_elapsed       | 5.38e+03     |\n",
            "| total_timesteps    | 1986560      |\n",
            "| value_loss         | 0.27500767   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=1990000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.00 +/- 3.58\n",
            "------------------------------------\n",
            "| approxkl           | 0.06514752  |\n",
            "| clipfrac           | 0.20712891  |\n",
            "| ep_len_mean        | 28.6        |\n",
            "| ep_reward_mean     | 0.92        |\n",
            "| explained_variance | 0.718       |\n",
            "| fps                | 409         |\n",
            "| n_updates          | 486         |\n",
            "| policy_entropy     | 0.5071537   |\n",
            "| policy_loss        | 0.013033843 |\n",
            "| serial_timesteps   | 248832      |\n",
            "| time_elapsed       | 5.39e+03    |\n",
            "| total_timesteps    | 1990656     |\n",
            "| value_loss         | 0.18187463  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018733734  |\n",
            "| clipfrac           | 0.19265136   |\n",
            "| ep_len_mean        | 27.4         |\n",
            "| ep_reward_mean     | 0.922        |\n",
            "| explained_variance | 0.893        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 487          |\n",
            "| policy_entropy     | 0.68886673   |\n",
            "| policy_loss        | -0.039786976 |\n",
            "| serial_timesteps   | 249344       |\n",
            "| time_elapsed       | 5.4e+03      |\n",
            "| total_timesteps    | 1994752      |\n",
            "| value_loss         | 0.1269869    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022064568  |\n",
            "| clipfrac           | 0.17363282   |\n",
            "| ep_len_mean        | 26.1         |\n",
            "| ep_reward_mean     | 0.926        |\n",
            "| explained_variance | 0.878        |\n",
            "| fps                | 416          |\n",
            "| n_updates          | 488          |\n",
            "| policy_entropy     | 0.5032523    |\n",
            "| policy_loss        | -0.039415345 |\n",
            "| serial_timesteps   | 249856       |\n",
            "| time_elapsed       | 5.41e+03     |\n",
            "| total_timesteps    | 1998848      |\n",
            "| value_loss         | 0.115540884  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2000000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.00 +/- 3.16\n",
            "-------------------------------------\n",
            "| approxkl           | 4.180621     |\n",
            "| clipfrac           | 0.20275879   |\n",
            "| ep_len_mean        | 28.1         |\n",
            "| ep_reward_mean     | 0.92         |\n",
            "| explained_variance | 0.846        |\n",
            "| fps                | 398          |\n",
            "| n_updates          | 489          |\n",
            "| policy_entropy     | 0.43092912   |\n",
            "| policy_loss        | -0.049231447 |\n",
            "| serial_timesteps   | 250368       |\n",
            "| time_elapsed       | 5.42e+03     |\n",
            "| total_timesteps    | 2002944      |\n",
            "| value_loss         | 0.16372177   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024245625  |\n",
            "| clipfrac           | 0.13122559   |\n",
            "| ep_len_mean        | 18.1         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.931        |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 490          |\n",
            "| policy_entropy     | 0.3271758    |\n",
            "| policy_loss        | -0.039070077 |\n",
            "| serial_timesteps   | 250880       |\n",
            "| time_elapsed       | 5.43e+03     |\n",
            "| total_timesteps    | 2007040      |\n",
            "| value_loss         | 0.0403058    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2010000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.80 +/- 1.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023380613  |\n",
            "| clipfrac           | 0.09794922   |\n",
            "| ep_len_mean        | 17.1         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.928        |\n",
            "| fps                | 398          |\n",
            "| n_updates          | 491          |\n",
            "| policy_entropy     | 0.24591622   |\n",
            "| policy_loss        | -0.030932035 |\n",
            "| serial_timesteps   | 251392       |\n",
            "| time_elapsed       | 5.44e+03     |\n",
            "| total_timesteps    | 2011136      |\n",
            "| value_loss         | 0.03560015   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018883636  |\n",
            "| clipfrac           | 0.10585938   |\n",
            "| ep_len_mean        | 22.6         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.945        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 492          |\n",
            "| policy_entropy     | 0.3230899    |\n",
            "| policy_loss        | -0.030263608 |\n",
            "| serial_timesteps   | 251904       |\n",
            "| time_elapsed       | 5.45e+03     |\n",
            "| total_timesteps    | 2015232      |\n",
            "| value_loss         | 0.05061365   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.02551707  |\n",
            "| clipfrac           | 0.14992675  |\n",
            "| ep_len_mean        | 25.9        |\n",
            "| ep_reward_mean     | 0.927       |\n",
            "| explained_variance | 0.943       |\n",
            "| fps                | 419         |\n",
            "| n_updates          | 493         |\n",
            "| policy_entropy     | 0.45685783  |\n",
            "| policy_loss        | -0.03306001 |\n",
            "| serial_timesteps   | 252416      |\n",
            "| time_elapsed       | 5.46e+03    |\n",
            "| total_timesteps    | 2019328     |\n",
            "| value_loss         | 0.060938895 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2020000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 19.40 +/- 4.84\n",
            "-------------------------------------\n",
            "| approxkl           | 1.4108832    |\n",
            "| clipfrac           | 0.17578125   |\n",
            "| ep_len_mean        | 21.8         |\n",
            "| ep_reward_mean     | 0.939        |\n",
            "| explained_variance | 0.891        |\n",
            "| fps                | 405          |\n",
            "| n_updates          | 494          |\n",
            "| policy_entropy     | 0.34303695   |\n",
            "| policy_loss        | -0.052591126 |\n",
            "| serial_timesteps   | 252928       |\n",
            "| time_elapsed       | 5.47e+03     |\n",
            "| total_timesteps    | 2023424      |\n",
            "| value_loss         | 0.12278651   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.029174035 |\n",
            "| clipfrac           | 0.19301757  |\n",
            "| ep_len_mean        | 26.9        |\n",
            "| ep_reward_mean     | 0.923       |\n",
            "| explained_variance | 0.975       |\n",
            "| fps                | 423         |\n",
            "| n_updates          | 495         |\n",
            "| policy_entropy     | 0.5672678   |\n",
            "| policy_loss        | -0.04067945 |\n",
            "| serial_timesteps   | 253440      |\n",
            "| time_elapsed       | 5.48e+03    |\n",
            "| total_timesteps    | 2027520     |\n",
            "| value_loss         | 0.034999434 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2030000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 77.20 +/- 123.41\n",
            "-------------------------------------\n",
            "| approxkl           | 2.494794     |\n",
            "| clipfrac           | 0.21367188   |\n",
            "| ep_len_mean        | 31.1         |\n",
            "| ep_reward_mean     | 0.911        |\n",
            "| explained_variance | 0.914        |\n",
            "| fps                | 364          |\n",
            "| n_updates          | 496          |\n",
            "| policy_entropy     | 0.6145811    |\n",
            "| policy_loss        | -0.047519002 |\n",
            "| serial_timesteps   | 253952       |\n",
            "| time_elapsed       | 5.49e+03     |\n",
            "| total_timesteps    | 2031616      |\n",
            "| value_loss         | 0.14033218   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.02786541  |\n",
            "| clipfrac           | 0.1498291   |\n",
            "| ep_len_mean        | 19.8        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.956       |\n",
            "| fps                | 421         |\n",
            "| n_updates          | 497         |\n",
            "| policy_entropy     | 0.34417823  |\n",
            "| policy_loss        | -0.0394895  |\n",
            "| serial_timesteps   | 254464      |\n",
            "| time_elapsed       | 5.5e+03     |\n",
            "| total_timesteps    | 2035712     |\n",
            "| value_loss         | 0.028861621 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016247092  |\n",
            "| clipfrac           | 0.13344726   |\n",
            "| ep_len_mean        | 18.8         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.975        |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 498          |\n",
            "| policy_entropy     | 0.42765814   |\n",
            "| policy_loss        | -0.040945154 |\n",
            "| serial_timesteps   | 254976       |\n",
            "| time_elapsed       | 5.51e+03     |\n",
            "| total_timesteps    | 2039808      |\n",
            "| value_loss         | 0.032375194  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2040000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 0.80\n",
            "------------------------------------\n",
            "| approxkl           | 0.031058874 |\n",
            "| clipfrac           | 0.10336914  |\n",
            "| ep_len_mean        | 17.9        |\n",
            "| ep_reward_mean     | 0.95        |\n",
            "| explained_variance | 0.969       |\n",
            "| fps                | 410         |\n",
            "| n_updates          | 499         |\n",
            "| policy_entropy     | 0.27824217  |\n",
            "| policy_loss        | -0.03287097 |\n",
            "| serial_timesteps   | 255488      |\n",
            "| time_elapsed       | 5.52e+03    |\n",
            "| total_timesteps    | 2043904     |\n",
            "| value_loss         | 0.021612149 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.9491153    |\n",
            "| clipfrac           | 0.17060547   |\n",
            "| ep_len_mean        | 22.6         |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.945        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 500          |\n",
            "| policy_entropy     | 0.35159463   |\n",
            "| policy_loss        | -0.041824125 |\n",
            "| serial_timesteps   | 256000       |\n",
            "| time_elapsed       | 5.53e+03     |\n",
            "| total_timesteps    | 2048000      |\n",
            "| value_loss         | 0.055088263  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2050000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 1.02\n",
            "------------------------------------\n",
            "| approxkl           | 0.1324782   |\n",
            "| clipfrac           | 0.1373291   |\n",
            "| ep_len_mean        | 18.1        |\n",
            "| ep_reward_mean     | 0.95        |\n",
            "| explained_variance | 0.957       |\n",
            "| fps                | 408         |\n",
            "| n_updates          | 501         |\n",
            "| policy_entropy     | 0.40757784  |\n",
            "| policy_loss        | 0.0784912   |\n",
            "| serial_timesteps   | 256512      |\n",
            "| time_elapsed       | 5.54e+03    |\n",
            "| total_timesteps    | 2052096     |\n",
            "| value_loss         | 0.062112816 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.074851155  |\n",
            "| clipfrac           | 0.18066406   |\n",
            "| ep_len_mean        | 23.5         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.915        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 502          |\n",
            "| policy_entropy     | 0.39674926   |\n",
            "| policy_loss        | -0.024736926 |\n",
            "| serial_timesteps   | 257024       |\n",
            "| time_elapsed       | 5.55e+03     |\n",
            "| total_timesteps    | 2056192      |\n",
            "| value_loss         | 0.116671726  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2060000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.00 +/- 2.10\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026209727  |\n",
            "| clipfrac           | 0.1460205    |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.955        |\n",
            "| fps                | 407          |\n",
            "| n_updates          | 503          |\n",
            "| policy_entropy     | 0.34061632   |\n",
            "| policy_loss        | -0.038934767 |\n",
            "| serial_timesteps   | 257536       |\n",
            "| time_elapsed       | 5.56e+03     |\n",
            "| total_timesteps    | 2060288      |\n",
            "| value_loss         | 0.028436828  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0154042365 |\n",
            "| clipfrac           | 0.12077637   |\n",
            "| ep_len_mean        | 21.5         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.952        |\n",
            "| fps                | 425          |\n",
            "| n_updates          | 504          |\n",
            "| policy_entropy     | 0.32621127   |\n",
            "| policy_loss        | -0.03571735  |\n",
            "| serial_timesteps   | 258048       |\n",
            "| time_elapsed       | 5.57e+03     |\n",
            "| total_timesteps    | 2064384      |\n",
            "| value_loss         | 0.038398143  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.19802897  |\n",
            "| clipfrac           | 0.10175781  |\n",
            "| ep_len_mean        | 16.9        |\n",
            "| ep_reward_mean     | 0.953       |\n",
            "| explained_variance | 0.951       |\n",
            "| fps                | 416         |\n",
            "| n_updates          | 505         |\n",
            "| policy_entropy     | 0.18017623  |\n",
            "| policy_loss        | -0.02920948 |\n",
            "| serial_timesteps   | 258560      |\n",
            "| time_elapsed       | 5.58e+03    |\n",
            "| total_timesteps    | 2068480     |\n",
            "| value_loss         | 0.01784483  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2070000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 262.40 +/- 123.20\n",
            "-----------------------------------\n",
            "| approxkl           | 3.5272117  |\n",
            "| clipfrac           | 0.41013184 |\n",
            "| ep_len_mean        | 38.5       |\n",
            "| ep_reward_mean     | 0.888      |\n",
            "| explained_variance | 0.597      |\n",
            "| fps                | 285        |\n",
            "| n_updates          | 506        |\n",
            "| policy_entropy     | 0.33485597 |\n",
            "| policy_loss        | 0.06850989 |\n",
            "| serial_timesteps   | 259072     |\n",
            "| time_elapsed       | 5.59e+03   |\n",
            "| total_timesteps    | 2072576    |\n",
            "| value_loss         | 0.5169648  |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| approxkl           | 0.5818252  |\n",
            "| clipfrac           | 0.37924805 |\n",
            "| ep_len_mean        | 45.5       |\n",
            "| ep_reward_mean     | 0.868      |\n",
            "| explained_variance | 0.71       |\n",
            "| fps                | 426        |\n",
            "| n_updates          | 507        |\n",
            "| policy_entropy     | 0.45310283 |\n",
            "| policy_loss        | -0.0197618 |\n",
            "| serial_timesteps   | 259584     |\n",
            "| time_elapsed       | 5.6e+03    |\n",
            "| total_timesteps    | 2076672    |\n",
            "| value_loss         | 0.3399825  |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=2080000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.80 +/- 122.18\n",
            "-------------------------------------\n",
            "| approxkl           | 0.34900752   |\n",
            "| clipfrac           | 0.14985351   |\n",
            "| ep_len_mean        | 18.9         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.783        |\n",
            "| fps                | 364          |\n",
            "| n_updates          | 508          |\n",
            "| policy_entropy     | 0.2835202    |\n",
            "| policy_loss        | -0.030037826 |\n",
            "| serial_timesteps   | 260096       |\n",
            "| time_elapsed       | 5.61e+03     |\n",
            "| total_timesteps    | 2080768      |\n",
            "| value_loss         | 0.20364377   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.08528353  |\n",
            "| clipfrac           | 0.16743164  |\n",
            "| ep_len_mean        | 19.8        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.891       |\n",
            "| fps                | 421         |\n",
            "| n_updates          | 509         |\n",
            "| policy_entropy     | 0.29379666  |\n",
            "| policy_loss        | -0.03550934 |\n",
            "| serial_timesteps   | 260608      |\n",
            "| time_elapsed       | 5.62e+03    |\n",
            "| total_timesteps    | 2084864     |\n",
            "| value_loss         | 0.06548475  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.1145847    |\n",
            "| clipfrac           | 0.15690918   |\n",
            "| ep_len_mean        | 22.3         |\n",
            "| ep_reward_mean     | 0.938        |\n",
            "| explained_variance | 0.736        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 510          |\n",
            "| policy_entropy     | 0.21377453   |\n",
            "| policy_loss        | -0.034825664 |\n",
            "| serial_timesteps   | 261120       |\n",
            "| time_elapsed       | 5.63e+03     |\n",
            "| total_timesteps    | 2088960      |\n",
            "| value_loss         | 0.07708982   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2090000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.20 +/- 2.56\n",
            "-------------------------------------\n",
            "| approxkl           | 0.20956942   |\n",
            "| clipfrac           | 0.21960449   |\n",
            "| ep_len_mean        | 26.5         |\n",
            "| ep_reward_mean     | 0.923        |\n",
            "| explained_variance | 0.684        |\n",
            "| fps                | 405          |\n",
            "| n_updates          | 511          |\n",
            "| policy_entropy     | 0.35860184   |\n",
            "| policy_loss        | -0.040839713 |\n",
            "| serial_timesteps   | 261632       |\n",
            "| time_elapsed       | 5.64e+03     |\n",
            "| total_timesteps    | 2093056      |\n",
            "| value_loss         | 0.2270194    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.109324455 |\n",
            "| clipfrac           | 0.13442382  |\n",
            "| ep_len_mean        | 19.5        |\n",
            "| ep_reward_mean     | 0.946       |\n",
            "| explained_variance | 0.775       |\n",
            "| fps                | 420         |\n",
            "| n_updates          | 512         |\n",
            "| policy_entropy     | 0.18667904  |\n",
            "| policy_loss        | 0.046871345 |\n",
            "| serial_timesteps   | 262144      |\n",
            "| time_elapsed       | 5.65e+03    |\n",
            "| total_timesteps    | 2097152     |\n",
            "| value_loss         | 0.07830514  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2100000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.80 +/- 123.61\n",
            "-----------------------------------\n",
            "| approxkl           | 0.39404124 |\n",
            "| clipfrac           | 0.18803711 |\n",
            "| ep_len_mean        | 20.8       |\n",
            "| ep_reward_mean     | 0.941      |\n",
            "| explained_variance | 0.746      |\n",
            "| fps                | 363        |\n",
            "| n_updates          | 513        |\n",
            "| policy_entropy     | 0.26589125 |\n",
            "| policy_loss        | 0.0092266  |\n",
            "| serial_timesteps   | 262656     |\n",
            "| time_elapsed       | 5.66e+03   |\n",
            "| total_timesteps    | 2101248    |\n",
            "| value_loss         | 0.1436314  |\n",
            "-----------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.03513462  |\n",
            "| clipfrac           | 0.1605957   |\n",
            "| ep_len_mean        | 22          |\n",
            "| ep_reward_mean     | 0.938       |\n",
            "| explained_variance | 0.886       |\n",
            "| fps                | 419         |\n",
            "| n_updates          | 514         |\n",
            "| policy_entropy     | 0.35921735  |\n",
            "| policy_loss        | -0.04026066 |\n",
            "| serial_timesteps   | 263168      |\n",
            "| time_elapsed       | 5.67e+03    |\n",
            "| total_timesteps    | 2105344     |\n",
            "| value_loss         | 0.071433626 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.034196906  |\n",
            "| clipfrac           | 0.12250976   |\n",
            "| ep_len_mean        | 21.2         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.929        |\n",
            "| fps                | 420          |\n",
            "| n_updates          | 515          |\n",
            "| policy_entropy     | 0.26547235   |\n",
            "| policy_loss        | -0.034326453 |\n",
            "| serial_timesteps   | 263680       |\n",
            "| time_elapsed       | 5.68e+03     |\n",
            "| total_timesteps    | 2109440      |\n",
            "| value_loss         | 0.053267457  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2110000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.40 +/- 1.85\n",
            "-----------------------------------\n",
            "| approxkl           | 0.09828314 |\n",
            "| clipfrac           | 0.16335449 |\n",
            "| ep_len_mean        | 27.4       |\n",
            "| ep_reward_mean     | 0.921      |\n",
            "| explained_variance | 0.914      |\n",
            "| fps                | 408        |\n",
            "| n_updates          | 516        |\n",
            "| policy_entropy     | 0.40579724 |\n",
            "| policy_loss        | -0.0288207 |\n",
            "| serial_timesteps   | 264192     |\n",
            "| time_elapsed       | 5.69e+03   |\n",
            "| total_timesteps    | 2113536    |\n",
            "| value_loss         | 0.09632142 |\n",
            "-----------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.022205867 |\n",
            "| clipfrac           | 0.1026123   |\n",
            "| ep_len_mean        | 21.2        |\n",
            "| ep_reward_mean     | 0.94        |\n",
            "| explained_variance | 0.94        |\n",
            "| fps                | 421         |\n",
            "| n_updates          | 517         |\n",
            "| policy_entropy     | 0.288098    |\n",
            "| policy_loss        | -0.0226577  |\n",
            "| serial_timesteps   | 264704      |\n",
            "| time_elapsed       | 5.7e+03     |\n",
            "| total_timesteps    | 2117632     |\n",
            "| value_loss         | 0.044192336 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2120000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 16.40 +/- 1.62\n",
            "-------------------------------------\n",
            "| approxkl           | 0.11111691   |\n",
            "| clipfrac           | 0.11904297   |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.918        |\n",
            "| fps                | 409          |\n",
            "| n_updates          | 518          |\n",
            "| policy_entropy     | 0.24879861   |\n",
            "| policy_loss        | -0.030457955 |\n",
            "| serial_timesteps   | 265216       |\n",
            "| time_elapsed       | 5.71e+03     |\n",
            "| total_timesteps    | 2121728      |\n",
            "| value_loss         | 0.056137167  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031758852  |\n",
            "| clipfrac           | 0.036474608  |\n",
            "| ep_len_mean        | 16.5         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.882        |\n",
            "| fps                | 411          |\n",
            "| n_updates          | 519          |\n",
            "| policy_entropy     | 0.046893213  |\n",
            "| policy_loss        | -0.013183462 |\n",
            "| serial_timesteps   | 265728       |\n",
            "| time_elapsed       | 5.72e+03     |\n",
            "| total_timesteps    | 2125824      |\n",
            "| value_loss         | 0.009410223  |\n",
            "-------------------------------------\n",
            "-----------------------------------\n",
            "| approxkl           | 2.381956   |\n",
            "| clipfrac           | 0.26672363 |\n",
            "| ep_len_mean        | 23.6       |\n",
            "| ep_reward_mean     | 0.932      |\n",
            "| explained_variance | 0.491      |\n",
            "| fps                | 433        |\n",
            "| n_updates          | 520        |\n",
            "| policy_entropy     | 0.08814953 |\n",
            "| policy_loss        | -0.0626712 |\n",
            "| serial_timesteps   | 266240     |\n",
            "| time_elapsed       | 5.73e+03   |\n",
            "| total_timesteps    | 2129920    |\n",
            "| value_loss         | 0.18077728 |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=2130000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.60 +/- 2.33\n",
            "------------------------------------\n",
            "| approxkl           | 0.44741136  |\n",
            "| clipfrac           | 0.13439941  |\n",
            "| ep_len_mean        | 19          |\n",
            "| ep_reward_mean     | 0.947       |\n",
            "| explained_variance | 0.37        |\n",
            "| fps                | 414         |\n",
            "| n_updates          | 521         |\n",
            "| policy_entropy     | 0.081464656 |\n",
            "| policy_loss        | 0.007042624 |\n",
            "| serial_timesteps   | 266752      |\n",
            "| time_elapsed       | 5.74e+03    |\n",
            "| total_timesteps    | 2134016     |\n",
            "| value_loss         | 0.15224288  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.09093954   |\n",
            "| clipfrac           | 0.03125      |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.93         |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 522          |\n",
            "| policy_entropy     | 0.024501737  |\n",
            "| policy_loss        | -0.006621825 |\n",
            "| serial_timesteps   | 267264       |\n",
            "| time_elapsed       | 5.75e+03     |\n",
            "| total_timesteps    | 2138112      |\n",
            "| value_loss         | 0.0024740584 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2140000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.20 +/- 2.32\n",
            "-------------------------------------\n",
            "| approxkl           | 0.18393132   |\n",
            "| clipfrac           | 0.44482422   |\n",
            "| ep_len_mean        | 34.2         |\n",
            "| ep_reward_mean     | 0.902        |\n",
            "| explained_variance | -0.218       |\n",
            "| fps                | 397          |\n",
            "| n_updates          | 523          |\n",
            "| policy_entropy     | 0.41190273   |\n",
            "| policy_loss        | -0.018501878 |\n",
            "| serial_timesteps   | 267776       |\n",
            "| time_elapsed       | 5.76e+03     |\n",
            "| total_timesteps    | 2142208      |\n",
            "| value_loss         | 0.51797765   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.7060565    |\n",
            "| clipfrac           | 0.3512207    |\n",
            "| ep_len_mean        | 35.3         |\n",
            "| ep_reward_mean     | 0.899        |\n",
            "| explained_variance | 0.142        |\n",
            "| fps                | 405          |\n",
            "| n_updates          | 524          |\n",
            "| policy_entropy     | 0.45970923   |\n",
            "| policy_loss        | -0.026766678 |\n",
            "| serial_timesteps   | 268288       |\n",
            "| time_elapsed       | 5.77e+03     |\n",
            "| total_timesteps    | 2146304      |\n",
            "| value_loss         | 0.4222061    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2150000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.60 +/- 1.96\n",
            "-------------------------------------\n",
            "| approxkl           | 0.70636034   |\n",
            "| clipfrac           | 0.21049805   |\n",
            "| ep_len_mean        | 24           |\n",
            "| ep_reward_mean     | 0.932        |\n",
            "| explained_variance | 0.553        |\n",
            "| fps                | 407          |\n",
            "| n_updates          | 525          |\n",
            "| policy_entropy     | 0.17545517   |\n",
            "| policy_loss        | -0.034685306 |\n",
            "| serial_timesteps   | 268800       |\n",
            "| time_elapsed       | 5.78e+03     |\n",
            "| total_timesteps    | 2150400      |\n",
            "| value_loss         | 0.18801571   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.09581043   |\n",
            "| clipfrac           | 0.20202637   |\n",
            "| ep_len_mean        | 22.7         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.766        |\n",
            "| fps                | 426          |\n",
            "| n_updates          | 526          |\n",
            "| policy_entropy     | 0.33049557   |\n",
            "| policy_loss        | -0.044697102 |\n",
            "| serial_timesteps   | 269312       |\n",
            "| time_elapsed       | 5.79e+03     |\n",
            "| total_timesteps    | 2154496      |\n",
            "| value_loss         | 0.11838454   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.033608824  |\n",
            "| clipfrac           | 0.094897464  |\n",
            "| ep_len_mean        | 17.9         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.878        |\n",
            "| fps                | 427          |\n",
            "| n_updates          | 527          |\n",
            "| policy_entropy     | 0.17155485   |\n",
            "| policy_loss        | -0.027526712 |\n",
            "| serial_timesteps   | 269824       |\n",
            "| time_elapsed       | 5.8e+03      |\n",
            "| total_timesteps    | 2158592      |\n",
            "| value_loss         | 0.01918681   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2160000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.20 +/- 123.91\n",
            "-------------------------------------\n",
            "| approxkl           | 0.058467854  |\n",
            "| clipfrac           | 0.08828125   |\n",
            "| ep_len_mean        | 18.6         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.879        |\n",
            "| fps                | 371          |\n",
            "| n_updates          | 528          |\n",
            "| policy_entropy     | 0.13588242   |\n",
            "| policy_loss        | -0.029534971 |\n",
            "| serial_timesteps   | 270336       |\n",
            "| time_elapsed       | 5.81e+03     |\n",
            "| total_timesteps    | 2162688      |\n",
            "| value_loss         | 0.015006979  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.09855052   |\n",
            "| clipfrac           | 0.1864502    |\n",
            "| ep_len_mean        | 18.1         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.831        |\n",
            "| fps                | 426          |\n",
            "| n_updates          | 529          |\n",
            "| policy_entropy     | 0.27130705   |\n",
            "| policy_loss        | -0.037567366 |\n",
            "| serial_timesteps   | 270848       |\n",
            "| time_elapsed       | 5.82e+03     |\n",
            "| total_timesteps    | 2166784      |\n",
            "| value_loss         | 0.050702255  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2170000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.60 +/- 122.23\n",
            "-------------------------------------\n",
            "| approxkl           | 0.11480285   |\n",
            "| clipfrac           | 0.13078614   |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.624        |\n",
            "| fps                | 364          |\n",
            "| n_updates          | 530          |\n",
            "| policy_entropy     | 0.21820183   |\n",
            "| policy_loss        | -0.035228323 |\n",
            "| serial_timesteps   | 271360       |\n",
            "| time_elapsed       | 5.83e+03     |\n",
            "| total_timesteps    | 2170880      |\n",
            "| value_loss         | 0.13319853   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.039532978  |\n",
            "| clipfrac           | 0.15939942   |\n",
            "| ep_len_mean        | 24.7         |\n",
            "| ep_reward_mean     | 0.93         |\n",
            "| explained_variance | 0.736        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 531          |\n",
            "| policy_entropy     | 0.31829184   |\n",
            "| policy_loss        | -0.037215583 |\n",
            "| serial_timesteps   | 271872       |\n",
            "| time_elapsed       | 5.84e+03     |\n",
            "| total_timesteps    | 2174976      |\n",
            "| value_loss         | 0.09236865   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04376337   |\n",
            "| clipfrac           | 0.11271973   |\n",
            "| ep_len_mean        | 19.7         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.751        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 532          |\n",
            "| policy_entropy     | 0.20482889   |\n",
            "| policy_loss        | -0.031741727 |\n",
            "| serial_timesteps   | 272384       |\n",
            "| time_elapsed       | 5.85e+03     |\n",
            "| total_timesteps    | 2179072      |\n",
            "| value_loss         | 0.047046516  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2180000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 17.20 +/- 1.47\n",
            "------------------------------------\n",
            "| approxkl           | 0.10295564  |\n",
            "| clipfrac           | 0.16928712  |\n",
            "| ep_len_mean        | 23.9        |\n",
            "| ep_reward_mean     | 0.933       |\n",
            "| explained_variance | 0.822       |\n",
            "| fps                | 410         |\n",
            "| n_updates          | 533         |\n",
            "| policy_entropy     | 0.27128524  |\n",
            "| policy_loss        | -0.01586574 |\n",
            "| serial_timesteps   | 272896      |\n",
            "| time_elapsed       | 5.86e+03    |\n",
            "| total_timesteps    | 2183168     |\n",
            "| value_loss         | 0.074589826 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02791747   |\n",
            "| clipfrac           | 0.097509764  |\n",
            "| ep_len_mean        | 19.9         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.902        |\n",
            "| fps                | 430          |\n",
            "| n_updates          | 534          |\n",
            "| policy_entropy     | 0.19994448   |\n",
            "| policy_loss        | -0.028498542 |\n",
            "| serial_timesteps   | 273408       |\n",
            "| time_elapsed       | 5.87e+03     |\n",
            "| total_timesteps    | 2187264      |\n",
            "| value_loss         | 0.023497924  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2190000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.40 +/- 4.76\n",
            "------------------------------------\n",
            "| approxkl           | 0.56582034  |\n",
            "| clipfrac           | 0.16608886  |\n",
            "| ep_len_mean        | 22.7        |\n",
            "| ep_reward_mean     | 0.936       |\n",
            "| explained_variance | 0.68        |\n",
            "| fps                | 412         |\n",
            "| n_updates          | 535         |\n",
            "| policy_entropy     | 0.24080649  |\n",
            "| policy_loss        | -0.04517749 |\n",
            "| serial_timesteps   | 273920      |\n",
            "| time_elapsed       | 5.88e+03    |\n",
            "| total_timesteps    | 2191360     |\n",
            "| value_loss         | 0.09729402  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023341125  |\n",
            "| clipfrac           | 0.11706543   |\n",
            "| ep_len_mean        | 19.2         |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.937        |\n",
            "| fps                | 425          |\n",
            "| n_updates          | 536          |\n",
            "| policy_entropy     | 0.25835854   |\n",
            "| policy_loss        | -0.026008382 |\n",
            "| serial_timesteps   | 274432       |\n",
            "| time_elapsed       | 5.89e+03     |\n",
            "| total_timesteps    | 2195456      |\n",
            "| value_loss         | 0.021796938  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04214852   |\n",
            "| clipfrac           | 0.06948242   |\n",
            "| ep_len_mean        | 18.2         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.82         |\n",
            "| fps                | 425          |\n",
            "| n_updates          | 537          |\n",
            "| policy_entropy     | 0.11007675   |\n",
            "| policy_loss        | -0.027030125 |\n",
            "| serial_timesteps   | 274944       |\n",
            "| time_elapsed       | 5.9e+03      |\n",
            "| total_timesteps    | 2199552      |\n",
            "| value_loss         | 0.034769136  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2200000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.60 +/- 122.73\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03581714   |\n",
            "| clipfrac           | 0.11166992   |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.821        |\n",
            "| fps                | 367          |\n",
            "| n_updates          | 538          |\n",
            "| policy_entropy     | 0.1881083    |\n",
            "| policy_loss        | -0.035867434 |\n",
            "| serial_timesteps   | 275456       |\n",
            "| time_elapsed       | 5.91e+03     |\n",
            "| total_timesteps    | 2203648      |\n",
            "| value_loss         | 0.030039132  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.038441658  |\n",
            "| clipfrac           | 0.07932129   |\n",
            "| ep_len_mean        | 17           |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.876        |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 539          |\n",
            "| policy_entropy     | 0.13640901   |\n",
            "| policy_loss        | -0.019867767 |\n",
            "| serial_timesteps   | 275968       |\n",
            "| time_elapsed       | 5.92e+03     |\n",
            "| total_timesteps    | 2207744      |\n",
            "| value_loss         | 0.013616011  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2210000, episode_reward=0.75 +/- 0.38\n",
            "Episode length: 81.00 +/- 121.67\n",
            "-------------------------------------\n",
            "| approxkl           | 0.07626824   |\n",
            "| clipfrac           | 0.14260253   |\n",
            "| ep_len_mean        | 23.3         |\n",
            "| ep_reward_mean     | 0.935        |\n",
            "| explained_variance | 0.79         |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 540          |\n",
            "| policy_entropy     | 0.25082323   |\n",
            "| policy_loss        | -0.019957384 |\n",
            "| serial_timesteps   | 276480       |\n",
            "| time_elapsed       | 5.93e+03     |\n",
            "| total_timesteps    | 2211840      |\n",
            "| value_loss         | 0.04867989   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.23071933  |\n",
            "| clipfrac           | 0.20778808  |\n",
            "| ep_len_mean        | 25.3        |\n",
            "| ep_reward_mean     | 0.928       |\n",
            "| explained_variance | 0.617       |\n",
            "| fps                | 422         |\n",
            "| n_updates          | 541         |\n",
            "| policy_entropy     | 0.30244252  |\n",
            "| policy_loss        | 0.006574802 |\n",
            "| serial_timesteps   | 276992      |\n",
            "| time_elapsed       | 5.94e+03    |\n",
            "| total_timesteps    | 2215936     |\n",
            "| value_loss         | 0.19488446  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2220000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.20 +/- 123.91\n",
            "------------------------------------\n",
            "| approxkl           | 0.10869098  |\n",
            "| clipfrac           | 0.16467285  |\n",
            "| ep_len_mean        | 23.7        |\n",
            "| ep_reward_mean     | 0.934       |\n",
            "| explained_variance | 0.844       |\n",
            "| fps                | 367         |\n",
            "| n_updates          | 542         |\n",
            "| policy_entropy     | 0.26497984  |\n",
            "| policy_loss        | -0.04335691 |\n",
            "| serial_timesteps   | 277504      |\n",
            "| time_elapsed       | 5.95e+03    |\n",
            "| total_timesteps    | 2220032     |\n",
            "| value_loss         | 0.065249994 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.020083215 |\n",
            "| clipfrac           | 0.086523436 |\n",
            "| ep_len_mean        | 20.1        |\n",
            "| ep_reward_mean     | 0.944       |\n",
            "| explained_variance | 0.881       |\n",
            "| fps                | 422         |\n",
            "| n_updates          | 543         |\n",
            "| policy_entropy     | 0.18131733  |\n",
            "| policy_loss        | -0.028241   |\n",
            "| serial_timesteps   | 278016      |\n",
            "| time_elapsed       | 5.96e+03    |\n",
            "| total_timesteps    | 2224128     |\n",
            "| value_loss         | 0.025881797 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.47403517  |\n",
            "| clipfrac           | 0.14650878  |\n",
            "| ep_len_mean        | 20.2        |\n",
            "| ep_reward_mean     | 0.943       |\n",
            "| explained_variance | 0.442       |\n",
            "| fps                | 422         |\n",
            "| n_updates          | 544         |\n",
            "| policy_entropy     | 0.097091004 |\n",
            "| policy_loss        | -0.04720702 |\n",
            "| serial_timesteps   | 278528      |\n",
            "| time_elapsed       | 5.97e+03    |\n",
            "| total_timesteps    | 2228224     |\n",
            "| value_loss         | 0.1177161   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2230000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.80 +/- 123.61\n",
            "------------------------------------\n",
            "| approxkl           | 0.12583324  |\n",
            "| clipfrac           | 0.06367187  |\n",
            "| ep_len_mean        | 19.5        |\n",
            "| ep_reward_mean     | 0.946       |\n",
            "| explained_variance | 0.554       |\n",
            "| fps                | 372         |\n",
            "| n_updates          | 545         |\n",
            "| policy_entropy     | 0.057014517 |\n",
            "| policy_loss        | 0.2047728   |\n",
            "| serial_timesteps   | 279040      |\n",
            "| time_elapsed       | 5.98e+03    |\n",
            "| total_timesteps    | 2232320     |\n",
            "| value_loss         | 0.0986933   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.054787286  |\n",
            "| clipfrac           | 0.1237793    |\n",
            "| ep_len_mean        | 19.9         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.739        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 546          |\n",
            "| policy_entropy     | 0.19701779   |\n",
            "| policy_loss        | -0.027374815 |\n",
            "| serial_timesteps   | 279552       |\n",
            "| time_elapsed       | 5.99e+03     |\n",
            "| total_timesteps    | 2236416      |\n",
            "| value_loss         | 0.038638704  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2240000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 16.40 +/- 1.02\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04524889   |\n",
            "| clipfrac           | 0.16611329   |\n",
            "| ep_len_mean        | 18.3         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.817        |\n",
            "| fps                | 401          |\n",
            "| n_updates          | 547          |\n",
            "| policy_entropy     | 0.32407004   |\n",
            "| policy_loss        | -0.041602366 |\n",
            "| serial_timesteps   | 280064       |\n",
            "| time_elapsed       | 6e+03        |\n",
            "| total_timesteps    | 2240512      |\n",
            "| value_loss         | 0.045304257  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.1840649   |\n",
            "| clipfrac           | 0.09140625  |\n",
            "| ep_len_mean        | 20.9        |\n",
            "| ep_reward_mean     | 0.941       |\n",
            "| explained_variance | 0.538       |\n",
            "| fps                | 423         |\n",
            "| n_updates          | 548         |\n",
            "| policy_entropy     | 0.16240978  |\n",
            "| policy_loss        | -0.02170984 |\n",
            "| serial_timesteps   | 280576      |\n",
            "| time_elapsed       | 6.01e+03    |\n",
            "| total_timesteps    | 2244608     |\n",
            "| value_loss         | 0.14751476  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.021179628 |\n",
            "| clipfrac           | 0.06633301  |\n",
            "| ep_len_mean        | 19.2        |\n",
            "| ep_reward_mean     | 0.947       |\n",
            "| explained_variance | 0.858       |\n",
            "| fps                | 419         |\n",
            "| n_updates          | 549         |\n",
            "| policy_entropy     | 0.1336712   |\n",
            "| policy_loss        | -0.02589346 |\n",
            "| serial_timesteps   | 281088      |\n",
            "| time_elapsed       | 6.02e+03    |\n",
            "| total_timesteps    | 2248704     |\n",
            "| value_loss         | 0.013769838 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2250000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.80 +/- 1.47\n",
            "-------------------------------------\n",
            "| approxkl           | 0.6990412    |\n",
            "| clipfrac           | 0.16223145   |\n",
            "| ep_len_mean        | 23.7         |\n",
            "| ep_reward_mean     | 0.933        |\n",
            "| explained_variance | 0.671        |\n",
            "| fps                | 402          |\n",
            "| n_updates          | 550          |\n",
            "| policy_entropy     | 0.2262573    |\n",
            "| policy_loss        | -0.044038117 |\n",
            "| serial_timesteps   | 281600       |\n",
            "| time_elapsed       | 6.03e+03     |\n",
            "| total_timesteps    | 2252800      |\n",
            "| value_loss         | 0.11505749   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.16424951  |\n",
            "| clipfrac           | 0.20107421  |\n",
            "| ep_len_mean        | 26.6        |\n",
            "| ep_reward_mean     | 0.923       |\n",
            "| explained_variance | 0.822       |\n",
            "| fps                | 423         |\n",
            "| n_updates          | 551         |\n",
            "| policy_entropy     | 0.40960556  |\n",
            "| policy_loss        | -0.03025623 |\n",
            "| serial_timesteps   | 282112      |\n",
            "| time_elapsed       | 6.04e+03    |\n",
            "| total_timesteps    | 2256896     |\n",
            "| value_loss         | 0.1274908   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2260000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.40 +/- 4.18\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023737062  |\n",
            "| clipfrac           | 0.09865723   |\n",
            "| ep_len_mean        | 19.6         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.851        |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 552          |\n",
            "| policy_entropy     | 0.2054528    |\n",
            "| policy_loss        | -0.031210814 |\n",
            "| serial_timesteps   | 282624       |\n",
            "| time_elapsed       | 6.05e+03     |\n",
            "| total_timesteps    | 2260992      |\n",
            "| value_loss         | 0.035007734  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031997286  |\n",
            "| clipfrac           | 0.13391113   |\n",
            "| ep_len_mean        | 20.1         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.858        |\n",
            "| fps                | 413          |\n",
            "| n_updates          | 553          |\n",
            "| policy_entropy     | 0.28577885   |\n",
            "| policy_loss        | -0.031796493 |\n",
            "| serial_timesteps   | 283136       |\n",
            "| time_elapsed       | 6.06e+03     |\n",
            "| total_timesteps    | 2265088      |\n",
            "| value_loss         | 0.06276815   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.022303453   |\n",
            "| clipfrac           | 0.072070315   |\n",
            "| ep_len_mean        | 18.6          |\n",
            "| ep_reward_mean     | 0.948         |\n",
            "| explained_variance | 0.858         |\n",
            "| fps                | 418           |\n",
            "| n_updates          | 554           |\n",
            "| policy_entropy     | 0.13634294    |\n",
            "| policy_loss        | -0.0067363023 |\n",
            "| serial_timesteps   | 283648        |\n",
            "| time_elapsed       | 6.07e+03      |\n",
            "| total_timesteps    | 2269184       |\n",
            "| value_loss         | 0.022848075   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2270000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.00 +/- 5.06\n",
            "------------------------------------\n",
            "| approxkl           | 0.104975246 |\n",
            "| clipfrac           | 0.08669434  |\n",
            "| ep_len_mean        | 17.9        |\n",
            "| ep_reward_mean     | 0.95        |\n",
            "| explained_variance | 0.808       |\n",
            "| fps                | 401         |\n",
            "| n_updates          | 555         |\n",
            "| policy_entropy     | 0.09829316  |\n",
            "| policy_loss        | -0.03100487 |\n",
            "| serial_timesteps   | 284160      |\n",
            "| time_elapsed       | 6.08e+03    |\n",
            "| total_timesteps    | 2273280     |\n",
            "| value_loss         | 0.022732688 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.5916869   |\n",
            "| clipfrac           | 0.12927246  |\n",
            "| ep_len_mean        | 18.6        |\n",
            "| ep_reward_mean     | 0.948       |\n",
            "| explained_variance | 0.815       |\n",
            "| fps                | 419         |\n",
            "| n_updates          | 556         |\n",
            "| policy_entropy     | 0.24050887  |\n",
            "| policy_loss        | -0.03991156 |\n",
            "| serial_timesteps   | 284672      |\n",
            "| time_elapsed       | 6.09e+03    |\n",
            "| total_timesteps    | 2277376     |\n",
            "| value_loss         | 0.09214024  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2280000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.20 +/- 2.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.028141107  |\n",
            "| clipfrac           | 0.02055664   |\n",
            "| ep_len_mean        | 17.7         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.805        |\n",
            "| fps                | 408          |\n",
            "| n_updates          | 557          |\n",
            "| policy_entropy     | 0.022609975  |\n",
            "| policy_loss        | -0.010034902 |\n",
            "| serial_timesteps   | 285184       |\n",
            "| time_elapsed       | 6.1e+03      |\n",
            "| total_timesteps    | 2281472      |\n",
            "| value_loss         | 0.018681463  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.64044225  |\n",
            "| clipfrac           | 0.1553711   |\n",
            "| ep_len_mean        | 20.6        |\n",
            "| ep_reward_mean     | 0.943       |\n",
            "| explained_variance | 0.86        |\n",
            "| fps                | 424         |\n",
            "| n_updates          | 558         |\n",
            "| policy_entropy     | 0.24694145  |\n",
            "| policy_loss        | -0.03858594 |\n",
            "| serial_timesteps   | 285696      |\n",
            "| time_elapsed       | 6.11e+03    |\n",
            "| total_timesteps    | 2285568     |\n",
            "| value_loss         | 0.04821636  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.37025103   |\n",
            "| clipfrac           | 0.069165036  |\n",
            "| ep_len_mean        | 17.7         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.834        |\n",
            "| fps                | 411          |\n",
            "| n_updates          | 559          |\n",
            "| policy_entropy     | 0.117567085  |\n",
            "| policy_loss        | -0.023619674 |\n",
            "| serial_timesteps   | 286208       |\n",
            "| time_elapsed       | 6.12e+03     |\n",
            "| total_timesteps    | 2289664      |\n",
            "| value_loss         | 0.051494278  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2290000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.40 +/- 2.15\n",
            "-------------------------------------\n",
            "| approxkl           | 0.19085935   |\n",
            "| clipfrac           | 0.11254883   |\n",
            "| ep_len_mean        | 17.8         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.91         |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 560          |\n",
            "| policy_entropy     | 0.1912204    |\n",
            "| policy_loss        | -0.039416037 |\n",
            "| serial_timesteps   | 286720       |\n",
            "| time_elapsed       | 6.13e+03     |\n",
            "| total_timesteps    | 2293760      |\n",
            "| value_loss         | 0.02356888   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031726263  |\n",
            "| clipfrac           | 0.057958983  |\n",
            "| ep_len_mean        | 16.8         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.735        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 561          |\n",
            "| policy_entropy     | 0.09675343   |\n",
            "| policy_loss        | -0.027929742 |\n",
            "| serial_timesteps   | 287232       |\n",
            "| time_elapsed       | 6.14e+03     |\n",
            "| total_timesteps    | 2297856      |\n",
            "| value_loss         | 0.04627579   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2300000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 13.80 +/- 0.75\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.1701566    |\n",
            "| clipfrac           | 0.17600098   |\n",
            "| ep_len_mean        | 20.8         |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.723        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 562          |\n",
            "| policy_entropy     | 0.17791715   |\n",
            "| policy_loss        | -0.055719845 |\n",
            "| serial_timesteps   | 287744       |\n",
            "| time_elapsed       | 6.15e+03     |\n",
            "| total_timesteps    | 2301952      |\n",
            "| value_loss         | 0.07914932   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02972814   |\n",
            "| clipfrac           | 0.07700195   |\n",
            "| ep_len_mean        | 18.6         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.815        |\n",
            "| fps                | 411          |\n",
            "| n_updates          | 563          |\n",
            "| policy_entropy     | 0.17386231   |\n",
            "| policy_loss        | -0.021805068 |\n",
            "| serial_timesteps   | 288256       |\n",
            "| time_elapsed       | 6.16e+03     |\n",
            "| total_timesteps    | 2306048      |\n",
            "| value_loss         | 0.044980794  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2310000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.47\n",
            "------------------------------------\n",
            "| approxkl           | 0.27299663  |\n",
            "| clipfrac           | 0.07714844  |\n",
            "| ep_len_mean        | 18.2        |\n",
            "| ep_reward_mean     | 0.949       |\n",
            "| explained_variance | 0.561       |\n",
            "| fps                | 404         |\n",
            "| n_updates          | 564         |\n",
            "| policy_entropy     | 0.09507857  |\n",
            "| policy_loss        | 0.007779613 |\n",
            "| serial_timesteps   | 288768      |\n",
            "| time_elapsed       | 6.17e+03    |\n",
            "| total_timesteps    | 2310144     |\n",
            "| value_loss         | 0.038070045 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.6915878    |\n",
            "| clipfrac           | 0.12402344   |\n",
            "| ep_len_mean        | 20.6         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.614        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 565          |\n",
            "| policy_entropy     | 0.12870157   |\n",
            "| policy_loss        | -0.033237815 |\n",
            "| serial_timesteps   | 289280       |\n",
            "| time_elapsed       | 6.18e+03     |\n",
            "| total_timesteps    | 2314240      |\n",
            "| value_loss         | 0.07525588   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.17553352   |\n",
            "| clipfrac           | 0.099682614  |\n",
            "| ep_len_mean        | 18.4         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.935        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 566          |\n",
            "| policy_entropy     | 0.17448714   |\n",
            "| policy_loss        | -0.034950584 |\n",
            "| serial_timesteps   | 289792       |\n",
            "| time_elapsed       | 6.19e+03     |\n",
            "| total_timesteps    | 2318336      |\n",
            "| value_loss         | 0.013844699  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2320000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.20 +/- 2.93\n",
            "------------------------------------\n",
            "| approxkl           | 0.28337646  |\n",
            "| clipfrac           | 0.16550294  |\n",
            "| ep_len_mean        | 21.8        |\n",
            "| ep_reward_mean     | 0.938       |\n",
            "| explained_variance | 0.88        |\n",
            "| fps                | 410         |\n",
            "| n_updates          | 567         |\n",
            "| policy_entropy     | 0.31911993  |\n",
            "| policy_loss        | 0.10627167  |\n",
            "| serial_timesteps   | 290304      |\n",
            "| time_elapsed       | 6.2e+03     |\n",
            "| total_timesteps    | 2322432     |\n",
            "| value_loss         | 0.077771135 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.056184776  |\n",
            "| clipfrac           | 0.12758788   |\n",
            "| ep_len_mean        | 18.2         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.825        |\n",
            "| fps                | 416          |\n",
            "| n_updates          | 568          |\n",
            "| policy_entropy     | 0.2781872    |\n",
            "| policy_loss        | -0.033739872 |\n",
            "| serial_timesteps   | 290816       |\n",
            "| time_elapsed       | 6.21e+03     |\n",
            "| total_timesteps    | 2326528      |\n",
            "| value_loss         | 0.060723227  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2330000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.80 +/- 4.31\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024925191  |\n",
            "| clipfrac           | 0.10285644   |\n",
            "| ep_len_mean        | 18.3         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.941        |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 569          |\n",
            "| policy_entropy     | 0.26086777   |\n",
            "| policy_loss        | -0.034503765 |\n",
            "| serial_timesteps   | 291328       |\n",
            "| time_elapsed       | 6.22e+03     |\n",
            "| total_timesteps    | 2330624      |\n",
            "| value_loss         | 0.019047704  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 1.801589     |\n",
            "| clipfrac           | 0.16455078   |\n",
            "| ep_len_mean        | 21.3         |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.839        |\n",
            "| fps                | 425          |\n",
            "| n_updates          | 570          |\n",
            "| policy_entropy     | 0.29293635   |\n",
            "| policy_loss        | -0.046824172 |\n",
            "| serial_timesteps   | 291840       |\n",
            "| time_elapsed       | 6.23e+03     |\n",
            "| total_timesteps    | 2334720      |\n",
            "| value_loss         | 0.07706096   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.045831613 |\n",
            "| clipfrac           | 0.060913086 |\n",
            "| ep_len_mean        | 16.7        |\n",
            "| ep_reward_mean     | 0.954       |\n",
            "| explained_variance | 0.774       |\n",
            "| fps                | 417         |\n",
            "| n_updates          | 571         |\n",
            "| policy_entropy     | 0.10433227  |\n",
            "| policy_loss        | -0.01716591 |\n",
            "| serial_timesteps   | 292352      |\n",
            "| time_elapsed       | 6.24e+03    |\n",
            "| total_timesteps    | 2338816     |\n",
            "| value_loss         | 0.030483559 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2340000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.20 +/- 4.53\n",
            "------------------------------------\n",
            "| approxkl           | 2.2681515   |\n",
            "| clipfrac           | 0.19335938  |\n",
            "| ep_len_mean        | 22          |\n",
            "| ep_reward_mean     | 0.938       |\n",
            "| explained_variance | 0.76        |\n",
            "| fps                | 408         |\n",
            "| n_updates          | 572         |\n",
            "| policy_entropy     | 0.25811273  |\n",
            "| policy_loss        | -0.05788232 |\n",
            "| serial_timesteps   | 292864      |\n",
            "| time_elapsed       | 6.25e+03    |\n",
            "| total_timesteps    | 2342912     |\n",
            "| value_loss         | 0.098535076 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.06113132  |\n",
            "| clipfrac           | 0.1309082   |\n",
            "| ep_len_mean        | 21          |\n",
            "| ep_reward_mean     | 0.942       |\n",
            "| explained_variance | 0.875       |\n",
            "| fps                | 423         |\n",
            "| n_updates          | 573         |\n",
            "| policy_entropy     | 0.267783    |\n",
            "| policy_loss        | -0.03754152 |\n",
            "| serial_timesteps   | 293376      |\n",
            "| time_elapsed       | 6.26e+03    |\n",
            "| total_timesteps    | 2347008     |\n",
            "| value_loss         | 0.04463909  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2350000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.80 +/- 123.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026876247  |\n",
            "| clipfrac           | 0.09995117   |\n",
            "| ep_len_mean        | 18.7         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.952        |\n",
            "| fps                | 372          |\n",
            "| n_updates          | 574          |\n",
            "| policy_entropy     | 0.23218279   |\n",
            "| policy_loss        | -0.029665908 |\n",
            "| serial_timesteps   | 293888       |\n",
            "| time_elapsed       | 6.27e+03     |\n",
            "| total_timesteps    | 2351104      |\n",
            "| value_loss         | 0.013222525  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.51349205  |\n",
            "| clipfrac           | 0.21582031  |\n",
            "| ep_len_mean        | 24.7        |\n",
            "| ep_reward_mean     | 0.929       |\n",
            "| explained_variance | 0.939       |\n",
            "| fps                | 423         |\n",
            "| n_updates          | 575         |\n",
            "| policy_entropy     | 0.57096523  |\n",
            "| policy_loss        | -0.04665576 |\n",
            "| serial_timesteps   | 294400      |\n",
            "| time_elapsed       | 6.28e+03    |\n",
            "| total_timesteps    | 2355200     |\n",
            "| value_loss         | 0.07172867  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.020306278 |\n",
            "| clipfrac           | 0.07478027  |\n",
            "| ep_len_mean        | 16.6        |\n",
            "| ep_reward_mean     | 0.954       |\n",
            "| explained_variance | 0.915       |\n",
            "| fps                | 425         |\n",
            "| n_updates          | 576         |\n",
            "| policy_entropy     | 0.20580244  |\n",
            "| policy_loss        | -0.01715429 |\n",
            "| serial_timesteps   | 294912      |\n",
            "| time_elapsed       | 6.29e+03    |\n",
            "| total_timesteps    | 2359296     |\n",
            "| value_loss         | 0.043825567 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2360000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 1.74\n",
            "--------------------------------------\n",
            "| approxkl           | 1.1513917     |\n",
            "| clipfrac           | 0.18947753    |\n",
            "| ep_len_mean        | 21.8          |\n",
            "| ep_reward_mean     | 0.938         |\n",
            "| explained_variance | 0.763         |\n",
            "| fps                | 408           |\n",
            "| n_updates          | 577           |\n",
            "| policy_entropy     | 0.2062538     |\n",
            "| policy_loss        | -0.0033344675 |\n",
            "| serial_timesteps   | 295424        |\n",
            "| time_elapsed       | 6.3e+03       |\n",
            "| total_timesteps    | 2363392       |\n",
            "| value_loss         | 0.16500533    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.045551006  |\n",
            "| clipfrac           | 0.08552246   |\n",
            "| ep_len_mean        | 16.7         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.903        |\n",
            "| fps                | 427          |\n",
            "| n_updates          | 578          |\n",
            "| policy_entropy     | 0.16152963   |\n",
            "| policy_loss        | -0.023583481 |\n",
            "| serial_timesteps   | 295936       |\n",
            "| time_elapsed       | 6.31e+03     |\n",
            "| total_timesteps    | 2367488      |\n",
            "| value_loss         | 0.04058756   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2370000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.40 +/- 1.36\n",
            "-------------------------------------\n",
            "| approxkl           | 0.08088072   |\n",
            "| clipfrac           | 0.038671874  |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.874        |\n",
            "| fps                | 401          |\n",
            "| n_updates          | 579          |\n",
            "| policy_entropy     | 0.052132946  |\n",
            "| policy_loss        | -0.016930224 |\n",
            "| serial_timesteps   | 296448       |\n",
            "| time_elapsed       | 6.32e+03     |\n",
            "| total_timesteps    | 2371584      |\n",
            "| value_loss         | 0.014282119  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 3.3253155   |\n",
            "| clipfrac           | 0.29372558  |\n",
            "| ep_len_mean        | 28.3        |\n",
            "| ep_reward_mean     | 0.918       |\n",
            "| explained_variance | 0.498       |\n",
            "| fps                | 416         |\n",
            "| n_updates          | 580         |\n",
            "| policy_entropy     | 0.14017174  |\n",
            "| policy_loss        | 0.048865456 |\n",
            "| serial_timesteps   | 296960      |\n",
            "| time_elapsed       | 6.33e+03    |\n",
            "| total_timesteps    | 2375680     |\n",
            "| value_loss         | 0.22732422  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.58699536   |\n",
            "| clipfrac           | 0.13845214   |\n",
            "| ep_len_mean        | 22.4         |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.88         |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 581          |\n",
            "| policy_entropy     | 0.18361834   |\n",
            "| policy_loss        | -0.028189704 |\n",
            "| serial_timesteps   | 297472       |\n",
            "| time_elapsed       | 6.34e+03     |\n",
            "| total_timesteps    | 2379776      |\n",
            "| value_loss         | 0.076012     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2380000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.80 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.072415546  |\n",
            "| clipfrac           | 0.086035155  |\n",
            "| ep_len_mean        | 19.3         |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.93         |\n",
            "| fps                | 405          |\n",
            "| n_updates          | 582          |\n",
            "| policy_entropy     | 0.14308351   |\n",
            "| policy_loss        | -0.012322885 |\n",
            "| serial_timesteps   | 297984       |\n",
            "| time_elapsed       | 6.35e+03     |\n",
            "| total_timesteps    | 2383872      |\n",
            "| value_loss         | 0.014529755  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.029782007  |\n",
            "| clipfrac           | 0.064135745  |\n",
            "| ep_len_mean        | 16.9         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.693        |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 583          |\n",
            "| policy_entropy     | 0.057358224  |\n",
            "| policy_loss        | -0.003397467 |\n",
            "| serial_timesteps   | 298496       |\n",
            "| time_elapsed       | 6.36e+03     |\n",
            "| total_timesteps    | 2387968      |\n",
            "| value_loss         | 0.0488886    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2390000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.80 +/- 2.99\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03223894   |\n",
            "| clipfrac           | 0.06665039   |\n",
            "| ep_len_mean        | 17.3         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.91         |\n",
            "| fps                | 409          |\n",
            "| n_updates          | 584          |\n",
            "| policy_entropy     | 0.09656359   |\n",
            "| policy_loss        | -0.024186226 |\n",
            "| serial_timesteps   | 299008       |\n",
            "| time_elapsed       | 6.37e+03     |\n",
            "| total_timesteps    | 2392064      |\n",
            "| value_loss         | 0.014321643  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.09583114   |\n",
            "| clipfrac           | 0.05432129   |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.878        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 585          |\n",
            "| policy_entropy     | 0.064291686  |\n",
            "| policy_loss        | -0.017254459 |\n",
            "| serial_timesteps   | 299520       |\n",
            "| time_elapsed       | 6.38e+03     |\n",
            "| total_timesteps    | 2396160      |\n",
            "| value_loss         | 0.011675729  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2400000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.80 +/- 123.12\n",
            "-------------------------------------\n",
            "| approxkl           | 0.14524676   |\n",
            "| clipfrac           | 0.22443847   |\n",
            "| ep_len_mean        | 24.5         |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.48         |\n",
            "| fps                | 369          |\n",
            "| n_updates          | 586          |\n",
            "| policy_entropy     | 0.2501164    |\n",
            "| policy_loss        | -0.010539555 |\n",
            "| serial_timesteps   | 300032       |\n",
            "| time_elapsed       | 6.39e+03     |\n",
            "| total_timesteps    | 2400256      |\n",
            "| value_loss         | 0.13261838   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.052071787  |\n",
            "| clipfrac           | 0.20952149   |\n",
            "| ep_len_mean        | 19.8         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.718        |\n",
            "| fps                | 416          |\n",
            "| n_updates          | 587          |\n",
            "| policy_entropy     | 0.37845176   |\n",
            "| policy_loss        | -0.035559673 |\n",
            "| serial_timesteps   | 300544       |\n",
            "| time_elapsed       | 6.4e+03      |\n",
            "| total_timesteps    | 2404352      |\n",
            "| value_loss         | 0.08650477   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.02839314  |\n",
            "| clipfrac           | 0.19069824  |\n",
            "| ep_len_mean        | 26.8        |\n",
            "| ep_reward_mean     | 0.925       |\n",
            "| explained_variance | 0.758       |\n",
            "| fps                | 414         |\n",
            "| n_updates          | 588         |\n",
            "| policy_entropy     | 0.4368519   |\n",
            "| policy_loss        | -0.04248243 |\n",
            "| serial_timesteps   | 301056      |\n",
            "| time_elapsed       | 6.41e+03    |\n",
            "| total_timesteps    | 2408448     |\n",
            "| value_loss         | 0.13033578  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2410000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 1.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04174111   |\n",
            "| clipfrac           | 0.2076172    |\n",
            "| ep_len_mean        | 24.2         |\n",
            "| ep_reward_mean     | 0.932        |\n",
            "| explained_variance | 0.778        |\n",
            "| fps                | 405          |\n",
            "| n_updates          | 589          |\n",
            "| policy_entropy     | 0.48866606   |\n",
            "| policy_loss        | -0.040446304 |\n",
            "| serial_timesteps   | 301568       |\n",
            "| time_elapsed       | 6.42e+03     |\n",
            "| total_timesteps    | 2412544      |\n",
            "| value_loss         | 0.102843806  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.5732065   |\n",
            "| clipfrac           | 0.24055175  |\n",
            "| ep_len_mean        | 22.3        |\n",
            "| ep_reward_mean     | 0.937       |\n",
            "| explained_variance | 0.781       |\n",
            "| fps                | 424         |\n",
            "| n_updates          | 590         |\n",
            "| policy_entropy     | 0.37898397  |\n",
            "| policy_loss        | -0.03808312 |\n",
            "| serial_timesteps   | 302080      |\n",
            "| time_elapsed       | 6.43e+03    |\n",
            "| total_timesteps    | 2416640     |\n",
            "| value_loss         | 0.1175215   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2420000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.00 +/- 1.90\n",
            "-------------------------------------\n",
            "| approxkl           | 0.037327636  |\n",
            "| clipfrac           | 0.10759278   |\n",
            "| ep_len_mean        | 21           |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.836        |\n",
            "| fps                | 406          |\n",
            "| n_updates          | 591          |\n",
            "| policy_entropy     | 0.2916319    |\n",
            "| policy_loss        | -0.030245379 |\n",
            "| serial_timesteps   | 302592       |\n",
            "| time_elapsed       | 6.44e+03     |\n",
            "| total_timesteps    | 2420736      |\n",
            "| value_loss         | 0.06665893   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.14220802   |\n",
            "| clipfrac           | 0.16159669   |\n",
            "| ep_len_mean        | 22.3         |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.881        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 592          |\n",
            "| policy_entropy     | 0.31006804   |\n",
            "| policy_loss        | -0.014126028 |\n",
            "| serial_timesteps   | 303104       |\n",
            "| time_elapsed       | 6.45e+03     |\n",
            "| total_timesteps    | 2424832      |\n",
            "| value_loss         | 0.04817339   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020491213  |\n",
            "| clipfrac           | 0.17260742   |\n",
            "| ep_len_mean        | 25.3         |\n",
            "| ep_reward_mean     | 0.929        |\n",
            "| explained_variance | 0.89         |\n",
            "| fps                | 428          |\n",
            "| n_updates          | 593          |\n",
            "| policy_entropy     | 0.58160067   |\n",
            "| policy_loss        | -0.037034355 |\n",
            "| serial_timesteps   | 303616       |\n",
            "| time_elapsed       | 6.46e+03     |\n",
            "| total_timesteps    | 2428928      |\n",
            "| value_loss         | 0.09333508   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2430000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.20 +/- 123.90\n",
            "------------------------------------\n",
            "| approxkl           | 0.020541802 |\n",
            "| clipfrac           | 0.1503418   |\n",
            "| ep_len_mean        | 28.3        |\n",
            "| ep_reward_mean     | 0.919       |\n",
            "| explained_variance | 0.776       |\n",
            "| fps                | 370         |\n",
            "| n_updates          | 594         |\n",
            "| policy_entropy     | 0.42030016  |\n",
            "| policy_loss        | -0.0296631  |\n",
            "| serial_timesteps   | 304128      |\n",
            "| time_elapsed       | 6.47e+03    |\n",
            "| total_timesteps    | 2433024     |\n",
            "| value_loss         | 0.13639824  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.015337303 |\n",
            "| clipfrac           | 0.10454102  |\n",
            "| ep_len_mean        | 21          |\n",
            "| ep_reward_mean     | 0.942       |\n",
            "| explained_variance | 0.925       |\n",
            "| fps                | 409         |\n",
            "| n_updates          | 595         |\n",
            "| policy_entropy     | 0.28286728  |\n",
            "| policy_loss        | -0.03617829 |\n",
            "| serial_timesteps   | 304640      |\n",
            "| time_elapsed       | 6.48e+03    |\n",
            "| total_timesteps    | 2437120     |\n",
            "| value_loss         | 0.030481642 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2440000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.40 +/- 2.06\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02030276   |\n",
            "| clipfrac           | 0.13249512   |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.891        |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 596          |\n",
            "| policy_entropy     | 0.34414458   |\n",
            "| policy_loss        | -0.038075194 |\n",
            "| serial_timesteps   | 305152       |\n",
            "| time_elapsed       | 6.49e+03     |\n",
            "| total_timesteps    | 2441216      |\n",
            "| value_loss         | 0.047215994  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.054013662  |\n",
            "| clipfrac           | 0.07539062   |\n",
            "| ep_len_mean        | 21.5         |\n",
            "| ep_reward_mean     | 0.939        |\n",
            "| explained_variance | 0.926        |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 597          |\n",
            "| policy_entropy     | 0.19092685   |\n",
            "| policy_loss        | -0.020281564 |\n",
            "| serial_timesteps   | 305664       |\n",
            "| time_elapsed       | 6.5e+03      |\n",
            "| total_timesteps    | 2445312      |\n",
            "| value_loss         | 0.03117762   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019194763  |\n",
            "| clipfrac           | 0.14069824   |\n",
            "| ep_len_mean        | 23.2         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.906        |\n",
            "| fps                | 425          |\n",
            "| n_updates          | 598          |\n",
            "| policy_entropy     | 0.33654544   |\n",
            "| policy_loss        | -0.034955867 |\n",
            "| serial_timesteps   | 306176       |\n",
            "| time_elapsed       | 6.51e+03     |\n",
            "| total_timesteps    | 2449408      |\n",
            "| value_loss         | 0.063303545  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2450000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.00 +/- 0.89\n",
            "-------------------------------------\n",
            "| approxkl           | 0.15361267   |\n",
            "| clipfrac           | 0.13286133   |\n",
            "| ep_len_mean        | 19           |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.831        |\n",
            "| fps                | 407          |\n",
            "| n_updates          | 599          |\n",
            "| policy_entropy     | 0.24306488   |\n",
            "| policy_loss        | -0.032808665 |\n",
            "| serial_timesteps   | 306688       |\n",
            "| time_elapsed       | 6.52e+03     |\n",
            "| total_timesteps    | 2453504      |\n",
            "| value_loss         | 0.058602482  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.012681092  |\n",
            "| clipfrac           | 0.0746582    |\n",
            "| ep_len_mean        | 19.5         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.861        |\n",
            "| fps                | 428          |\n",
            "| n_updates          | 600          |\n",
            "| policy_entropy     | 0.18222044   |\n",
            "| policy_loss        | -0.022383207 |\n",
            "| serial_timesteps   | 307200       |\n",
            "| time_elapsed       | 6.53e+03     |\n",
            "| total_timesteps    | 2457600      |\n",
            "| value_loss         | 0.038197603  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2460000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.40 +/- 1.85\n",
            "-------------------------------------\n",
            "| approxkl           | 0.8281497    |\n",
            "| clipfrac           | 0.15617676   |\n",
            "| ep_len_mean        | 20.8         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.838        |\n",
            "| fps                | 413          |\n",
            "| n_updates          | 601          |\n",
            "| policy_entropy     | 0.198083     |\n",
            "| policy_loss        | -0.050720554 |\n",
            "| serial_timesteps   | 307712       |\n",
            "| time_elapsed       | 6.54e+03     |\n",
            "| total_timesteps    | 2461696      |\n",
            "| value_loss         | 0.048298884  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016915489  |\n",
            "| clipfrac           | 0.055395506  |\n",
            "| ep_len_mean        | 16.8         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.93         |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 602          |\n",
            "| policy_entropy     | 0.12101735   |\n",
            "| policy_loss        | -0.026646713 |\n",
            "| serial_timesteps   | 308224       |\n",
            "| time_elapsed       | 6.55e+03     |\n",
            "| total_timesteps    | 2465792      |\n",
            "| value_loss         | 0.01792128   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.06161837    |\n",
            "| clipfrac           | 0.025512695   |\n",
            "| ep_len_mean        | 16.4          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.892         |\n",
            "| fps                | 426           |\n",
            "| n_updates          | 603           |\n",
            "| policy_entropy     | 0.021160293   |\n",
            "| policy_loss        | -0.0012926385 |\n",
            "| serial_timesteps   | 308736        |\n",
            "| time_elapsed       | 6.56e+03      |\n",
            "| total_timesteps    | 2469888       |\n",
            "| value_loss         | 0.0042564045  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2470000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 5.11\n",
            "-------------------------------------\n",
            "| approxkl           | 3.1695855    |\n",
            "| clipfrac           | 0.14528808   |\n",
            "| ep_len_mean        | 17.1         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.755        |\n",
            "| fps                | 414          |\n",
            "| n_updates          | 604          |\n",
            "| policy_entropy     | 0.031018186  |\n",
            "| policy_loss        | -0.027969742 |\n",
            "| serial_timesteps   | 309248       |\n",
            "| time_elapsed       | 6.57e+03     |\n",
            "| total_timesteps    | 2473984      |\n",
            "| value_loss         | 0.03216248   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.022245439   |\n",
            "| clipfrac           | 0.013085937   |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.641         |\n",
            "| fps                | 421           |\n",
            "| n_updates          | 605           |\n",
            "| policy_entropy     | 0.014098075   |\n",
            "| policy_loss        | -0.0004151632 |\n",
            "| serial_timesteps   | 309760        |\n",
            "| time_elapsed       | 6.58e+03      |\n",
            "| total_timesteps    | 2478080       |\n",
            "| value_loss         | 0.032733336   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2480000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.20 +/- 1.60\n",
            "------------------------------------\n",
            "| approxkl           | 1.1769744   |\n",
            "| clipfrac           | 0.09382324  |\n",
            "| ep_len_mean        | 19.7        |\n",
            "| ep_reward_mean     | 0.944       |\n",
            "| explained_variance | 0.333       |\n",
            "| fps                | 412         |\n",
            "| n_updates          | 606         |\n",
            "| policy_entropy     | 0.022912918 |\n",
            "| policy_loss        | 0.053250916 |\n",
            "| serial_timesteps   | 310272      |\n",
            "| time_elapsed       | 6.59e+03    |\n",
            "| total_timesteps    | 2482176     |\n",
            "| value_loss         | 0.0745001   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.20722003   |\n",
            "| clipfrac           | 0.044311523  |\n",
            "| ep_len_mean        | 16.5         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.954        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 607          |\n",
            "| policy_entropy     | 0.03675718   |\n",
            "| policy_loss        | -0.007236565 |\n",
            "| serial_timesteps   | 310784       |\n",
            "| time_elapsed       | 6.6e+03      |\n",
            "| total_timesteps    | 2486272      |\n",
            "| value_loss         | 0.0015782131 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2490000, episode_reward=0.58 +/- 0.47\n",
            "Episode length: 138.20 +/- 151.71\n",
            "------------------------------------\n",
            "| approxkl           | 0.37386668  |\n",
            "| clipfrac           | 0.3214844   |\n",
            "| ep_len_mean        | 33.5        |\n",
            "| ep_reward_mean     | 0.904       |\n",
            "| explained_variance | 0.416       |\n",
            "| fps                | 334         |\n",
            "| n_updates          | 608         |\n",
            "| policy_entropy     | 0.3542522   |\n",
            "| policy_loss        | -0.03640156 |\n",
            "| serial_timesteps   | 311296      |\n",
            "| time_elapsed       | 6.61e+03    |\n",
            "| total_timesteps    | 2490368     |\n",
            "| value_loss         | 0.26712686  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.19385013   |\n",
            "| clipfrac           | 0.31745607   |\n",
            "| ep_len_mean        | 34           |\n",
            "| ep_reward_mean     | 0.902        |\n",
            "| explained_variance | 0.479        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 609          |\n",
            "| policy_entropy     | 0.44727015   |\n",
            "| policy_loss        | -0.026078176 |\n",
            "| serial_timesteps   | 311808       |\n",
            "| time_elapsed       | 6.62e+03     |\n",
            "| total_timesteps    | 2494464      |\n",
            "| value_loss         | 0.27402008   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.07069163    |\n",
            "| clipfrac           | 0.23669434    |\n",
            "| ep_len_mean        | 27.6          |\n",
            "| ep_reward_mean     | 0.922         |\n",
            "| explained_variance | 0.573         |\n",
            "| fps                | 424           |\n",
            "| n_updates          | 610           |\n",
            "| policy_entropy     | 0.4337396     |\n",
            "| policy_loss        | -0.0059115905 |\n",
            "| serial_timesteps   | 312320        |\n",
            "| time_elapsed       | 6.63e+03      |\n",
            "| total_timesteps    | 2498560       |\n",
            "| value_loss         | 0.20076862    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2500000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.60 +/- 122.71\n",
            "------------------------------------\n",
            "| approxkl           | 0.07064779  |\n",
            "| clipfrac           | 0.21340331  |\n",
            "| ep_len_mean        | 24          |\n",
            "| ep_reward_mean     | 0.932       |\n",
            "| explained_variance | 0.589       |\n",
            "| fps                | 368         |\n",
            "| n_updates          | 611         |\n",
            "| policy_entropy     | 0.348165    |\n",
            "| policy_loss        | -0.03965314 |\n",
            "| serial_timesteps   | 312832      |\n",
            "| time_elapsed       | 6.64e+03    |\n",
            "| total_timesteps    | 2502656     |\n",
            "| value_loss         | 0.15401518  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.027805027  |\n",
            "| clipfrac           | 0.14519043   |\n",
            "| ep_len_mean        | 17.7         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.875        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 612          |\n",
            "| policy_entropy     | 0.348155     |\n",
            "| policy_loss        | -0.035765354 |\n",
            "| serial_timesteps   | 313344       |\n",
            "| time_elapsed       | 6.65e+03     |\n",
            "| total_timesteps    | 2506752      |\n",
            "| value_loss         | 0.05226487   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2510000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 77.20 +/- 123.41\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018023197  |\n",
            "| clipfrac           | 0.15004882   |\n",
            "| ep_len_mean        | 21.4         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.821        |\n",
            "| fps                | 366          |\n",
            "| n_updates          | 613          |\n",
            "| policy_entropy     | 0.44701046   |\n",
            "| policy_loss        | -0.029619927 |\n",
            "| serial_timesteps   | 313856       |\n",
            "| time_elapsed       | 6.66e+03     |\n",
            "| total_timesteps    | 2510848      |\n",
            "| value_loss         | 0.08692322   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.043148812 |\n",
            "| clipfrac           | 0.25014648  |\n",
            "| ep_len_mean        | 25.2        |\n",
            "| ep_reward_mean     | 0.929       |\n",
            "| explained_variance | 0.812       |\n",
            "| fps                | 426         |\n",
            "| n_updates          | 614         |\n",
            "| policy_entropy     | 0.5059482   |\n",
            "| policy_loss        | -0.0460475  |\n",
            "| serial_timesteps   | 314368      |\n",
            "| time_elapsed       | 6.67e+03    |\n",
            "| total_timesteps    | 2514944     |\n",
            "| value_loss         | 0.10792208  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.077095725  |\n",
            "| clipfrac           | 0.22600098   |\n",
            "| ep_len_mean        | 23.2         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.818        |\n",
            "| fps                | 428          |\n",
            "| n_updates          | 615          |\n",
            "| policy_entropy     | 0.5526828    |\n",
            "| policy_loss        | -0.044281434 |\n",
            "| serial_timesteps   | 314880       |\n",
            "| time_elapsed       | 6.68e+03     |\n",
            "| total_timesteps    | 2519040      |\n",
            "| value_loss         | 0.13060555   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2520000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.00 +/- 3.41\n",
            "-------------------------------------\n",
            "| approxkl           | 0.027481908  |\n",
            "| clipfrac           | 0.12460937   |\n",
            "| ep_len_mean        | 22.7         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.829        |\n",
            "| fps                | 409          |\n",
            "| n_updates          | 616          |\n",
            "| policy_entropy     | 0.33325014   |\n",
            "| policy_loss        | -0.020752203 |\n",
            "| serial_timesteps   | 315392       |\n",
            "| time_elapsed       | 6.69e+03     |\n",
            "| total_timesteps    | 2523136      |\n",
            "| value_loss         | 0.10792033   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02027298   |\n",
            "| clipfrac           | 0.14055176   |\n",
            "| ep_len_mean        | 21.4         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.938        |\n",
            "| fps                | 425          |\n",
            "| n_updates          | 617          |\n",
            "| policy_entropy     | 0.41957012   |\n",
            "| policy_loss        | -0.034770694 |\n",
            "| serial_timesteps   | 315904       |\n",
            "| time_elapsed       | 6.7e+03      |\n",
            "| total_timesteps    | 2527232      |\n",
            "| value_loss         | 0.039336048  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2530000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 14.80 +/- 1.83\n",
            "------------------------------------\n",
            "| approxkl           | 1.9132421   |\n",
            "| clipfrac           | 0.1628418   |\n",
            "| ep_len_mean        | 20.6        |\n",
            "| ep_reward_mean     | 0.943       |\n",
            "| explained_variance | 0.858       |\n",
            "| fps                | 413         |\n",
            "| n_updates          | 618         |\n",
            "| policy_entropy     | 0.21196334  |\n",
            "| policy_loss        | -0.04419465 |\n",
            "| serial_timesteps   | 316416      |\n",
            "| time_elapsed       | 6.71e+03    |\n",
            "| total_timesteps    | 2531328     |\n",
            "| value_loss         | 0.05505074  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01924078   |\n",
            "| clipfrac           | 0.12680665   |\n",
            "| ep_len_mean        | 27.5         |\n",
            "| ep_reward_mean     | 0.922        |\n",
            "| explained_variance | 0.944        |\n",
            "| fps                | 414          |\n",
            "| n_updates          | 619          |\n",
            "| policy_entropy     | 0.4752195    |\n",
            "| policy_loss        | -0.026182944 |\n",
            "| serial_timesteps   | 316928       |\n",
            "| time_elapsed       | 6.72e+03     |\n",
            "| total_timesteps    | 2535424      |\n",
            "| value_loss         | 0.055615198  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 2.190275     |\n",
            "| clipfrac           | 0.119091794  |\n",
            "| ep_len_mean        | 21.8         |\n",
            "| ep_reward_mean     | 0.938        |\n",
            "| explained_variance | 0.748        |\n",
            "| fps                | 416          |\n",
            "| n_updates          | 620          |\n",
            "| policy_entropy     | 0.11197938   |\n",
            "| policy_loss        | -0.048309714 |\n",
            "| serial_timesteps   | 317440       |\n",
            "| time_elapsed       | 6.73e+03     |\n",
            "| total_timesteps    | 2539520      |\n",
            "| value_loss         | 0.07631596   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2540000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.80 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03197313   |\n",
            "| clipfrac           | 0.11811523   |\n",
            "| ep_len_mean        | 26.8         |\n",
            "| ep_reward_mean     | 0.923        |\n",
            "| explained_variance | 0.975        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 621          |\n",
            "| policy_entropy     | 0.52855843   |\n",
            "| policy_loss        | -0.018967578 |\n",
            "| serial_timesteps   | 317952       |\n",
            "| time_elapsed       | 6.74e+03     |\n",
            "| total_timesteps    | 2543616      |\n",
            "| value_loss         | 0.031941038  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 5.7456837   |\n",
            "| clipfrac           | 0.13713379  |\n",
            "| ep_len_mean        | 19.6        |\n",
            "| ep_reward_mean     | 0.946       |\n",
            "| explained_variance | 0.952       |\n",
            "| fps                | 427         |\n",
            "| n_updates          | 622         |\n",
            "| policy_entropy     | 0.36855647  |\n",
            "| policy_loss        | 0.030111695 |\n",
            "| serial_timesteps   | 318464      |\n",
            "| time_elapsed       | 6.75e+03    |\n",
            "| total_timesteps    | 2547712     |\n",
            "| value_loss         | 0.058207016 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2550000, episode_reward=0.95 +/- 0.02\n",
            "Episode length: 18.80 +/- 5.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02705225   |\n",
            "| clipfrac           | 0.09814453   |\n",
            "| ep_len_mean        | 18.8         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.979        |\n",
            "| fps                | 390          |\n",
            "| n_updates          | 623          |\n",
            "| policy_entropy     | 0.33341637   |\n",
            "| policy_loss        | -0.018990913 |\n",
            "| serial_timesteps   | 318976       |\n",
            "| time_elapsed       | 6.76e+03     |\n",
            "| total_timesteps    | 2551808      |\n",
            "| value_loss         | 0.018679438  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.013849038 |\n",
            "| clipfrac           | 0.09694824  |\n",
            "| ep_len_mean        | 18.8        |\n",
            "| ep_reward_mean     | 0.948       |\n",
            "| explained_variance | 0.977       |\n",
            "| fps                | 415         |\n",
            "| n_updates          | 624         |\n",
            "| policy_entropy     | 0.3370304   |\n",
            "| policy_loss        | -0.02009085 |\n",
            "| serial_timesteps   | 319488      |\n",
            "| time_elapsed       | 6.77e+03    |\n",
            "| total_timesteps    | 2555904     |\n",
            "| value_loss         | 0.021759091 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2560000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.40 +/- 2.50\n",
            "-------------------------------------\n",
            "| approxkl           | 2.3231616    |\n",
            "| clipfrac           | 0.158667     |\n",
            "| ep_len_mean        | 23.6         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.902        |\n",
            "| fps                | 399          |\n",
            "| n_updates          | 625          |\n",
            "| policy_entropy     | 0.35603723   |\n",
            "| policy_loss        | -0.045077167 |\n",
            "| serial_timesteps   | 320000       |\n",
            "| time_elapsed       | 6.78e+03     |\n",
            "| total_timesteps    | 2560000      |\n",
            "| value_loss         | 0.10505588   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.7145302     |\n",
            "| clipfrac           | 0.18830566    |\n",
            "| ep_len_mean        | 18.1          |\n",
            "| ep_reward_mean     | 0.95          |\n",
            "| explained_variance | 0.675         |\n",
            "| fps                | 406           |\n",
            "| n_updates          | 626           |\n",
            "| policy_entropy     | 0.16209377    |\n",
            "| policy_loss        | -0.0147503065 |\n",
            "| serial_timesteps   | 320512        |\n",
            "| time_elapsed       | 6.79e+03      |\n",
            "| total_timesteps    | 2564096       |\n",
            "| value_loss         | 0.062544905   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.115772925 |\n",
            "| clipfrac           | 0.14851074  |\n",
            "| ep_len_mean        | 22.7        |\n",
            "| ep_reward_mean     | 0.936       |\n",
            "| explained_variance | 0.668       |\n",
            "| fps                | 408         |\n",
            "| n_updates          | 627         |\n",
            "| policy_entropy     | 0.23405042  |\n",
            "| policy_loss        | -0.03926394 |\n",
            "| serial_timesteps   | 321024      |\n",
            "| time_elapsed       | 6.8e+03     |\n",
            "| total_timesteps    | 2568192     |\n",
            "| value_loss         | 0.12256964  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2570000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.80 +/- 123.13\n",
            "------------------------------------\n",
            "| approxkl           | 0.116873994 |\n",
            "| clipfrac           | 0.089379884 |\n",
            "| ep_len_mean        | 20          |\n",
            "| ep_reward_mean     | 0.944       |\n",
            "| explained_variance | 0.638       |\n",
            "| fps                | 360         |\n",
            "| n_updates          | 628         |\n",
            "| policy_entropy     | 0.08823355  |\n",
            "| policy_loss        | 0.021713387 |\n",
            "| serial_timesteps   | 321536      |\n",
            "| time_elapsed       | 6.81e+03    |\n",
            "| total_timesteps    | 2572288     |\n",
            "| value_loss         | 0.0616697   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.040037863  |\n",
            "| clipfrac           | 0.06311035   |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.83         |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 629          |\n",
            "| policy_entropy     | 0.1025056    |\n",
            "| policy_loss        | -0.017117396 |\n",
            "| serial_timesteps   | 322048       |\n",
            "| time_elapsed       | 6.82e+03     |\n",
            "| total_timesteps    | 2576384      |\n",
            "| value_loss         | 0.017276969  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2580000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 1.26\n",
            "-------------------------------------\n",
            "| approxkl           | 0.037813984  |\n",
            "| clipfrac           | 0.19416504   |\n",
            "| ep_len_mean        | 20.9         |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.771        |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 630          |\n",
            "| policy_entropy     | 0.34138775   |\n",
            "| policy_loss        | -0.041785352 |\n",
            "| serial_timesteps   | 322560       |\n",
            "| time_elapsed       | 6.83e+03     |\n",
            "| total_timesteps    | 2580480      |\n",
            "| value_loss         | 0.079389855  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024441194  |\n",
            "| clipfrac           | 0.095825195  |\n",
            "| ep_len_mean        | 18.7         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.683        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 631          |\n",
            "| policy_entropy     | 0.1849351    |\n",
            "| policy_loss        | -0.028718865 |\n",
            "| serial_timesteps   | 323072       |\n",
            "| time_elapsed       | 6.84e+03     |\n",
            "| total_timesteps    | 2584576      |\n",
            "| value_loss         | 0.0631611    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.21091604    |\n",
            "| clipfrac           | 0.21586914    |\n",
            "| ep_len_mean        | 24.2          |\n",
            "| ep_reward_mean     | 0.932         |\n",
            "| explained_variance | 0.659         |\n",
            "| fps                | 419           |\n",
            "| n_updates          | 632           |\n",
            "| policy_entropy     | 0.28839082    |\n",
            "| policy_loss        | -0.0020530196 |\n",
            "| serial_timesteps   | 323584        |\n",
            "| time_elapsed       | 6.85e+03      |\n",
            "| total_timesteps    | 2588672       |\n",
            "| value_loss         | 0.10440414    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2590000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.33091483   |\n",
            "| clipfrac           | 0.0987793    |\n",
            "| ep_len_mean        | 17.9         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.74         |\n",
            "| fps                | 405          |\n",
            "| n_updates          | 633          |\n",
            "| policy_entropy     | 0.14752951   |\n",
            "| policy_loss        | -0.028457755 |\n",
            "| serial_timesteps   | 324096       |\n",
            "| time_elapsed       | 6.86e+03     |\n",
            "| total_timesteps    | 2592768      |\n",
            "| value_loss         | 0.04948468   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.027243108  |\n",
            "| clipfrac           | 0.16091308   |\n",
            "| ep_len_mean        | 17.9         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.899        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 634          |\n",
            "| policy_entropy     | 0.3414593    |\n",
            "| policy_loss        | -0.041110612 |\n",
            "| serial_timesteps   | 324608       |\n",
            "| time_elapsed       | 6.87e+03     |\n",
            "| total_timesteps    | 2596864      |\n",
            "| value_loss         | 0.03213603   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2600000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 1.67\n",
            "-------------------------------------\n",
            "| approxkl           | 1.3170379    |\n",
            "| clipfrac           | 0.2084961    |\n",
            "| ep_len_mean        | 21.1         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.85         |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 635          |\n",
            "| policy_entropy     | 0.28746015   |\n",
            "| policy_loss        | -0.049732335 |\n",
            "| serial_timesteps   | 325120       |\n",
            "| time_elapsed       | 6.88e+03     |\n",
            "| total_timesteps    | 2600960      |\n",
            "| value_loss         | 0.07267481   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.061421733  |\n",
            "| clipfrac           | 0.14570312   |\n",
            "| ep_len_mean        | 20.4         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.813        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 636          |\n",
            "| policy_entropy     | 0.2938509    |\n",
            "| policy_loss        | -0.021771729 |\n",
            "| serial_timesteps   | 325632       |\n",
            "| time_elapsed       | 6.89e+03     |\n",
            "| total_timesteps    | 2605056      |\n",
            "| value_loss         | 0.07150587   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.06281634  |\n",
            "| clipfrac           | 0.14729004  |\n",
            "| ep_len_mean        | 20.4        |\n",
            "| ep_reward_mean     | 0.943       |\n",
            "| explained_variance | 0.806       |\n",
            "| fps                | 416         |\n",
            "| n_updates          | 637         |\n",
            "| policy_entropy     | 0.3019246   |\n",
            "| policy_loss        | 0.056844592 |\n",
            "| serial_timesteps   | 326144      |\n",
            "| time_elapsed       | 6.9e+03     |\n",
            "| total_timesteps    | 2609152     |\n",
            "| value_loss         | 0.061042376 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2610000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.40 +/- 4.76\n",
            "-------------------------------------\n",
            "| approxkl           | 0.16881089   |\n",
            "| clipfrac           | 0.18745117   |\n",
            "| ep_len_mean        | 21.1         |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.867        |\n",
            "| fps                | 404          |\n",
            "| n_updates          | 638          |\n",
            "| policy_entropy     | 0.34633717   |\n",
            "| policy_loss        | -0.043638095 |\n",
            "| serial_timesteps   | 326656       |\n",
            "| time_elapsed       | 6.91e+03     |\n",
            "| total_timesteps    | 2613248      |\n",
            "| value_loss         | 0.059493907  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.19381163   |\n",
            "| clipfrac           | 0.16740723   |\n",
            "| ep_len_mean        | 22           |\n",
            "| ep_reward_mean     | 0.938        |\n",
            "| explained_variance | 0.717        |\n",
            "| fps                | 416          |\n",
            "| n_updates          | 639          |\n",
            "| policy_entropy     | 0.22810669   |\n",
            "| policy_loss        | -0.005039662 |\n",
            "| serial_timesteps   | 327168       |\n",
            "| time_elapsed       | 6.92e+03     |\n",
            "| total_timesteps    | 2617344      |\n",
            "| value_loss         | 0.08786024   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2620000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.20 +/- 122.44\n",
            "-------------------------------------\n",
            "| approxkl           | 0.046421975  |\n",
            "| clipfrac           | 0.21381836   |\n",
            "| ep_len_mean        | 20.5         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.871        |\n",
            "| fps                | 361          |\n",
            "| n_updates          | 640          |\n",
            "| policy_entropy     | 0.4587637    |\n",
            "| policy_loss        | -0.032806374 |\n",
            "| serial_timesteps   | 327680       |\n",
            "| time_elapsed       | 6.93e+03     |\n",
            "| total_timesteps    | 2621440      |\n",
            "| value_loss         | 0.081434384  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.032712054  |\n",
            "| clipfrac           | 0.09943847   |\n",
            "| ep_len_mean        | 19           |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.917        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 641          |\n",
            "| policy_entropy     | 0.23686953   |\n",
            "| policy_loss        | -0.019149108 |\n",
            "| serial_timesteps   | 328192       |\n",
            "| time_elapsed       | 6.94e+03     |\n",
            "| total_timesteps    | 2625536      |\n",
            "| value_loss         | 0.037647475  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.038500704  |\n",
            "| clipfrac           | 0.15549317   |\n",
            "| ep_len_mean        | 21.1         |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.908        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 642          |\n",
            "| policy_entropy     | 0.31423223   |\n",
            "| policy_loss        | -0.037804008 |\n",
            "| serial_timesteps   | 328704       |\n",
            "| time_elapsed       | 6.95e+03     |\n",
            "| total_timesteps    | 2629632      |\n",
            "| value_loss         | 0.05067988   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2630000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.60 +/- 3.93\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02942786   |\n",
            "| clipfrac           | 0.082495116  |\n",
            "| ep_len_mean        | 18.6         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.907        |\n",
            "| fps                | 403          |\n",
            "| n_updates          | 643          |\n",
            "| policy_entropy     | 0.13332269   |\n",
            "| policy_loss        | -0.019015979 |\n",
            "| serial_timesteps   | 329216       |\n",
            "| time_elapsed       | 6.96e+03     |\n",
            "| total_timesteps    | 2633728      |\n",
            "| value_loss         | 0.017275479  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.015656441 |\n",
            "| clipfrac           | 0.07390137  |\n",
            "| ep_len_mean        | 23.4        |\n",
            "| ep_reward_mean     | 0.933       |\n",
            "| explained_variance | 0.777       |\n",
            "| fps                | 413         |\n",
            "| n_updates          | 644         |\n",
            "| policy_entropy     | 0.16567096  |\n",
            "| policy_loss        | -0.0142535  |\n",
            "| serial_timesteps   | 329728      |\n",
            "| time_elapsed       | 6.97e+03    |\n",
            "| total_timesteps    | 2637824     |\n",
            "| value_loss         | 0.09177336  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2640000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.40 +/- 1.85\n",
            "------------------------------------\n",
            "| approxkl           | 0.72504854  |\n",
            "| clipfrac           | 0.045947265 |\n",
            "| ep_len_mean        | 17.1        |\n",
            "| ep_reward_mean     | 0.953       |\n",
            "| explained_variance | 0.701       |\n",
            "| fps                | 406         |\n",
            "| n_updates          | 645         |\n",
            "| policy_entropy     | 0.023384798 |\n",
            "| policy_loss        | 0.024216022 |\n",
            "| serial_timesteps   | 330240      |\n",
            "| time_elapsed       | 6.98e+03    |\n",
            "| total_timesteps    | 2641920     |\n",
            "| value_loss         | 0.01918346  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.051891685  |\n",
            "| clipfrac           | 0.0404541    |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.924        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 646          |\n",
            "| policy_entropy     | 0.03885631   |\n",
            "| policy_loss        | -0.00864747  |\n",
            "| serial_timesteps   | 330752       |\n",
            "| time_elapsed       | 6.99e+03     |\n",
            "| total_timesteps    | 2646016      |\n",
            "| value_loss         | 0.0041980324 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2650000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.60 +/- 123.71\n",
            "-------------------------------------\n",
            "| approxkl           | 6.653001     |\n",
            "| clipfrac           | 0.48085937   |\n",
            "| ep_len_mean        | 29.9         |\n",
            "| ep_reward_mean     | 0.914        |\n",
            "| explained_variance | 0.386        |\n",
            "| fps                | 367          |\n",
            "| n_updates          | 647          |\n",
            "| policy_entropy     | 0.18240747   |\n",
            "| policy_loss        | -0.030173415 |\n",
            "| serial_timesteps   | 331264       |\n",
            "| time_elapsed       | 7e+03        |\n",
            "| total_timesteps    | 2650112      |\n",
            "| value_loss         | 0.2432911    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.6977677    |\n",
            "| clipfrac           | 0.18134765   |\n",
            "| ep_len_mean        | 19.8         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.297        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 648          |\n",
            "| policy_entropy     | 0.10164467   |\n",
            "| policy_loss        | -0.047608115 |\n",
            "| serial_timesteps   | 331776       |\n",
            "| time_elapsed       | 7.01e+03     |\n",
            "| total_timesteps    | 2654208      |\n",
            "| value_loss         | 0.17890233   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.48046023   |\n",
            "| clipfrac           | 0.047021486  |\n",
            "| ep_len_mean        | 17.7         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.873        |\n",
            "| fps                | 424          |\n",
            "| n_updates          | 649          |\n",
            "| policy_entropy     | 0.0473106    |\n",
            "| policy_loss        | -0.016723529 |\n",
            "| serial_timesteps   | 332288       |\n",
            "| time_elapsed       | 7.02e+03     |\n",
            "| total_timesteps    | 2658304      |\n",
            "| value_loss         | 0.0043192566 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2660000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.80 +/- 122.17\n",
            "-------------------------------------\n",
            "| approxkl           | 0.3373764    |\n",
            "| clipfrac           | 0.25039062   |\n",
            "| ep_len_mean        | 24.9         |\n",
            "| ep_reward_mean     | 0.93         |\n",
            "| explained_variance | 0.412        |\n",
            "| fps                | 368          |\n",
            "| n_updates          | 650          |\n",
            "| policy_entropy     | 0.24580109   |\n",
            "| policy_loss        | -0.031287756 |\n",
            "| serial_timesteps   | 332800       |\n",
            "| time_elapsed       | 7.03e+03     |\n",
            "| total_timesteps    | 2662400      |\n",
            "| value_loss         | 0.14959615   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.08587302  |\n",
            "| clipfrac           | 0.22944336  |\n",
            "| ep_len_mean        | 19.1        |\n",
            "| ep_reward_mean     | 0.947       |\n",
            "| explained_variance | 0.631       |\n",
            "| fps                | 424         |\n",
            "| n_updates          | 651         |\n",
            "| policy_entropy     | 0.33235213  |\n",
            "| policy_loss        | -0.05196604 |\n",
            "| serial_timesteps   | 333312      |\n",
            "| time_elapsed       | 7.04e+03    |\n",
            "| total_timesteps    | 2666496     |\n",
            "| value_loss         | 0.07543109  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2670000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.47\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04916459   |\n",
            "| clipfrac           | 0.2517334    |\n",
            "| ep_len_mean        | 30.1         |\n",
            "| ep_reward_mean     | 0.913        |\n",
            "| explained_variance | 0.653        |\n",
            "| fps                | 409          |\n",
            "| n_updates          | 652          |\n",
            "| policy_entropy     | 0.6168825    |\n",
            "| policy_loss        | -0.026938558 |\n",
            "| serial_timesteps   | 333824       |\n",
            "| time_elapsed       | 7.05e+03     |\n",
            "| total_timesteps    | 2670592      |\n",
            "| value_loss         | 0.22174633   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.033261564  |\n",
            "| clipfrac           | 0.20744629   |\n",
            "| ep_len_mean        | 24.3         |\n",
            "| ep_reward_mean     | 0.933        |\n",
            "| explained_variance | 0.794        |\n",
            "| fps                | 424          |\n",
            "| n_updates          | 653          |\n",
            "| policy_entropy     | 0.49853605   |\n",
            "| policy_loss        | -0.042570297 |\n",
            "| serial_timesteps   | 334336       |\n",
            "| time_elapsed       | 7.06e+03     |\n",
            "| total_timesteps    | 2674688      |\n",
            "| value_loss         | 0.08949466   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.24934919   |\n",
            "| clipfrac           | 0.21601562   |\n",
            "| ep_len_mean        | 26.2         |\n",
            "| ep_reward_mean     | 0.926        |\n",
            "| explained_variance | 0.821        |\n",
            "| fps                | 428          |\n",
            "| n_updates          | 654          |\n",
            "| policy_entropy     | 0.45872656   |\n",
            "| policy_loss        | -0.040285256 |\n",
            "| serial_timesteps   | 334848       |\n",
            "| time_elapsed       | 7.07e+03     |\n",
            "| total_timesteps    | 2678784      |\n",
            "| value_loss         | 0.082583204  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2680000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.20 +/- 1.72\n",
            "-----------------------------------\n",
            "| approxkl           | 0.196976   |\n",
            "| clipfrac           | 0.21896973 |\n",
            "| ep_len_mean        | 22         |\n",
            "| ep_reward_mean     | 0.938      |\n",
            "| explained_variance | 0.771      |\n",
            "| fps                | 411        |\n",
            "| n_updates          | 655        |\n",
            "| policy_entropy     | 0.3924968  |\n",
            "| policy_loss        | -0.0457947 |\n",
            "| serial_timesteps   | 335360     |\n",
            "| time_elapsed       | 7.08e+03   |\n",
            "| total_timesteps    | 2682880    |\n",
            "| value_loss         | 0.0977967  |\n",
            "-----------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.022550229 |\n",
            "| clipfrac           | 0.11452637  |\n",
            "| ep_len_mean        | 18.7        |\n",
            "| ep_reward_mean     | 0.948       |\n",
            "| explained_variance | 0.794       |\n",
            "| fps                | 418         |\n",
            "| n_updates          | 656         |\n",
            "| policy_entropy     | 0.31419873  |\n",
            "| policy_loss        | -0.02510919 |\n",
            "| serial_timesteps   | 335872      |\n",
            "| time_elapsed       | 7.09e+03    |\n",
            "| total_timesteps    | 2686976     |\n",
            "| value_loss         | 0.088597566 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2690000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.20 +/- 2.40\n",
            "-------------------------------------\n",
            "| approxkl           | 3.0533156    |\n",
            "| clipfrac           | 0.21289062   |\n",
            "| ep_len_mean        | 26.9         |\n",
            "| ep_reward_mean     | 0.923        |\n",
            "| explained_variance | 0.697        |\n",
            "| fps                | 409          |\n",
            "| n_updates          | 657          |\n",
            "| policy_entropy     | 0.3258332    |\n",
            "| policy_loss        | -0.047535725 |\n",
            "| serial_timesteps   | 336384       |\n",
            "| time_elapsed       | 7.1e+03      |\n",
            "| total_timesteps    | 2691072      |\n",
            "| value_loss         | 0.15572691   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.016527623 |\n",
            "| clipfrac           | 0.07607422  |\n",
            "| ep_len_mean        | 20          |\n",
            "| ep_reward_mean     | 0.943       |\n",
            "| explained_variance | 0.848       |\n",
            "| fps                | 420         |\n",
            "| n_updates          | 658         |\n",
            "| policy_entropy     | 0.16294684  |\n",
            "| policy_loss        | -0.02071515 |\n",
            "| serial_timesteps   | 336896      |\n",
            "| time_elapsed       | 7.11e+03    |\n",
            "| total_timesteps    | 2695168     |\n",
            "| value_loss         | 0.03477664  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.36711186   |\n",
            "| clipfrac           | 0.10424805   |\n",
            "| ep_len_mean        | 18.4         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.833        |\n",
            "| fps                | 426          |\n",
            "| n_updates          | 659          |\n",
            "| policy_entropy     | 0.12309857   |\n",
            "| policy_loss        | -0.017684704 |\n",
            "| serial_timesteps   | 337408       |\n",
            "| time_elapsed       | 7.12e+03     |\n",
            "| total_timesteps    | 2699264      |\n",
            "| value_loss         | 0.021776896  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2700000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.20 +/- 3.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.416501     |\n",
            "| clipfrac           | 0.09938965   |\n",
            "| ep_len_mean        | 19.8         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.832        |\n",
            "| fps                | 406          |\n",
            "| n_updates          | 660          |\n",
            "| policy_entropy     | 0.16885158   |\n",
            "| policy_loss        | -0.020081181 |\n",
            "| serial_timesteps   | 337920       |\n",
            "| time_elapsed       | 7.13e+03     |\n",
            "| total_timesteps    | 2703360      |\n",
            "| value_loss         | 0.030760711  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.46643215  |\n",
            "| clipfrac           | 0.14401856  |\n",
            "| ep_len_mean        | 22.4        |\n",
            "| ep_reward_mean     | 0.937       |\n",
            "| explained_variance | 0.802       |\n",
            "| fps                | 421         |\n",
            "| n_updates          | 661         |\n",
            "| policy_entropy     | 0.20121256  |\n",
            "| policy_loss        | 0.026390076 |\n",
            "| serial_timesteps   | 338432      |\n",
            "| time_elapsed       | 7.14e+03    |\n",
            "| total_timesteps    | 2707456     |\n",
            "| value_loss         | 0.060097136 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2710000, episode_reward=0.95 +/- 0.02\n",
            "Episode length: 17.00 +/- 5.59\n",
            "-------------------------------------\n",
            "| approxkl           | 0.99522513   |\n",
            "| clipfrac           | 0.1317627    |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.908        |\n",
            "| fps                | 409          |\n",
            "| n_updates          | 662          |\n",
            "| policy_entropy     | 0.26169842   |\n",
            "| policy_loss        | -0.043509215 |\n",
            "| serial_timesteps   | 338944       |\n",
            "| time_elapsed       | 7.15e+03     |\n",
            "| total_timesteps    | 2711552      |\n",
            "| value_loss         | 0.05053665   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.028402582  |\n",
            "| clipfrac           | 0.10917969   |\n",
            "| ep_len_mean        | 20.6         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.933        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 663          |\n",
            "| policy_entropy     | 0.28567404   |\n",
            "| policy_loss        | -0.026522437 |\n",
            "| serial_timesteps   | 339456       |\n",
            "| time_elapsed       | 7.16e+03     |\n",
            "| total_timesteps    | 2715648      |\n",
            "| value_loss         | 0.032706495  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024277141  |\n",
            "| clipfrac           | 0.08427735   |\n",
            "| ep_len_mean        | 19.8         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.973        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 664          |\n",
            "| policy_entropy     | 0.29509297   |\n",
            "| policy_loss        | -0.028106228 |\n",
            "| serial_timesteps   | 339968       |\n",
            "| time_elapsed       | 7.17e+03     |\n",
            "| total_timesteps    | 2719744      |\n",
            "| value_loss         | 0.01434529   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2720000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.20 +/- 2.79\n",
            "-------------------------------------\n",
            "| approxkl           | 0.85993624   |\n",
            "| clipfrac           | 0.14865723   |\n",
            "| ep_len_mean        | 19.7         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.711        |\n",
            "| fps                | 410          |\n",
            "| n_updates          | 665          |\n",
            "| policy_entropy     | 0.14336078   |\n",
            "| policy_loss        | -0.043183852 |\n",
            "| serial_timesteps   | 340480       |\n",
            "| time_elapsed       | 7.18e+03     |\n",
            "| total_timesteps    | 2723840      |\n",
            "| value_loss         | 0.086510554  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.32590428  |\n",
            "| clipfrac           | 0.09411621  |\n",
            "| ep_len_mean        | 17.5        |\n",
            "| ep_reward_mean     | 0.951       |\n",
            "| explained_variance | 0.682       |\n",
            "| fps                | 424         |\n",
            "| n_updates          | 666         |\n",
            "| policy_entropy     | 0.07200666  |\n",
            "| policy_loss        | 0.027003542 |\n",
            "| serial_timesteps   | 340992      |\n",
            "| time_elapsed       | 7.19e+03    |\n",
            "| total_timesteps    | 2727936     |\n",
            "| value_loss         | 0.03393533  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2730000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.40 +/- 3.93\n",
            "------------------------------------\n",
            "| approxkl           | 0.5638634   |\n",
            "| clipfrac           | 0.1546875   |\n",
            "| ep_len_mean        | 18.1        |\n",
            "| ep_reward_mean     | 0.95        |\n",
            "| explained_variance | 0.886       |\n",
            "| fps                | 408         |\n",
            "| n_updates          | 667         |\n",
            "| policy_entropy     | 0.19257067  |\n",
            "| policy_loss        | 0.004013838 |\n",
            "| serial_timesteps   | 341504      |\n",
            "| time_elapsed       | 7.2e+03     |\n",
            "| total_timesteps    | 2732032     |\n",
            "| value_loss         | 0.037404962 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.9294613    |\n",
            "| clipfrac           | 0.057763673  |\n",
            "| ep_len_mean        | 16.5         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.675        |\n",
            "| fps                | 420          |\n",
            "| n_updates          | 668          |\n",
            "| policy_entropy     | 0.02940378   |\n",
            "| policy_loss        | -0.025438348 |\n",
            "| serial_timesteps   | 342016       |\n",
            "| time_elapsed       | 7.21e+03     |\n",
            "| total_timesteps    | 2736128      |\n",
            "| value_loss         | 0.03733816   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2740000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.80 +/- 3.19\n",
            "-------------------------------------\n",
            "| approxkl           | 0.05605679   |\n",
            "| clipfrac           | 0.0642334    |\n",
            "| ep_len_mean        | 18           |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.85         |\n",
            "| fps                | 408          |\n",
            "| n_updates          | 669          |\n",
            "| policy_entropy     | 0.10989083   |\n",
            "| policy_loss        | -0.015411501 |\n",
            "| serial_timesteps   | 342528       |\n",
            "| time_elapsed       | 7.22e+03     |\n",
            "| total_timesteps    | 2740224      |\n",
            "| value_loss         | 0.025033241  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.2312335    |\n",
            "| clipfrac           | 0.15473633   |\n",
            "| ep_len_mean        | 20.6         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.742        |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 670          |\n",
            "| policy_entropy     | 0.16468515   |\n",
            "| policy_loss        | -0.044049602 |\n",
            "| serial_timesteps   | 343040       |\n",
            "| time_elapsed       | 7.23e+03     |\n",
            "| total_timesteps    | 2744320      |\n",
            "| value_loss         | 0.06356795   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.038327936  |\n",
            "| clipfrac           | 0.08618164   |\n",
            "| ep_len_mean        | 19.9         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.777        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 671          |\n",
            "| policy_entropy     | 0.13718264   |\n",
            "| policy_loss        | -0.005535812 |\n",
            "| serial_timesteps   | 343552       |\n",
            "| time_elapsed       | 7.24e+03     |\n",
            "| total_timesteps    | 2748416      |\n",
            "| value_loss         | 0.06451942   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2750000, episode_reward=0.94 +/- 0.02\n",
            "Episode length: 20.00 +/- 5.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020784656  |\n",
            "| clipfrac           | 0.0529541    |\n",
            "| ep_len_mean        | 18.5         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.801        |\n",
            "| fps                | 398          |\n",
            "| n_updates          | 672          |\n",
            "| policy_entropy     | 0.07832958   |\n",
            "| policy_loss        | -0.024924165 |\n",
            "| serial_timesteps   | 344064       |\n",
            "| time_elapsed       | 7.25e+03     |\n",
            "| total_timesteps    | 2752512      |\n",
            "| value_loss         | 0.015653865  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03123939   |\n",
            "| clipfrac           | 0.09326172   |\n",
            "| ep_len_mean        | 16.8         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.916        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 673          |\n",
            "| policy_entropy     | 0.16162163   |\n",
            "| policy_loss        | -0.032834534 |\n",
            "| serial_timesteps   | 344576       |\n",
            "| time_elapsed       | 7.26e+03     |\n",
            "| total_timesteps    | 2756608      |\n",
            "| value_loss         | 0.013171451  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2760000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.00 +/- 3.79\n",
            "-------------------------------------\n",
            "| approxkl           | 0.029308956  |\n",
            "| clipfrac           | 0.18862304   |\n",
            "| ep_len_mean        | 22.6         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.882        |\n",
            "| fps                | 416          |\n",
            "| n_updates          | 674          |\n",
            "| policy_entropy     | 0.37508395   |\n",
            "| policy_loss        | -0.038524725 |\n",
            "| serial_timesteps   | 345088       |\n",
            "| time_elapsed       | 7.27e+03     |\n",
            "| total_timesteps    | 2760704      |\n",
            "| value_loss         | 0.0665594    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.1716548   |\n",
            "| clipfrac           | 0.08522949  |\n",
            "| ep_len_mean        | 16.8        |\n",
            "| ep_reward_mean     | 0.953       |\n",
            "| explained_variance | 0.836       |\n",
            "| fps                | 412         |\n",
            "| n_updates          | 675         |\n",
            "| policy_entropy     | 0.12237445  |\n",
            "| policy_loss        | -0.03149963 |\n",
            "| serial_timesteps   | 345600      |\n",
            "| time_elapsed       | 7.28e+03    |\n",
            "| total_timesteps    | 2764800     |\n",
            "| value_loss         | 0.03781008  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.05638182  |\n",
            "| clipfrac           | 0.09475098  |\n",
            "| ep_len_mean        | 17.9        |\n",
            "| ep_reward_mean     | 0.95        |\n",
            "| explained_variance | 0.798       |\n",
            "| fps                | 410         |\n",
            "| n_updates          | 676         |\n",
            "| policy_entropy     | 0.16199812  |\n",
            "| policy_loss        | 0.009403574 |\n",
            "| serial_timesteps   | 346112      |\n",
            "| time_elapsed       | 7.29e+03    |\n",
            "| total_timesteps    | 2768896     |\n",
            "| value_loss         | 0.036068544 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2770000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 17.20 +/- 1.17\n",
            "-----------------------------------\n",
            "| approxkl           | 0.10205907 |\n",
            "| clipfrac           | 0.21738282 |\n",
            "| ep_len_mean        | 26         |\n",
            "| ep_reward_mean     | 0.926      |\n",
            "| explained_variance | 0.807      |\n",
            "| fps                | 402        |\n",
            "| n_updates          | 677        |\n",
            "| policy_entropy     | 0.43109003 |\n",
            "| policy_loss        | 0.11664063 |\n",
            "| serial_timesteps   | 346624     |\n",
            "| time_elapsed       | 7.3e+03    |\n",
            "| total_timesteps    | 2772992    |\n",
            "| value_loss         | 0.11880954 |\n",
            "-----------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0107692005 |\n",
            "| clipfrac           | 0.036132812  |\n",
            "| ep_len_mean        | 15.9         |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.699        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 678          |\n",
            "| policy_entropy     | 0.06387595   |\n",
            "| policy_loss        | -0.012470601 |\n",
            "| serial_timesteps   | 347136       |\n",
            "| time_elapsed       | 7.31e+03     |\n",
            "| total_timesteps    | 2777088      |\n",
            "| value_loss         | 0.03459934   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2780000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.20 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023067739  |\n",
            "| clipfrac           | 0.09914551   |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.888        |\n",
            "| fps                | 406          |\n",
            "| n_updates          | 679          |\n",
            "| policy_entropy     | 0.21425231   |\n",
            "| policy_loss        | -0.039683003 |\n",
            "| serial_timesteps   | 347648       |\n",
            "| time_elapsed       | 7.32e+03     |\n",
            "| total_timesteps    | 2781184      |\n",
            "| value_loss         | 0.026541105  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02354427   |\n",
            "| clipfrac           | 0.084423825  |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.913        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 680          |\n",
            "| policy_entropy     | 0.20429945   |\n",
            "| policy_loss        | -0.030225301 |\n",
            "| serial_timesteps   | 348160       |\n",
            "| time_elapsed       | 7.33e+03     |\n",
            "| total_timesteps    | 2785280      |\n",
            "| value_loss         | 0.025547713  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.011819582 |\n",
            "| clipfrac           | 0.07338867  |\n",
            "| ep_len_mean        | 19.4        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.937       |\n",
            "| fps                | 429         |\n",
            "| n_updates          | 681         |\n",
            "| policy_entropy     | 0.24247849  |\n",
            "| policy_loss        | -0.02468976 |\n",
            "| serial_timesteps   | 348672      |\n",
            "| time_elapsed       | 7.34e+03    |\n",
            "| total_timesteps    | 2789376     |\n",
            "| value_loss         | 0.032067724 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2790000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.00 +/- 0.89\n",
            "--------------------------------------\n",
            "| approxkl           | 0.04947008    |\n",
            "| clipfrac           | 0.021142578   |\n",
            "| ep_len_mean        | 16.6          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.956         |\n",
            "| fps                | 407           |\n",
            "| n_updates          | 682           |\n",
            "| policy_entropy     | 0.024739068   |\n",
            "| policy_loss        | -0.0054773437 |\n",
            "| serial_timesteps   | 349184        |\n",
            "| time_elapsed       | 7.35e+03      |\n",
            "| total_timesteps    | 2793472       |\n",
            "| value_loss         | 0.0017901137  |\n",
            "--------------------------------------\n",
            "-----------------------------------\n",
            "| approxkl           | 0.12710235 |\n",
            "| clipfrac           | 0.14562988 |\n",
            "| ep_len_mean        | 20.1       |\n",
            "| ep_reward_mean     | 0.944      |\n",
            "| explained_variance | 0.388      |\n",
            "| fps                | 424        |\n",
            "| n_updates          | 683        |\n",
            "| policy_entropy     | 0.09353056 |\n",
            "| policy_loss        | 0.16782735 |\n",
            "| serial_timesteps   | 349696     |\n",
            "| time_elapsed       | 7.36e+03   |\n",
            "| total_timesteps    | 2797568    |\n",
            "| value_loss         | 0.06822234 |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=2800000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.40 +/- 123.80\n",
            "------------------------------------\n",
            "| approxkl           | 0.6261804   |\n",
            "| clipfrac           | 0.16071777  |\n",
            "| ep_len_mean        | 23.7        |\n",
            "| ep_reward_mean     | 0.932       |\n",
            "| explained_variance | 0.509       |\n",
            "| fps                | 369         |\n",
            "| n_updates          | 684         |\n",
            "| policy_entropy     | 0.09989739  |\n",
            "| policy_loss        | 0.008425421 |\n",
            "| serial_timesteps   | 350208      |\n",
            "| time_elapsed       | 7.37e+03    |\n",
            "| total_timesteps    | 2801664     |\n",
            "| value_loss         | 0.11265882  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024120662  |\n",
            "| clipfrac           | 0.09848633   |\n",
            "| ep_len_mean        | 21.3         |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.882        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 685          |\n",
            "| policy_entropy     | 0.2296638    |\n",
            "| policy_loss        | -0.017267477 |\n",
            "| serial_timesteps   | 350720       |\n",
            "| time_elapsed       | 7.38e+03     |\n",
            "| total_timesteps    | 2805760      |\n",
            "| value_loss         | 0.030085873  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.031189922   |\n",
            "| clipfrac           | 0.03967285    |\n",
            "| ep_len_mean        | 16.3          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.891         |\n",
            "| fps                | 421           |\n",
            "| n_updates          | 686           |\n",
            "| policy_entropy     | 0.049777456   |\n",
            "| policy_loss        | -0.0124601135 |\n",
            "| serial_timesteps   | 351232        |\n",
            "| time_elapsed       | 7.39e+03      |\n",
            "| total_timesteps    | 2809856       |\n",
            "| value_loss         | 0.005408936   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2810000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.40 +/- 2.06\n",
            "------------------------------------\n",
            "| approxkl           | 0.04722684  |\n",
            "| clipfrac           | 0.12583008  |\n",
            "| ep_len_mean        | 19.1        |\n",
            "| ep_reward_mean     | 0.947       |\n",
            "| explained_variance | 0.812       |\n",
            "| fps                | 408         |\n",
            "| n_updates          | 687         |\n",
            "| policy_entropy     | 0.23343244  |\n",
            "| policy_loss        | -0.04070553 |\n",
            "| serial_timesteps   | 351744      |\n",
            "| time_elapsed       | 7.4e+03     |\n",
            "| total_timesteps    | 2813952     |\n",
            "| value_loss         | 0.030268654 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.026477164 |\n",
            "| clipfrac           | 0.14265136  |\n",
            "| ep_len_mean        | 19.9        |\n",
            "| ep_reward_mean     | 0.944       |\n",
            "| explained_variance | 0.946       |\n",
            "| fps                | 427         |\n",
            "| n_updates          | 688         |\n",
            "| policy_entropy     | 0.38524085  |\n",
            "| policy_loss        | -0.03540107 |\n",
            "| serial_timesteps   | 352256      |\n",
            "| time_elapsed       | 7.41e+03    |\n",
            "| total_timesteps    | 2818048     |\n",
            "| value_loss         | 0.03132892  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2820000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.40 +/- 1.85\n",
            "-------------------------------------\n",
            "| approxkl           | 0.053431183  |\n",
            "| clipfrac           | 0.14807129   |\n",
            "| ep_len_mean        | 17.3         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.775        |\n",
            "| fps                | 412          |\n",
            "| n_updates          | 689          |\n",
            "| policy_entropy     | 0.27827442   |\n",
            "| policy_loss        | -0.038670097 |\n",
            "| serial_timesteps   | 352768       |\n",
            "| time_elapsed       | 7.42e+03     |\n",
            "| total_timesteps    | 2822144      |\n",
            "| value_loss         | 0.06977662   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022604967  |\n",
            "| clipfrac           | 0.09682617   |\n",
            "| ep_len_mean        | 16.3         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.867        |\n",
            "| fps                | 415          |\n",
            "| n_updates          | 690          |\n",
            "| policy_entropy     | 0.22618589   |\n",
            "| policy_loss        | -0.008015727 |\n",
            "| serial_timesteps   | 353280       |\n",
            "| time_elapsed       | 7.43e+03     |\n",
            "| total_timesteps    | 2826240      |\n",
            "| value_loss         | 0.04548203   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2830000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.80 +/- 2.23\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022205021  |\n",
            "| clipfrac           | 0.10893555   |\n",
            "| ep_len_mean        | 19.5         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.876        |\n",
            "| fps                | 413          |\n",
            "| n_updates          | 691          |\n",
            "| policy_entropy     | 0.24108282   |\n",
            "| policy_loss        | -0.039820366 |\n",
            "| serial_timesteps   | 353792       |\n",
            "| time_elapsed       | 7.44e+03     |\n",
            "| total_timesteps    | 2830336      |\n",
            "| value_loss         | 0.027317911  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.019439055 |\n",
            "| clipfrac           | 0.071826175 |\n",
            "| ep_len_mean        | 18.5        |\n",
            "| ep_reward_mean     | 0.949       |\n",
            "| explained_variance | 0.901       |\n",
            "| fps                | 407         |\n",
            "| n_updates          | 692         |\n",
            "| policy_entropy     | 0.14957353  |\n",
            "| policy_loss        | -0.02972371 |\n",
            "| serial_timesteps   | 354304      |\n",
            "| time_elapsed       | 7.45e+03    |\n",
            "| total_timesteps    | 2834432     |\n",
            "| value_loss         | 0.012491658 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 2.1618752    |\n",
            "| clipfrac           | 0.13339844   |\n",
            "| ep_len_mean        | 21.4         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.74         |\n",
            "| fps                | 413          |\n",
            "| n_updates          | 693          |\n",
            "| policy_entropy     | 0.14255454   |\n",
            "| policy_loss        | -0.049636867 |\n",
            "| serial_timesteps   | 354816       |\n",
            "| time_elapsed       | 7.46e+03     |\n",
            "| total_timesteps    | 2838528      |\n",
            "| value_loss         | 0.0644173    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2840000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.17\n",
            "------------------------------------\n",
            "| approxkl           | 0.2865548   |\n",
            "| clipfrac           | 0.097802736 |\n",
            "| ep_len_mean        | 16.8        |\n",
            "| ep_reward_mean     | 0.953       |\n",
            "| explained_variance | 0.937       |\n",
            "| fps                | 406         |\n",
            "| n_updates          | 694         |\n",
            "| policy_entropy     | 0.19714144  |\n",
            "| policy_loss        | -0.03192451 |\n",
            "| serial_timesteps   | 355328      |\n",
            "| time_elapsed       | 7.47e+03    |\n",
            "| total_timesteps    | 2842624     |\n",
            "| value_loss         | 0.013412148 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.047271654  |\n",
            "| clipfrac           | 0.09841309   |\n",
            "| ep_len_mean        | 16.4         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.915        |\n",
            "| fps                | 420          |\n",
            "| n_updates          | 695          |\n",
            "| policy_entropy     | 0.25591877   |\n",
            "| policy_loss        | -0.033131972 |\n",
            "| serial_timesteps   | 355840       |\n",
            "| time_elapsed       | 7.48e+03     |\n",
            "| total_timesteps    | 2846720      |\n",
            "| value_loss         | 0.044753008  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2850000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.20 +/- 2.32\n",
            "------------------------------------\n",
            "| approxkl           | 0.045971848 |\n",
            "| clipfrac           | 0.062426757 |\n",
            "| ep_len_mean        | 16.4        |\n",
            "| ep_reward_mean     | 0.954       |\n",
            "| explained_variance | 0.961       |\n",
            "| fps                | 410         |\n",
            "| n_updates          | 696         |\n",
            "| policy_entropy     | 0.12907846  |\n",
            "| policy_loss        | -0.02520448 |\n",
            "| serial_timesteps   | 356352      |\n",
            "| time_elapsed       | 7.49e+03    |\n",
            "| total_timesteps    | 2850816     |\n",
            "| value_loss         | 0.007819345 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.46370363   |\n",
            "| clipfrac           | 0.08537598   |\n",
            "| ep_len_mean        | 17.9         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.935        |\n",
            "| fps                | 426          |\n",
            "| n_updates          | 697          |\n",
            "| policy_entropy     | 0.14663312   |\n",
            "| policy_loss        | -0.033714574 |\n",
            "| serial_timesteps   | 356864       |\n",
            "| time_elapsed       | 7.5e+03      |\n",
            "| total_timesteps    | 2854912      |\n",
            "| value_loss         | 0.017835513  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0070757084 |\n",
            "| clipfrac           | 0.042260744  |\n",
            "| ep_len_mean        | 18.7         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.967        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 698          |\n",
            "| policy_entropy     | 0.09610235   |\n",
            "| policy_loss        | -0.020569136 |\n",
            "| serial_timesteps   | 357376       |\n",
            "| time_elapsed       | 7.51e+03     |\n",
            "| total_timesteps    | 2859008      |\n",
            "| value_loss         | 0.007609076  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2860000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.20 +/- 3.19\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022329282  |\n",
            "| clipfrac           | 0.10788574   |\n",
            "| ep_len_mean        | 21.3         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.963        |\n",
            "| fps                | 408          |\n",
            "| n_updates          | 699          |\n",
            "| policy_entropy     | 0.2910494    |\n",
            "| policy_loss        | -0.033158407 |\n",
            "| serial_timesteps   | 357888       |\n",
            "| time_elapsed       | 7.52e+03     |\n",
            "| total_timesteps    | 2863104      |\n",
            "| value_loss         | 0.019374743  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.9377524    |\n",
            "| clipfrac           | 0.15310058   |\n",
            "| ep_len_mean        | 18.8         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.905        |\n",
            "| fps                | 427          |\n",
            "| n_updates          | 700          |\n",
            "| policy_entropy     | 0.23233211   |\n",
            "| policy_loss        | -0.050298125 |\n",
            "| serial_timesteps   | 358400       |\n",
            "| time_elapsed       | 7.53e+03     |\n",
            "| total_timesteps    | 2867200      |\n",
            "| value_loss         | 0.031887118  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2870000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.20 +/- 3.92\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014634369  |\n",
            "| clipfrac           | 0.049365234  |\n",
            "| ep_len_mean        | 16.7         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.871        |\n",
            "| fps                | 403          |\n",
            "| n_updates          | 701          |\n",
            "| policy_entropy     | 0.11292164   |\n",
            "| policy_loss        | -0.020043546 |\n",
            "| serial_timesteps   | 358912       |\n",
            "| time_elapsed       | 7.54e+03     |\n",
            "| total_timesteps    | 2871296      |\n",
            "| value_loss         | 0.03494588   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019497829  |\n",
            "| clipfrac           | 0.11611328   |\n",
            "| ep_len_mean        | 21.5         |\n",
            "| ep_reward_mean     | 0.939        |\n",
            "| explained_variance | 0.921        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 702          |\n",
            "| policy_entropy     | 0.27942422   |\n",
            "| policy_loss        | -0.035018317 |\n",
            "| serial_timesteps   | 359424       |\n",
            "| time_elapsed       | 7.55e+03     |\n",
            "| total_timesteps    | 2875392      |\n",
            "| value_loss         | 0.033743106  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.012845568  |\n",
            "| clipfrac           | 0.04387207   |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.906        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 703          |\n",
            "| policy_entropy     | 0.09760758   |\n",
            "| policy_loss        | -0.025499474 |\n",
            "| serial_timesteps   | 359936       |\n",
            "| time_elapsed       | 7.56e+03     |\n",
            "| total_timesteps    | 2879488      |\n",
            "| value_loss         | 0.019511636  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2880000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.80 +/- 0.98\n",
            "------------------------------------\n",
            "| approxkl           | 3.4747753   |\n",
            "| clipfrac           | 0.13278809  |\n",
            "| ep_len_mean        | 23.5        |\n",
            "| ep_reward_mean     | 0.934       |\n",
            "| explained_variance | 0.874       |\n",
            "| fps                | 410         |\n",
            "| n_updates          | 704         |\n",
            "| policy_entropy     | 0.18049742  |\n",
            "| policy_loss        | -0.05153189 |\n",
            "| serial_timesteps   | 360448      |\n",
            "| time_elapsed       | 7.57e+03    |\n",
            "| total_timesteps    | 2883584     |\n",
            "| value_loss         | 0.06125576  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.058692902  |\n",
            "| clipfrac           | 0.028295899  |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.937        |\n",
            "| fps                | 422          |\n",
            "| n_updates          | 705          |\n",
            "| policy_entropy     | 0.037286464  |\n",
            "| policy_loss        | -0.013219601 |\n",
            "| serial_timesteps   | 360960       |\n",
            "| time_elapsed       | 7.58e+03     |\n",
            "| total_timesteps    | 2887680      |\n",
            "| value_loss         | 0.0032839451 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2890000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 1.79\n",
            "-------------------------------------\n",
            "| approxkl           | 0.23388319   |\n",
            "| clipfrac           | 0.07614746   |\n",
            "| ep_len_mean        | 17.2         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.859        |\n",
            "| fps                | 402          |\n",
            "| n_updates          | 706          |\n",
            "| policy_entropy     | 0.082165316  |\n",
            "| policy_loss        | -0.008195235 |\n",
            "| serial_timesteps   | 361472       |\n",
            "| time_elapsed       | 7.59e+03     |\n",
            "| total_timesteps    | 2891776      |\n",
            "| value_loss         | 0.011279862  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.569559     |\n",
            "| clipfrac           | 0.06298828   |\n",
            "| ep_len_mean        | 16.9         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.779        |\n",
            "| fps                | 415          |\n",
            "| n_updates          | 707          |\n",
            "| policy_entropy     | 0.061547417  |\n",
            "| policy_loss        | -0.028068965 |\n",
            "| serial_timesteps   | 361984       |\n",
            "| time_elapsed       | 7.6e+03      |\n",
            "| total_timesteps    | 2895872      |\n",
            "| value_loss         | 0.015158936  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 1.3538704   |\n",
            "| clipfrac           | 0.11025391  |\n",
            "| ep_len_mean        | 20.1        |\n",
            "| ep_reward_mean     | 0.943       |\n",
            "| explained_variance | 0.666       |\n",
            "| fps                | 422         |\n",
            "| n_updates          | 708         |\n",
            "| policy_entropy     | 0.07484315  |\n",
            "| policy_loss        | -0.04245744 |\n",
            "| serial_timesteps   | 362496      |\n",
            "| time_elapsed       | 7.61e+03    |\n",
            "| total_timesteps    | 2899968     |\n",
            "| value_loss         | 0.056612976 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2900000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.40 +/- 1.85\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01946664   |\n",
            "| clipfrac           | 0.059936523  |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.929        |\n",
            "| fps                | 404          |\n",
            "| n_updates          | 709          |\n",
            "| policy_entropy     | 0.10208106   |\n",
            "| policy_loss        | -0.019736955 |\n",
            "| serial_timesteps   | 363008       |\n",
            "| time_elapsed       | 7.62e+03     |\n",
            "| total_timesteps    | 2904064      |\n",
            "| value_loss         | 0.0049833646 |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.035837047 |\n",
            "| clipfrac           | 0.093481444 |\n",
            "| ep_len_mean        | 18.5        |\n",
            "| ep_reward_mean     | 0.949       |\n",
            "| explained_variance | 0.834       |\n",
            "| fps                | 423         |\n",
            "| n_updates          | 710         |\n",
            "| policy_entropy     | 0.17460974  |\n",
            "| policy_loss        | -0.03625444 |\n",
            "| serial_timesteps   | 363520      |\n",
            "| time_elapsed       | 7.63e+03    |\n",
            "| total_timesteps    | 2908160     |\n",
            "| value_loss         | 0.019686496 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2910000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 0.75\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03365501   |\n",
            "| clipfrac           | 0.072216794  |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.918        |\n",
            "| fps                | 399          |\n",
            "| n_updates          | 711          |\n",
            "| policy_entropy     | 0.12864612   |\n",
            "| policy_loss        | -0.027762935 |\n",
            "| serial_timesteps   | 364032       |\n",
            "| time_elapsed       | 7.63e+03     |\n",
            "| total_timesteps    | 2912256      |\n",
            "| value_loss         | 0.0056901025 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 1.514943     |\n",
            "| clipfrac           | 0.12983398   |\n",
            "| ep_len_mean        | 21           |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.513        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 712          |\n",
            "| policy_entropy     | 0.11982407   |\n",
            "| policy_loss        | -0.048470717 |\n",
            "| serial_timesteps   | 364544       |\n",
            "| time_elapsed       | 7.65e+03     |\n",
            "| total_timesteps    | 2916352      |\n",
            "| value_loss         | 0.078312896  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2920000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.60 +/- 0.49\n",
            "------------------------------------\n",
            "| approxkl           | 0.25726748  |\n",
            "| clipfrac           | 0.13618164  |\n",
            "| ep_len_mean        | 23.4        |\n",
            "| ep_reward_mean     | 0.934       |\n",
            "| explained_variance | 0.826       |\n",
            "| fps                | 406         |\n",
            "| n_updates          | 713         |\n",
            "| policy_entropy     | 0.21668322  |\n",
            "| policy_loss        | 0.043442287 |\n",
            "| serial_timesteps   | 365056      |\n",
            "| time_elapsed       | 7.66e+03    |\n",
            "| total_timesteps    | 2920448     |\n",
            "| value_loss         | 0.04906444  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010688614  |\n",
            "| clipfrac           | 0.07692871   |\n",
            "| ep_len_mean        | 20.1         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.919        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 714          |\n",
            "| policy_entropy     | 0.18838507   |\n",
            "| policy_loss        | -0.030560073 |\n",
            "| serial_timesteps   | 365568       |\n",
            "| time_elapsed       | 7.67e+03     |\n",
            "| total_timesteps    | 2924544      |\n",
            "| value_loss         | 0.022340637  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013658273  |\n",
            "| clipfrac           | 0.036694337  |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.941        |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 715          |\n",
            "| policy_entropy     | 0.07748508   |\n",
            "| policy_loss        | -0.016918499 |\n",
            "| serial_timesteps   | 366080       |\n",
            "| time_elapsed       | 7.67e+03     |\n",
            "| total_timesteps    | 2928640      |\n",
            "| value_loss         | 0.003593109  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2930000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.20 +/- 2.14\n",
            "------------------------------------\n",
            "| approxkl           | 0.108045794 |\n",
            "| clipfrac           | 0.10256348  |\n",
            "| ep_len_mean        | 20          |\n",
            "| ep_reward_mean     | 0.944       |\n",
            "| explained_variance | 0.922       |\n",
            "| fps                | 404         |\n",
            "| n_updates          | 716         |\n",
            "| policy_entropy     | 0.22339413  |\n",
            "| policy_loss        | -0.03755805 |\n",
            "| serial_timesteps   | 366592      |\n",
            "| time_elapsed       | 7.68e+03    |\n",
            "| total_timesteps    | 2932736     |\n",
            "| value_loss         | 0.026875641 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04930438   |\n",
            "| clipfrac           | 0.06418457   |\n",
            "| ep_len_mean        | 16.9         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.955        |\n",
            "| fps                | 423          |\n",
            "| n_updates          | 717          |\n",
            "| policy_entropy     | 0.108567216  |\n",
            "| policy_loss        | -0.028099786 |\n",
            "| serial_timesteps   | 367104       |\n",
            "| time_elapsed       | 7.69e+03     |\n",
            "| total_timesteps    | 2936832      |\n",
            "| value_loss         | 0.005533365  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2940000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.60 +/- 1.02\n",
            "------------------------------------\n",
            "| approxkl           | 1.0691378   |\n",
            "| clipfrac           | 0.14421387  |\n",
            "| ep_len_mean        | 22.1        |\n",
            "| ep_reward_mean     | 0.938       |\n",
            "| explained_variance | 0.805       |\n",
            "| fps                | 405         |\n",
            "| n_updates          | 718         |\n",
            "| policy_entropy     | 0.16622582  |\n",
            "| policy_loss        | -0.05089025 |\n",
            "| serial_timesteps   | 367616      |\n",
            "| time_elapsed       | 7.7e+03     |\n",
            "| total_timesteps    | 2940928     |\n",
            "| value_loss         | 0.07201809  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.33804303   |\n",
            "| clipfrac           | 0.039038084  |\n",
            "| ep_len_mean        | 16.2         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.951        |\n",
            "| fps                | 419          |\n",
            "| n_updates          | 719          |\n",
            "| policy_entropy     | 0.041361347  |\n",
            "| policy_loss        | -0.015153678 |\n",
            "| serial_timesteps   | 368128       |\n",
            "| time_elapsed       | 7.71e+03     |\n",
            "| total_timesteps    | 2945024      |\n",
            "| value_loss         | 0.002065015  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.36851057  |\n",
            "| clipfrac           | 0.24106446  |\n",
            "| ep_len_mean        | 25.7        |\n",
            "| ep_reward_mean     | 0.927       |\n",
            "| explained_variance | 0.523       |\n",
            "| fps                | 418         |\n",
            "| n_updates          | 720         |\n",
            "| policy_entropy     | 0.2634579   |\n",
            "| policy_loss        | 0.008643763 |\n",
            "| serial_timesteps   | 368640      |\n",
            "| time_elapsed       | 7.72e+03    |\n",
            "| total_timesteps    | 2949120     |\n",
            "| value_loss         | 0.16534579  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2950000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.80 +/- 3.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024826728  |\n",
            "| clipfrac           | 0.12165527   |\n",
            "| ep_len_mean        | 20.4         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.839        |\n",
            "| fps                | 404          |\n",
            "| n_updates          | 721          |\n",
            "| policy_entropy     | 0.25921804   |\n",
            "| policy_loss        | -0.037696667 |\n",
            "| serial_timesteps   | 369152       |\n",
            "| time_elapsed       | 7.73e+03     |\n",
            "| total_timesteps    | 2953216      |\n",
            "| value_loss         | 0.05811148   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.04212482  |\n",
            "| clipfrac           | 0.107177734 |\n",
            "| ep_len_mean        | 18.9        |\n",
            "| ep_reward_mean     | 0.948       |\n",
            "| explained_variance | 0.964       |\n",
            "| fps                | 419         |\n",
            "| n_updates          | 722         |\n",
            "| policy_entropy     | 0.21202102  |\n",
            "| policy_loss        | -0.03585658 |\n",
            "| serial_timesteps   | 369664      |\n",
            "| time_elapsed       | 7.74e+03    |\n",
            "| total_timesteps    | 2957312     |\n",
            "| value_loss         | 0.007485663 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2960000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.20 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031122953  |\n",
            "| clipfrac           | 0.08254395   |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.912        |\n",
            "| fps                | 408          |\n",
            "| n_updates          | 723          |\n",
            "| policy_entropy     | 0.15629837   |\n",
            "| policy_loss        | -0.034605745 |\n",
            "| serial_timesteps   | 370176       |\n",
            "| time_elapsed       | 7.75e+03     |\n",
            "| total_timesteps    | 2961408      |\n",
            "| value_loss         | 0.011639535  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 2.9971566   |\n",
            "| clipfrac           | 0.21721192  |\n",
            "| ep_len_mean        | 21.7        |\n",
            "| ep_reward_mean     | 0.939       |\n",
            "| explained_variance | 0.837       |\n",
            "| fps                | 419         |\n",
            "| n_updates          | 724         |\n",
            "| policy_entropy     | 0.3342166   |\n",
            "| policy_loss        | -0.04295043 |\n",
            "| serial_timesteps   | 370688      |\n",
            "| time_elapsed       | 7.76e+03    |\n",
            "| total_timesteps    | 2965504     |\n",
            "| value_loss         | 0.0777282   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.022552405 |\n",
            "| clipfrac           | 0.10539551  |\n",
            "| ep_len_mean        | 19.7        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.884       |\n",
            "| fps                | 421         |\n",
            "| n_updates          | 725         |\n",
            "| policy_entropy     | 0.2544933   |\n",
            "| policy_loss        | -0.02651212 |\n",
            "| serial_timesteps   | 371200      |\n",
            "| time_elapsed       | 7.77e+03    |\n",
            "| total_timesteps    | 2969600     |\n",
            "| value_loss         | 0.04254333  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2970000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.60 +/- 123.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.06857447   |\n",
            "| clipfrac           | 0.09462891   |\n",
            "| ep_len_mean        | 18.2         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.803        |\n",
            "| fps                | 367          |\n",
            "| n_updates          | 726          |\n",
            "| policy_entropy     | 0.16979337   |\n",
            "| policy_loss        | -0.009215604 |\n",
            "| serial_timesteps   | 371712       |\n",
            "| time_elapsed       | 7.78e+03     |\n",
            "| total_timesteps    | 2973696      |\n",
            "| value_loss         | 0.051853232  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.025735203  |\n",
            "| clipfrac           | 0.14729004   |\n",
            "| ep_len_mean        | 24.8         |\n",
            "| ep_reward_mean     | 0.929        |\n",
            "| explained_variance | 0.811        |\n",
            "| fps                | 417          |\n",
            "| n_updates          | 727          |\n",
            "| policy_entropy     | 0.36912182   |\n",
            "| policy_loss        | -0.030341279 |\n",
            "| serial_timesteps   | 372224       |\n",
            "| time_elapsed       | 7.79e+03     |\n",
            "| total_timesteps    | 2977792      |\n",
            "| value_loss         | 0.08330171   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2980000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.60 +/- 1.02\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022804221  |\n",
            "| clipfrac           | 0.082617186  |\n",
            "| ep_len_mean        | 16.8         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.817        |\n",
            "| fps                | 407          |\n",
            "| n_updates          | 728          |\n",
            "| policy_entropy     | 0.14640698   |\n",
            "| policy_loss        | -0.027474424 |\n",
            "| serial_timesteps   | 372736       |\n",
            "| time_elapsed       | 7.8e+03      |\n",
            "| total_timesteps    | 2981888      |\n",
            "| value_loss         | 0.027203891  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 2.9637933    |\n",
            "| clipfrac           | 0.15476075   |\n",
            "| ep_len_mean        | 24.2         |\n",
            "| ep_reward_mean     | 0.932        |\n",
            "| explained_variance | 0.542        |\n",
            "| fps                | 428          |\n",
            "| n_updates          | 729          |\n",
            "| policy_entropy     | 0.10125842   |\n",
            "| policy_loss        | -0.032872595 |\n",
            "| serial_timesteps   | 373248       |\n",
            "| time_elapsed       | 7.81e+03     |\n",
            "| total_timesteps    | 2985984      |\n",
            "| value_loss         | 0.10789041   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2990000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.20 +/- 2.32\n",
            "------------------------------------\n",
            "| approxkl           | 0.1309016   |\n",
            "| clipfrac           | 0.16901855  |\n",
            "| ep_len_mean        | 25.1        |\n",
            "| ep_reward_mean     | 0.928       |\n",
            "| explained_variance | 0.897       |\n",
            "| fps                | 405         |\n",
            "| n_updates          | 730         |\n",
            "| policy_entropy     | 0.43580228  |\n",
            "| policy_loss        | -0.04315377 |\n",
            "| serial_timesteps   | 373760      |\n",
            "| time_elapsed       | 7.82e+03    |\n",
            "| total_timesteps    | 2990080     |\n",
            "| value_loss         | 0.055450767 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.1969558    |\n",
            "| clipfrac           | 0.17924805   |\n",
            "| ep_len_mean        | 20.4         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.841        |\n",
            "| fps                | 418          |\n",
            "| n_updates          | 731          |\n",
            "| policy_entropy     | 0.37937653   |\n",
            "| policy_loss        | -0.031971928 |\n",
            "| serial_timesteps   | 374272       |\n",
            "| time_elapsed       | 7.83e+03     |\n",
            "| total_timesteps    | 2994176      |\n",
            "| value_loss         | 0.067651264  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.24173018   |\n",
            "| clipfrac           | 0.18103027   |\n",
            "| ep_len_mean        | 22.5         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.719        |\n",
            "| fps                | 421          |\n",
            "| n_updates          | 732          |\n",
            "| policy_entropy     | 0.31689245   |\n",
            "| policy_loss        | -0.038835436 |\n",
            "| serial_timesteps   | 374784       |\n",
            "| time_elapsed       | 7.84e+03     |\n",
            "| total_timesteps    | 2998272      |\n",
            "| value_loss         | 0.107374236  |\n",
            "-------------------------------------\n",
            "WARNING:tensorflow:From train.py:431: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "Saving to logs/ppo2/MiniGrid-SimpleCrossingS9N3-v0_1\n",
            "\u001b[0m[ddd19424c71e:03326] *** Process received signal ***\n",
            "[ddd19424c71e:03326] Signal: Segmentation fault (11)\n",
            "[ddd19424c71e:03326] Signal code: Address not mapped (1)\n",
            "[ddd19424c71e:03326] Failing at address: 0x7fcdcc37720d\n",
            "[ddd19424c71e:03326] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fcdcf62c980]\n",
            "[ddd19424c71e:03326] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fcdcf26b8a5]\n",
            "[ddd19424c71e:03326] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fcdcfad6e44]\n",
            "[ddd19424c71e:03326] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fcdcf26c735]\n",
            "[ddd19424c71e:03326] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fcdcfad4cb3]\n",
            "[ddd19424c71e:03326] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fHBq73665yD"
      },
      "source": [
        "#### Evaluate trained agent\n",
        "\n",
        "\n",
        "You can remove the `--folder logs/` to evaluate pretrained agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8YuEgU6bT3"
      },
      "source": [
        "!python enjoy.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --no-render --n-timesteps 5000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Il2J0VHPLC"
      },
      "source": [
        "#### Tune Hyperparameters\n",
        "\n",
        "We use [Optuna](https://optuna.org/) for optimizing the hyperparameters.\n",
        "\n",
        "Tune the hyperparameters for PPO2, using a tpe sampler and median pruner, 2 parallels jobs,\n",
        "with a budget of 1000 trials and a maximum of 50000 steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Record  a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS1VBBaQ_emT"
      },
      "source": [
        "!pip install pyglet==1.3.1  # pyglet v1.4.1 throws an error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip3AauLzwNGP"
      },
      "source": [
        "!python -m utils.record_video --algo ppo2 --env MiniGrid-SimpleCrossingEnvUmaze-v0 --gym-packages gym_minigrid --exp-id 0 -f logs/ -n 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBuUfnzI8DN6"
      },
      "source": [
        "### Display the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3OTfpf8CXu"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOjFuwK9HI0"
      },
      "source": [
        "show_videos(prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjdpP0HE8D2p"
      },
      "source": [
        "### Continue Training\n",
        "\n",
        "Here, we will continue training of the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgMZQJJF6u1C"
      },
      "source": [
        "!python train.py --algo a2c --env CartPole-v1 --n-timesteps 50000 -i logs/a2c/CartPole-v1.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSaoyiAE8cVj"
      },
      "source": [
        "!python enjoy.py --algo a2c --env CartPole-v1 --no-render --n-timesteps 1000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9u4I1H-48O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}