{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codigo zoo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpg/RL-Agent-for-Unreal-Engine/blob/main/Codigo_zoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy9QoDC7XA7"
      },
      "source": [
        "# RL Baselines Zoo: Training in Colab\n",
        "\n",
        "\n",
        "\n",
        "Github Repo: [https://github.com/araffin/rl-baselines-zoo](https://github.com/araffin/rl-baselines-zoo)\n",
        "\n",
        "Stable-Baselines Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
        "\n",
        "# Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXVDDlTn02M9",
        "outputId": "aab6f6bd-fc14-43d5-a51c-74a62f84e85c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt-get update\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg freeglut3-dev xvfb\n",
        "!pip install stable-baselines[mpi] --upgrade\n",
        "!pip install pybullet\n",
        "!pip install box2d box2d-kengz pyyaml pytablewriter optuna scikit-optimize\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [407 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,688 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,354 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,119 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,750 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,167 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.6 kB]\n",
            "Fetched 10.7 MB in 2s (4,308 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0 xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 1,884 kB of archives.\n",
            "After this operation, 8,089 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.7 [783 kB]\n",
            "Fetched 1,884 kB in 1s (2,451 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 144786 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.7_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines[mpi]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl (240kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.15.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.1\n",
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/61/2fc2c19327966ca4e4133211be3f4dcc56c1ee6f392d71d0da8c6c1a4cba/pybullet-3.0.6-cp36-cp36m-manylinux1_x86_64.whl (102.2MB)\n",
            "\u001b[K     |████████████████████████████████| 102.2MB 120kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.0.6\n",
            "Collecting box2d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 10.9MB/s \n",
            "\u001b[?25hCollecting box2d-kengz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 57.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Collecting pytablewriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.2MB/s \n",
            "\u001b[?25hCollecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 43.5MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.1MB/s \n",
            "\u001b[?25hCollecting tabledata<2,>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/df/b2/264d9707502f0259a3eb82ec48064df98b1735d5a5f315b6a1d7105263f4/tabledata-1.1.3-py3-none-any.whl\n",
            "Collecting tcolorpy<1,>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/02/51/bbb0cc7f30771c285c354634bf83653a2871d58c6923bd29bfddeb9c9cb1/tcolorpy-0.0.8-py3-none-any.whl\n",
            "Collecting msgfy<1,>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.6/dist-packages (from pytablewriter) (50.3.2)\n",
            "Collecting typepy[datetime]<2,>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/55/a1111b2eb1f4096c28b14645ca62aec560b1768338af21620e470b60872f/typepy-1.1.1-py3-none-any.whl\n",
            "Collecting mbstrdecoder<2,>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/57/3bb55beafe0a5e9883621f01a560d16bcef6d4f844dc2dd40caa0a8d9182/mbstrdecoder-1.0.0-py3-none-any.whl\n",
            "Collecting pathvalidate<3,>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/fa/1a951084aa93940399800e37ed6f096ad5c0de3c26604be62f9464a39fc1/pathvalidate-2.3.0-py3-none-any.whl\n",
            "Collecting DataProperty<2,>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/2d/e5413965af992f4e489b6f5eebf52db9c17953c772962d1223d434b05cef/DataProperty-0.50.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.4)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.20)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/06/03b1f92d46546a18eabf33ff7f37ef422c18c93d5a926bf590fee32ebe75/cliff-3.4.0-py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c8/c16d30bbed11a1722060014c246d124582d1f781b26f5859d8dacc3e08e1/colorlog-4.6.2-py2.py3-none-any.whl\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 56.6MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/3c/06c76ec8b54b9b1fad7f35e903fd25010fe3e0d41bd94cea5e6f12e0d651/cmaes-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2018.9; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.4 in /usr/local/lib/python3.6/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/a1/004f04ba411a8002b02aadb089fd6868116c12ddc9f6d576175e89d07587/stevedore-3.2.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.6MB/s \n",
            "\u001b[?25hCollecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/94/0f4f16cff4977188d715a95ea3f90f054b7eb73b05afaf51431a3d77b992/cmd2-1.3.11-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 56.9MB/s \n",
            "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 56.6MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.9MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from stevedore>=2.0.1->cliff->optuna) (2.0.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=2.0.1->cliff->optuna) (3.4.0)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp36-none-any.whl size=359761 sha256=9fdb3d4aa8bc05c88abb0a1fd645045ffc1939a1b2f7de04973997537c11b618\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: box2d-kengz, PrettyTable, pyperclip\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp36-cp36m-linux_x86_64.whl size=2035674 sha256=18e2407a3becfcd500e983b21f9ad83fdd5b09481acc3844977f9058cc7f1c47\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-cp36-none-any.whl size=13700 sha256=d60e5e5809751e3f9ef4ba7ecdb7472949dc5acc7712a71ef6eedc7f437cdf64\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11119 sha256=53b34a9bbef16025ec09742251ee7d9543de1ba496a07d6b559ac9c774fbf7af\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built box2d-kengz PrettyTable pyperclip\n",
            "Installing collected packages: box2d, box2d-kengz, mbstrdecoder, typepy, DataProperty, tabledata, tcolorpy, msgfy, pathvalidate, pytablewriter, pbr, stevedore, colorama, pyperclip, cmd2, PrettyTable, cliff, colorlog, Mako, python-editor, alembic, cmaes, optuna, pyaml, scikit-optimize\n",
            "  Found existing installation: prettytable 1.0.1\n",
            "    Uninstalling prettytable-1.0.1:\n",
            "      Successfully uninstalled prettytable-1.0.1\n",
            "Successfully installed DataProperty-0.50.0 Mako-1.1.3 PrettyTable-0.7.2 alembic-1.4.3 box2d-2.3.10 box2d-kengz-2.3.3 cliff-3.4.0 cmaes-0.7.0 cmd2-1.3.11 colorama-0.4.4 colorlog-4.6.2 mbstrdecoder-1.0.0 msgfy-0.1.0 optuna-2.3.0 pathvalidate-2.3.0 pbr-5.5.1 pyaml-20.4.0 pyperclip-1.8.1 pytablewriter-0.58.0 python-editor-1.0.4 scikit-optimize-0.8.1 stevedore-3.2.2 tabledata-1.1.3 tcolorpy-0.0.8 typepy-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjF3qRg7oGH"
      },
      "source": [
        "## Clone RL Baselines Zoo Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjGikdT1DFy",
        "outputId": "70ff979a-a6d6-4615-a475-5a4e3aaf011e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines-zoo'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1829 (delta 12), reused 18 (delta 8), pack-reused 1796\u001b[K\n",
            "Receiving objects: 100% (1829/1829), 375.67 MiB | 40.89 MiB/s, done.\n",
            "Resolving deltas: 100% (1077/1077), done.\n",
            "Checking out files: 100% (333/333), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REMQlh-ezyVt",
        "outputId": "1d04f5a8-3836-4403-d260-b67bc63c2911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd rl-baselines-zoo/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rl-baselines-zoo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ-pAbF7zRZ"
      },
      "source": [
        "## Train an RL Agent\n",
        "\n",
        "\n",
        "The train agent can be found in the `logs/` folder.\n",
        "\n",
        "Here we will train A2C on CartPole-v1 environment for 100 000 steps. \n",
        "\n",
        "\n",
        "To train it on Pong (Atari), you just have to pass `--env PongNoFrameskip-v4`\n",
        "\n",
        "Note: You need to update `hyperparams/algo.yml` to support new environments. You can access it in the side panel of Google Colab. (see https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34lXM8ZfMQfG",
        "outputId": "75df6321-cdb3-41d9-e003-79bacb78fc31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gym-minigrid\n",
        "# go to gym-minigrid in /usr/local/lib/python3.6/dist-packages and replace the modeled crossing env"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-minigrid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/57/15171eff6222dd012cc89c001f5c50ad9e11b8cef385873db3b4c0d89aff/gym_minigrid-1.0.1-py3-none-any.whl (47kB)\n",
            "\r\u001b[K     |███████                         | 10kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 20kB 24.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 30kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 40kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.9.6->gym-minigrid) (0.16.0)\n",
            "Installing collected packages: gym-minigrid\n",
            "Successfully installed gym-minigrid-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bIR_N7R11XI",
        "outputId": "52660456-24d9-44df-d995-a1c9ff806329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --gym-packages gym_minigrid"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "| serial_timesteps   | 338432        |\n",
            "| time_elapsed       | 3.26e+03      |\n",
            "| total_timesteps    | 2707456       |\n",
            "| value_loss         | 0.8165801     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2710000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 19.60 +/- 2.87\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015166904   |\n",
            "| clipfrac           | 0.08266602    |\n",
            "| ep_len_mean        | 35.2          |\n",
            "| ep_reward_mean     | 0.901         |\n",
            "| explained_variance | 0.549         |\n",
            "| fps                | 945           |\n",
            "| n_updates          | 662           |\n",
            "| policy_entropy     | 0.19629632    |\n",
            "| policy_loss        | -0.0011375647 |\n",
            "| serial_timesteps   | 338944        |\n",
            "| time_elapsed       | 3.26e+03      |\n",
            "| total_timesteps    | 2711552       |\n",
            "| value_loss         | 0.76335955    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0133718755 |\n",
            "| clipfrac           | 0.08457031   |\n",
            "| ep_len_mean        | 43.6         |\n",
            "| ep_reward_mean     | 0.877        |\n",
            "| explained_variance | 0.43         |\n",
            "| fps                | 965          |\n",
            "| n_updates          | 663          |\n",
            "| policy_entropy     | 0.20752236   |\n",
            "| policy_loss        | -0.006854235 |\n",
            "| serial_timesteps   | 339456       |\n",
            "| time_elapsed       | 3.27e+03     |\n",
            "| total_timesteps    | 2715648      |\n",
            "| value_loss         | 0.88579845   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02962803   |\n",
            "| clipfrac           | 0.09047852   |\n",
            "| ep_len_mean        | 44.1         |\n",
            "| ep_reward_mean     | 0.875        |\n",
            "| explained_variance | 0.508        |\n",
            "| fps                | 1012         |\n",
            "| n_updates          | 664          |\n",
            "| policy_entropy     | 0.19517565   |\n",
            "| policy_loss        | -0.003267331 |\n",
            "| serial_timesteps   | 339968       |\n",
            "| time_elapsed       | 3.27e+03     |\n",
            "| total_timesteps    | 2719744      |\n",
            "| value_loss         | 0.85950744   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2720000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.20 +/- 122.46\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012936336   |\n",
            "| clipfrac           | 0.06850586    |\n",
            "| ep_len_mean        | 36.9          |\n",
            "| ep_reward_mean     | 0.897         |\n",
            "| explained_variance | 0.422         |\n",
            "| fps                | 848           |\n",
            "| n_updates          | 665           |\n",
            "| policy_entropy     | 0.1839118     |\n",
            "| policy_loss        | -0.0011599435 |\n",
            "| serial_timesteps   | 340480        |\n",
            "| time_elapsed       | 3.27e+03      |\n",
            "| total_timesteps    | 2723840       |\n",
            "| value_loss         | 0.801013      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.023846542   |\n",
            "| clipfrac           | 0.097753905   |\n",
            "| ep_len_mean        | 32.8          |\n",
            "| ep_reward_mean     | 0.908         |\n",
            "| explained_variance | 0.467         |\n",
            "| fps                | 1023          |\n",
            "| n_updates          | 666           |\n",
            "| policy_entropy     | 0.2116886     |\n",
            "| policy_loss        | -0.0059678964 |\n",
            "| serial_timesteps   | 340992        |\n",
            "| time_elapsed       | 3.28e+03      |\n",
            "| total_timesteps    | 2727936       |\n",
            "| value_loss         | 0.7986444     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2730000, episode_reward=0.58 +/- 0.47\n",
            "Episode length: 138.40 +/- 151.54\n",
            "-------------------------------------\n",
            "| approxkl           | 0.042537875  |\n",
            "| clipfrac           | 0.06113281   |\n",
            "| ep_len_mean        | 38.5         |\n",
            "| ep_reward_mean     | 0.893        |\n",
            "| explained_variance | 0.498        |\n",
            "| fps                | 734          |\n",
            "| n_updates          | 667          |\n",
            "| policy_entropy     | 0.16030186   |\n",
            "| policy_loss        | -0.002596819 |\n",
            "| serial_timesteps   | 341504       |\n",
            "| time_elapsed       | 3.28e+03     |\n",
            "| total_timesteps    | 2732032      |\n",
            "| value_loss         | 0.7563403    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010069765   |\n",
            "| clipfrac           | 0.049975585   |\n",
            "| ep_len_mean        | 37            |\n",
            "| ep_reward_mean     | 0.896         |\n",
            "| explained_variance | 0.521         |\n",
            "| fps                | 1018          |\n",
            "| n_updates          | 668           |\n",
            "| policy_entropy     | 0.16029307    |\n",
            "| policy_loss        | -0.0025467023 |\n",
            "| serial_timesteps   | 342016        |\n",
            "| time_elapsed       | 3.29e+03      |\n",
            "| total_timesteps    | 2736128       |\n",
            "| value_loss         | 0.88016427    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2740000, episode_reward=0.58 +/- 0.47\n",
            "Episode length: 137.80 +/- 152.03\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014527257   |\n",
            "| clipfrac           | 0.05407715    |\n",
            "| ep_len_mean        | 43.6          |\n",
            "| ep_reward_mean     | 0.878         |\n",
            "| explained_variance | 0.541         |\n",
            "| fps                | 732           |\n",
            "| n_updates          | 669           |\n",
            "| policy_entropy     | 0.1705962     |\n",
            "| policy_loss        | -0.0014597254 |\n",
            "| serial_timesteps   | 342528        |\n",
            "| time_elapsed       | 3.29e+03      |\n",
            "| total_timesteps    | 2740224       |\n",
            "| value_loss         | 0.8057639     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.1771774    |\n",
            "| clipfrac           | 0.12158203   |\n",
            "| ep_len_mean        | 34.2         |\n",
            "| ep_reward_mean     | 0.904        |\n",
            "| explained_variance | 0.556        |\n",
            "| fps                | 1003         |\n",
            "| n_updates          | 670          |\n",
            "| policy_entropy     | 0.199882     |\n",
            "| policy_loss        | -0.014893742 |\n",
            "| serial_timesteps   | 343040       |\n",
            "| time_elapsed       | 3.3e+03      |\n",
            "| total_timesteps    | 2744320      |\n",
            "| value_loss         | 0.726235     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0078037595   |\n",
            "| clipfrac           | 0.047314454    |\n",
            "| ep_len_mean        | 32.8           |\n",
            "| ep_reward_mean     | 0.909          |\n",
            "| explained_variance | 0.485          |\n",
            "| fps                | 984            |\n",
            "| n_updates          | 671            |\n",
            "| policy_entropy     | 0.1508735      |\n",
            "| policy_loss        | -0.00041887676 |\n",
            "| serial_timesteps   | 343552         |\n",
            "| time_elapsed       | 3.3e+03        |\n",
            "| total_timesteps    | 2748416        |\n",
            "| value_loss         | 0.6976508      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=2750000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 1.50\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013758306  |\n",
            "| clipfrac           | 0.058764648  |\n",
            "| ep_len_mean        | 29.8         |\n",
            "| ep_reward_mean     | 0.917        |\n",
            "| explained_variance | 0.569        |\n",
            "| fps                | 963          |\n",
            "| n_updates          | 672          |\n",
            "| policy_entropy     | 0.1459791    |\n",
            "| policy_loss        | -0.000506134 |\n",
            "| serial_timesteps   | 344064       |\n",
            "| time_elapsed       | 3.31e+03     |\n",
            "| total_timesteps    | 2752512      |\n",
            "| value_loss         | 0.71862996   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014488608   |\n",
            "| clipfrac           | 0.07570801    |\n",
            "| ep_len_mean        | 31.5          |\n",
            "| ep_reward_mean     | 0.912         |\n",
            "| explained_variance | 0.497         |\n",
            "| fps                | 1005          |\n",
            "| n_updates          | 673           |\n",
            "| policy_entropy     | 0.19782223    |\n",
            "| policy_loss        | -0.0012111235 |\n",
            "| serial_timesteps   | 344576        |\n",
            "| time_elapsed       | 3.31e+03      |\n",
            "| total_timesteps    | 2756608       |\n",
            "| value_loss         | 0.75093126    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2760000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.40 +/- 3.83\n",
            "-------------------------------------\n",
            "| approxkl           | 0.039539993  |\n",
            "| clipfrac           | 0.13239746   |\n",
            "| ep_len_mean        | 35.3         |\n",
            "| ep_reward_mean     | 0.901        |\n",
            "| explained_variance | 0.558        |\n",
            "| fps                | 974          |\n",
            "| n_updates          | 674          |\n",
            "| policy_entropy     | 0.25508386   |\n",
            "| policy_loss        | -0.011522543 |\n",
            "| serial_timesteps   | 345088       |\n",
            "| time_elapsed       | 3.31e+03     |\n",
            "| total_timesteps    | 2760704      |\n",
            "| value_loss         | 0.64866614   |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| approxkl           | 0.02248067      |\n",
            "| clipfrac           | 0.054833986     |\n",
            "| ep_len_mean        | 33              |\n",
            "| ep_reward_mean     | 0.907           |\n",
            "| explained_variance | 0.506           |\n",
            "| fps                | 1015            |\n",
            "| n_updates          | 675             |\n",
            "| policy_entropy     | 0.13838615      |\n",
            "| policy_loss        | -0.000103696715 |\n",
            "| serial_timesteps   | 345600          |\n",
            "| time_elapsed       | 3.32e+03        |\n",
            "| total_timesteps    | 2764800         |\n",
            "| value_loss         | 0.68935066      |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010521372   |\n",
            "| clipfrac           | 0.044433594   |\n",
            "| ep_len_mean        | 34.6          |\n",
            "| ep_reward_mean     | 0.901         |\n",
            "| explained_variance | 0.532         |\n",
            "| fps                | 977           |\n",
            "| n_updates          | 676           |\n",
            "| policy_entropy     | 0.14020112    |\n",
            "| policy_loss        | -0.0014179524 |\n",
            "| serial_timesteps   | 346112        |\n",
            "| time_elapsed       | 3.32e+03      |\n",
            "| total_timesteps    | 2768896       |\n",
            "| value_loss         | 0.7527197     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2770000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.80 +/- 122.64\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014375888   |\n",
            "| clipfrac           | 0.076660156   |\n",
            "| ep_len_mean        | 36.1          |\n",
            "| ep_reward_mean     | 0.897         |\n",
            "| explained_variance | 0.656         |\n",
            "| fps                | 843           |\n",
            "| n_updates          | 677           |\n",
            "| policy_entropy     | 0.20801505    |\n",
            "| policy_loss        | -0.0023478102 |\n",
            "| serial_timesteps   | 346624        |\n",
            "| time_elapsed       | 3.33e+03      |\n",
            "| total_timesteps    | 2772992       |\n",
            "| value_loss         | 0.6590135     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.021729115   |\n",
            "| clipfrac           | 0.058251955   |\n",
            "| ep_len_mean        | 27.4          |\n",
            "| ep_reward_mean     | 0.924         |\n",
            "| explained_variance | 0.484         |\n",
            "| fps                | 1002          |\n",
            "| n_updates          | 678           |\n",
            "| policy_entropy     | 0.14312822    |\n",
            "| policy_loss        | -0.0009732413 |\n",
            "| serial_timesteps   | 347136        |\n",
            "| time_elapsed       | 3.33e+03      |\n",
            "| total_timesteps    | 2777088       |\n",
            "| value_loss         | 0.6582893     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2780000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.00 +/- 122.53\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018137705  |\n",
            "| clipfrac           | 0.06774902   |\n",
            "| ep_len_mean        | 33.2         |\n",
            "| ep_reward_mean     | 0.907        |\n",
            "| explained_variance | 0.521        |\n",
            "| fps                | 824          |\n",
            "| n_updates          | 679          |\n",
            "| policy_entropy     | 0.15815496   |\n",
            "| policy_loss        | -0.003941639 |\n",
            "| serial_timesteps   | 347648       |\n",
            "| time_elapsed       | 3.34e+03     |\n",
            "| total_timesteps    | 2781184      |\n",
            "| value_loss         | 0.7121554    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01802751    |\n",
            "| clipfrac           | 0.07453613    |\n",
            "| ep_len_mean        | 33            |\n",
            "| ep_reward_mean     | 0.908         |\n",
            "| explained_variance | 0.469         |\n",
            "| fps                | 995           |\n",
            "| n_updates          | 680           |\n",
            "| policy_entropy     | 0.1673067     |\n",
            "| policy_loss        | -0.0001969554 |\n",
            "| serial_timesteps   | 348160        |\n",
            "| time_elapsed       | 3.34e+03      |\n",
            "| total_timesteps    | 2785280       |\n",
            "| value_loss         | 0.68137366    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.017461222   |\n",
            "| clipfrac           | 0.08134766    |\n",
            "| ep_len_mean        | 32.2          |\n",
            "| ep_reward_mean     | 0.91          |\n",
            "| explained_variance | 0.495         |\n",
            "| fps                | 985           |\n",
            "| n_updates          | 681           |\n",
            "| policy_entropy     | 0.15482561    |\n",
            "| policy_loss        | -0.0040697823 |\n",
            "| serial_timesteps   | 348672        |\n",
            "| time_elapsed       | 3.34e+03      |\n",
            "| total_timesteps    | 2789376       |\n",
            "| value_loss         | 0.6063061     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2790000, episode_reward=0.57 +/- 0.46\n",
            "Episode length: 140.80 +/- 149.60\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012690121   |\n",
            "| clipfrac           | 0.047680665   |\n",
            "| ep_len_mean        | 43.6          |\n",
            "| ep_reward_mean     | 0.875         |\n",
            "| explained_variance | 0.532         |\n",
            "| fps                | 748           |\n",
            "| n_updates          | 682           |\n",
            "| policy_entropy     | 0.16319652    |\n",
            "| policy_loss        | -0.0010811263 |\n",
            "| serial_timesteps   | 349184        |\n",
            "| time_elapsed       | 3.35e+03      |\n",
            "| total_timesteps    | 2793472       |\n",
            "| value_loss         | 0.8123701     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01255828   |\n",
            "| clipfrac           | 0.059326172  |\n",
            "| ep_len_mean        | 41.9         |\n",
            "| ep_reward_mean     | 0.882        |\n",
            "| explained_variance | 0.482        |\n",
            "| fps                | 997          |\n",
            "| n_updates          | 683          |\n",
            "| policy_entropy     | 0.15276115   |\n",
            "| policy_loss        | 0.0022262544 |\n",
            "| serial_timesteps   | 349696       |\n",
            "| time_elapsed       | 3.35e+03     |\n",
            "| total_timesteps    | 2797568      |\n",
            "| value_loss         | 0.75578225   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2800000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.80 +/- 122.62\n",
            "--------------------------------------\n",
            "| approxkl           | 0.020952987   |\n",
            "| clipfrac           | 0.07019043    |\n",
            "| ep_len_mean        | 31.6          |\n",
            "| ep_reward_mean     | 0.91          |\n",
            "| explained_variance | 0.514         |\n",
            "| fps                | 839           |\n",
            "| n_updates          | 684           |\n",
            "| policy_entropy     | 0.14568302    |\n",
            "| policy_loss        | -0.0051041865 |\n",
            "| serial_timesteps   | 350208        |\n",
            "| time_elapsed       | 3.36e+03      |\n",
            "| total_timesteps    | 2801664       |\n",
            "| value_loss         | 0.7490458     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0074245753  |\n",
            "| clipfrac           | 0.045092773   |\n",
            "| ep_len_mean        | 43.8          |\n",
            "| ep_reward_mean     | 0.873         |\n",
            "| explained_variance | 0.558         |\n",
            "| fps                | 999           |\n",
            "| n_updates          | 685           |\n",
            "| policy_entropy     | 0.1273805     |\n",
            "| policy_loss        | -0.0019918878 |\n",
            "| serial_timesteps   | 350720        |\n",
            "| time_elapsed       | 3.36e+03      |\n",
            "| total_timesteps    | 2805760       |\n",
            "| value_loss         | 0.8643974     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011865836  |\n",
            "| clipfrac           | 0.05620117   |\n",
            "| ep_len_mean        | 30           |\n",
            "| ep_reward_mean     | 0.917        |\n",
            "| explained_variance | 0.526        |\n",
            "| fps                | 991          |\n",
            "| n_updates          | 686          |\n",
            "| policy_entropy     | 0.1536003    |\n",
            "| policy_loss        | 0.0012423774 |\n",
            "| serial_timesteps   | 351232       |\n",
            "| time_elapsed       | 3.37e+03     |\n",
            "| total_timesteps    | 2809856      |\n",
            "| value_loss         | 0.78127116   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2810000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.80 +/- 150.41\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009580096  |\n",
            "| clipfrac           | 0.05305176   |\n",
            "| ep_len_mean        | 31.6         |\n",
            "| ep_reward_mean     | 0.911        |\n",
            "| explained_variance | 0.526        |\n",
            "| fps                | 751          |\n",
            "| n_updates          | 687          |\n",
            "| policy_entropy     | 0.15432104   |\n",
            "| policy_loss        | 0.0014512091 |\n",
            "| serial_timesteps   | 351744       |\n",
            "| time_elapsed       | 3.37e+03     |\n",
            "| total_timesteps    | 2813952      |\n",
            "| value_loss         | 0.77443886   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.025341552  |\n",
            "| clipfrac           | 0.079638675  |\n",
            "| ep_len_mean        | 29           |\n",
            "| ep_reward_mean     | 0.917        |\n",
            "| explained_variance | 0.561        |\n",
            "| fps                | 983          |\n",
            "| n_updates          | 688          |\n",
            "| policy_entropy     | 0.17585911   |\n",
            "| policy_loss        | -0.001281007 |\n",
            "| serial_timesteps   | 352256       |\n",
            "| time_elapsed       | 3.38e+03     |\n",
            "| total_timesteps    | 2818048      |\n",
            "| value_loss         | 0.78884584   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2820000, episode_reward=0.38 +/- 0.46\n",
            "Episode length: 202.00 +/- 149.42\n",
            "--------------------------------------\n",
            "| approxkl           | 0.016259842   |\n",
            "| clipfrac           | 0.08139648    |\n",
            "| ep_len_mean        | 32.2          |\n",
            "| ep_reward_mean     | 0.91          |\n",
            "| explained_variance | 0.538         |\n",
            "| fps                | 672           |\n",
            "| n_updates          | 689           |\n",
            "| policy_entropy     | 0.17875634    |\n",
            "| policy_loss        | -0.0035422177 |\n",
            "| serial_timesteps   | 352768        |\n",
            "| time_elapsed       | 3.38e+03      |\n",
            "| total_timesteps    | 2822144       |\n",
            "| value_loss         | 0.749264      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.021237193   |\n",
            "| clipfrac           | 0.07687988    |\n",
            "| ep_len_mean        | 49.7          |\n",
            "| ep_reward_mean     | 0.86          |\n",
            "| explained_variance | 0.48          |\n",
            "| fps                | 989           |\n",
            "| n_updates          | 690           |\n",
            "| policy_entropy     | 0.21671624    |\n",
            "| policy_loss        | 2.2103679e-05 |\n",
            "| serial_timesteps   | 353280        |\n",
            "| time_elapsed       | 3.39e+03      |\n",
            "| total_timesteps    | 2826240       |\n",
            "| value_loss         | 0.8080429     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2830000, episode_reward=0.75 +/- 0.38\n",
            "Episode length: 82.00 +/- 121.02\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0129306335 |\n",
            "| clipfrac           | 0.07714844   |\n",
            "| ep_len_mean        | 39.4         |\n",
            "| ep_reward_mean     | 0.889        |\n",
            "| explained_variance | 0.538        |\n",
            "| fps                | 825          |\n",
            "| n_updates          | 691          |\n",
            "| policy_entropy     | 0.19643651   |\n",
            "| policy_loss        | 0.0025684494 |\n",
            "| serial_timesteps   | 353792       |\n",
            "| time_elapsed       | 3.39e+03     |\n",
            "| total_timesteps    | 2830336      |\n",
            "| value_loss         | 0.83070934   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.028254494 |\n",
            "| clipfrac           | 0.10119629  |\n",
            "| ep_len_mean        | 42.2        |\n",
            "| ep_reward_mean     | 0.882       |\n",
            "| explained_variance | 0.454       |\n",
            "| fps                | 1014        |\n",
            "| n_updates          | 692         |\n",
            "| policy_entropy     | 0.23075351  |\n",
            "| policy_loss        | 0.005252863 |\n",
            "| serial_timesteps   | 354304      |\n",
            "| time_elapsed       | 3.4e+03     |\n",
            "| total_timesteps    | 2834432     |\n",
            "| value_loss         | 0.76177204  |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.02306446    |\n",
            "| clipfrac           | 0.0770752     |\n",
            "| ep_len_mean        | 37.8          |\n",
            "| ep_reward_mean     | 0.895         |\n",
            "| explained_variance | 0.383         |\n",
            "| fps                | 1011          |\n",
            "| n_updates          | 693           |\n",
            "| policy_entropy     | 0.23865716    |\n",
            "| policy_loss        | -0.0013569626 |\n",
            "| serial_timesteps   | 354816        |\n",
            "| time_elapsed       | 3.4e+03       |\n",
            "| total_timesteps    | 2838528       |\n",
            "| value_loss         | 0.73698884    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2840000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014279133   |\n",
            "| clipfrac           | 0.087573245   |\n",
            "| ep_len_mean        | 50.3          |\n",
            "| ep_reward_mean     | 0.856         |\n",
            "| explained_variance | 0.443         |\n",
            "| fps                | 550           |\n",
            "| n_updates          | 694           |\n",
            "| policy_entropy     | 0.24984331    |\n",
            "| policy_loss        | 0.00049698574 |\n",
            "| serial_timesteps   | 355328        |\n",
            "| time_elapsed       | 3.4e+03       |\n",
            "| total_timesteps    | 2842624       |\n",
            "| value_loss         | 0.8190274     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.025865655 |\n",
            "| clipfrac           | 0.0953125   |\n",
            "| ep_len_mean        | 53.5        |\n",
            "| ep_reward_mean     | 0.846       |\n",
            "| explained_variance | 0.269       |\n",
            "| fps                | 984         |\n",
            "| n_updates          | 695         |\n",
            "| policy_entropy     | 0.2775785   |\n",
            "| policy_loss        | 0.001236021 |\n",
            "| serial_timesteps   | 355840      |\n",
            "| time_elapsed       | 3.41e+03    |\n",
            "| total_timesteps    | 2846720     |\n",
            "| value_loss         | 0.8797677   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=2850000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.60 +/- 122.23\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014193165  |\n",
            "| clipfrac           | 0.09863281   |\n",
            "| ep_len_mean        | 58.9         |\n",
            "| ep_reward_mean     | 0.829        |\n",
            "| explained_variance | 0.463        |\n",
            "| fps                | 808          |\n",
            "| n_updates          | 696          |\n",
            "| policy_entropy     | 0.28135073   |\n",
            "| policy_loss        | 0.0010292106 |\n",
            "| serial_timesteps   | 356352       |\n",
            "| time_elapsed       | 3.42e+03     |\n",
            "| total_timesteps    | 2850816      |\n",
            "| value_loss         | 0.88289225   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011849788   |\n",
            "| clipfrac           | 0.07644043    |\n",
            "| ep_len_mean        | 47.5          |\n",
            "| ep_reward_mean     | 0.865         |\n",
            "| explained_variance | 0.483         |\n",
            "| fps                | 992           |\n",
            "| n_updates          | 697           |\n",
            "| policy_entropy     | 0.2804583     |\n",
            "| policy_loss        | -0.0034569309 |\n",
            "| serial_timesteps   | 356864        |\n",
            "| time_elapsed       | 3.42e+03      |\n",
            "| total_timesteps    | 2854912       |\n",
            "| value_loss         | 0.77071273    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011412257   |\n",
            "| clipfrac           | 0.072387695   |\n",
            "| ep_len_mean        | 42            |\n",
            "| ep_reward_mean     | 0.882         |\n",
            "| explained_variance | 0.413         |\n",
            "| fps                | 1012          |\n",
            "| n_updates          | 698           |\n",
            "| policy_entropy     | 0.25297445    |\n",
            "| policy_loss        | -0.0013877985 |\n",
            "| serial_timesteps   | 357376        |\n",
            "| time_elapsed       | 3.43e+03      |\n",
            "| total_timesteps    | 2859008       |\n",
            "| value_loss         | 0.8380804     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2860000, episode_reward=0.38 +/- 0.47\n",
            "Episode length: 200.40 +/- 151.38\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011569908   |\n",
            "| clipfrac           | 0.0798584     |\n",
            "| ep_len_mean        | 47.4          |\n",
            "| ep_reward_mean     | 0.867         |\n",
            "| explained_variance | 0.436         |\n",
            "| fps                | 669           |\n",
            "| n_updates          | 699           |\n",
            "| policy_entropy     | 0.2751746     |\n",
            "| policy_loss        | -0.0008931753 |\n",
            "| serial_timesteps   | 357888        |\n",
            "| time_elapsed       | 3.43e+03      |\n",
            "| total_timesteps    | 2863104       |\n",
            "| value_loss         | 0.79790676    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015303122   |\n",
            "| clipfrac           | 0.058154296   |\n",
            "| ep_len_mean        | 39.3          |\n",
            "| ep_reward_mean     | 0.888         |\n",
            "| explained_variance | 0.437         |\n",
            "| fps                | 1002          |\n",
            "| n_updates          | 700           |\n",
            "| policy_entropy     | 0.20868587    |\n",
            "| policy_loss        | 0.00021673483 |\n",
            "| serial_timesteps   | 358400        |\n",
            "| time_elapsed       | 3.44e+03      |\n",
            "| total_timesteps    | 2867200       |\n",
            "| value_loss         | 0.8612363     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2870000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.60 +/- 123.71\n",
            "--------------------------------------\n",
            "| approxkl           | 0.007714183   |\n",
            "| clipfrac           | 0.05493164    |\n",
            "| ep_len_mean        | 38.8          |\n",
            "| ep_reward_mean     | 0.891         |\n",
            "| explained_variance | 0.481         |\n",
            "| fps                | 860           |\n",
            "| n_updates          | 701           |\n",
            "| policy_entropy     | 0.19108337    |\n",
            "| policy_loss        | 0.00030561746 |\n",
            "| serial_timesteps   | 358912        |\n",
            "| time_elapsed       | 3.44e+03      |\n",
            "| total_timesteps    | 2871296       |\n",
            "| value_loss         | 0.8643392     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014425894   |\n",
            "| clipfrac           | 0.109521486   |\n",
            "| ep_len_mean        | 33.6          |\n",
            "| ep_reward_mean     | 0.905         |\n",
            "| explained_variance | 0.46          |\n",
            "| fps                | 1016          |\n",
            "| n_updates          | 702           |\n",
            "| policy_entropy     | 0.23803493    |\n",
            "| policy_loss        | -0.0074635195 |\n",
            "| serial_timesteps   | 359424        |\n",
            "| time_elapsed       | 3.44e+03      |\n",
            "| total_timesteps    | 2875392       |\n",
            "| value_loss         | 0.90051997    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.035150893   |\n",
            "| clipfrac           | 0.11218262    |\n",
            "| ep_len_mean        | 39            |\n",
            "| ep_reward_mean     | 0.89          |\n",
            "| explained_variance | 0.433         |\n",
            "| fps                | 997           |\n",
            "| n_updates          | 703           |\n",
            "| policy_entropy     | 0.2396408     |\n",
            "| policy_loss        | -0.0018017457 |\n",
            "| serial_timesteps   | 359936        |\n",
            "| time_elapsed       | 3.45e+03      |\n",
            "| total_timesteps    | 2879488       |\n",
            "| value_loss         | 0.78507686    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2880000, episode_reward=0.38 +/- 0.46\n",
            "Episode length: 203.00 +/- 148.21\n",
            "--------------------------------------\n",
            "| approxkl           | 0.024679467   |\n",
            "| clipfrac           | 0.1159668     |\n",
            "| ep_len_mean        | 45.5          |\n",
            "| ep_reward_mean     | 0.871         |\n",
            "| explained_variance | 0.465         |\n",
            "| fps                | 669           |\n",
            "| n_updates          | 704           |\n",
            "| policy_entropy     | 0.26190493    |\n",
            "| policy_loss        | -0.0054124165 |\n",
            "| serial_timesteps   | 360448        |\n",
            "| time_elapsed       | 3.45e+03      |\n",
            "| total_timesteps    | 2883584       |\n",
            "| value_loss         | 0.8911729     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.034179997   |\n",
            "| clipfrac           | 0.11340332    |\n",
            "| ep_len_mean        | 32.8          |\n",
            "| ep_reward_mean     | 0.909         |\n",
            "| explained_variance | 0.389         |\n",
            "| fps                | 1006          |\n",
            "| n_updates          | 705           |\n",
            "| policy_entropy     | 0.24994023    |\n",
            "| policy_loss        | -0.0055885366 |\n",
            "| serial_timesteps   | 360960        |\n",
            "| time_elapsed       | 3.46e+03      |\n",
            "| total_timesteps    | 2887680       |\n",
            "| value_loss         | 0.6642363     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2890000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.40 +/- 123.33\n",
            "--------------------------------------\n",
            "| approxkl           | 0.023317799   |\n",
            "| clipfrac           | 0.08898926    |\n",
            "| ep_len_mean        | 34.1          |\n",
            "| ep_reward_mean     | 0.905         |\n",
            "| explained_variance | 0.33          |\n",
            "| fps                | 846           |\n",
            "| n_updates          | 706           |\n",
            "| policy_entropy     | 0.21450333    |\n",
            "| policy_loss        | -0.0045108693 |\n",
            "| serial_timesteps   | 361472        |\n",
            "| time_elapsed       | 3.46e+03      |\n",
            "| total_timesteps    | 2891776       |\n",
            "| value_loss         | 0.6866919     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02026099   |\n",
            "| clipfrac           | 0.074682616  |\n",
            "| ep_len_mean        | 31.2         |\n",
            "| ep_reward_mean     | 0.913        |\n",
            "| explained_variance | 0.423        |\n",
            "| fps                | 1003         |\n",
            "| n_updates          | 707          |\n",
            "| policy_entropy     | 0.19656013   |\n",
            "| policy_loss        | 0.0016120527 |\n",
            "| serial_timesteps   | 361984       |\n",
            "| time_elapsed       | 3.47e+03     |\n",
            "| total_timesteps    | 2895872      |\n",
            "| value_loss         | 0.6661669    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.016490076   |\n",
            "| clipfrac           | 0.06081543    |\n",
            "| ep_len_mean        | 41.2          |\n",
            "| ep_reward_mean     | 0.882         |\n",
            "| explained_variance | 0.437         |\n",
            "| fps                | 983           |\n",
            "| n_updates          | 708           |\n",
            "| policy_entropy     | 0.19205457    |\n",
            "| policy_loss        | -0.0019194735 |\n",
            "| serial_timesteps   | 362496        |\n",
            "| time_elapsed       | 3.47e+03      |\n",
            "| total_timesteps    | 2899968       |\n",
            "| value_loss         | 0.84869194    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2900000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.20 +/- 150.89\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01704931    |\n",
            "| clipfrac           | 0.08527832    |\n",
            "| ep_len_mean        | 30.2          |\n",
            "| ep_reward_mean     | 0.916         |\n",
            "| explained_variance | 0.487         |\n",
            "| fps                | 739           |\n",
            "| n_updates          | 709           |\n",
            "| policy_entropy     | 0.20223987    |\n",
            "| policy_loss        | -0.0036889433 |\n",
            "| serial_timesteps   | 363008        |\n",
            "| time_elapsed       | 3.48e+03      |\n",
            "| total_timesteps    | 2904064       |\n",
            "| value_loss         | 0.5918093     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010837188   |\n",
            "| clipfrac           | 0.05654297    |\n",
            "| ep_len_mean        | 33.9          |\n",
            "| ep_reward_mean     | 0.906         |\n",
            "| explained_variance | 0.461         |\n",
            "| fps                | 987           |\n",
            "| n_updates          | 710           |\n",
            "| policy_entropy     | 0.15240717    |\n",
            "| policy_loss        | -0.0032469842 |\n",
            "| serial_timesteps   | 363520        |\n",
            "| time_elapsed       | 3.48e+03      |\n",
            "| total_timesteps    | 2908160       |\n",
            "| value_loss         | 0.62377447    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2910000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.00 +/- 122.53\n",
            "--------------------------------------\n",
            "| approxkl           | 0.006568294   |\n",
            "| clipfrac           | 0.043286134   |\n",
            "| ep_len_mean        | 32.8          |\n",
            "| ep_reward_mean     | 0.908         |\n",
            "| explained_variance | 0.52          |\n",
            "| fps                | 855           |\n",
            "| n_updates          | 711           |\n",
            "| policy_entropy     | 0.14786658    |\n",
            "| policy_loss        | -0.0012396192 |\n",
            "| serial_timesteps   | 364032        |\n",
            "| time_elapsed       | 3.49e+03      |\n",
            "| total_timesteps    | 2912256       |\n",
            "| value_loss         | 0.65657884    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013654475  |\n",
            "| clipfrac           | 0.043481447  |\n",
            "| ep_len_mean        | 33.2         |\n",
            "| ep_reward_mean     | 0.906        |\n",
            "| explained_variance | 0.496        |\n",
            "| fps                | 1009         |\n",
            "| n_updates          | 712          |\n",
            "| policy_entropy     | 0.14969255   |\n",
            "| policy_loss        | -9.64745e-05 |\n",
            "| serial_timesteps   | 364544       |\n",
            "| time_elapsed       | 3.49e+03     |\n",
            "| total_timesteps    | 2916352      |\n",
            "| value_loss         | 0.76787156   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2920000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.40 +/- 3.01\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0127998935  |\n",
            "| clipfrac           | 0.035546876   |\n",
            "| ep_len_mean        | 41.7          |\n",
            "| ep_reward_mean     | 0.882         |\n",
            "| explained_variance | 0.537         |\n",
            "| fps                | 955           |\n",
            "| n_updates          | 713           |\n",
            "| policy_entropy     | 0.09902606    |\n",
            "| policy_loss        | -0.0011733072 |\n",
            "| serial_timesteps   | 365056        |\n",
            "| time_elapsed       | 3.49e+03      |\n",
            "| total_timesteps    | 2920448       |\n",
            "| value_loss         | 0.75434077    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00775235    |\n",
            "| clipfrac           | 0.034716796   |\n",
            "| ep_len_mean        | 37.5          |\n",
            "| ep_reward_mean     | 0.892         |\n",
            "| explained_variance | 0.588         |\n",
            "| fps                | 989           |\n",
            "| n_updates          | 714           |\n",
            "| policy_entropy     | 0.12279141    |\n",
            "| policy_loss        | -0.0019486777 |\n",
            "| serial_timesteps   | 365568        |\n",
            "| time_elapsed       | 3.5e+03       |\n",
            "| total_timesteps    | 2924544       |\n",
            "| value_loss         | 0.7720003     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.008471789    |\n",
            "| clipfrac           | 0.03930664     |\n",
            "| ep_len_mean        | 34             |\n",
            "| ep_reward_mean     | 0.906          |\n",
            "| explained_variance | 0.504          |\n",
            "| fps                | 988            |\n",
            "| n_updates          | 715            |\n",
            "| policy_entropy     | 0.124055445    |\n",
            "| policy_loss        | -0.00055817765 |\n",
            "| serial_timesteps   | 366080         |\n",
            "| time_elapsed       | 3.5e+03        |\n",
            "| total_timesteps    | 2928640        |\n",
            "| value_loss         | 0.72794944     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=2930000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.60 +/- 4.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010429689   |\n",
            "| clipfrac           | 0.055004884   |\n",
            "| ep_len_mean        | 33.1          |\n",
            "| ep_reward_mean     | 0.907         |\n",
            "| explained_variance | 0.538         |\n",
            "| fps                | 961           |\n",
            "| n_updates          | 716           |\n",
            "| policy_entropy     | 0.14564814    |\n",
            "| policy_loss        | -0.0027777678 |\n",
            "| serial_timesteps   | 366592        |\n",
            "| time_elapsed       | 3.51e+03      |\n",
            "| total_timesteps    | 2932736       |\n",
            "| value_loss         | 0.739041      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014834322  |\n",
            "| clipfrac           | 0.033984374  |\n",
            "| ep_len_mean        | 36.5         |\n",
            "| ep_reward_mean     | 0.896        |\n",
            "| explained_variance | 0.581        |\n",
            "| fps                | 1003         |\n",
            "| n_updates          | 717          |\n",
            "| policy_entropy     | 0.11165537   |\n",
            "| policy_loss        | 0.0013253755 |\n",
            "| serial_timesteps   | 367104       |\n",
            "| time_elapsed       | 3.51e+03     |\n",
            "| total_timesteps    | 2936832      |\n",
            "| value_loss         | 0.7492045    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2940000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 19.20 +/- 2.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.007110792   |\n",
            "| clipfrac           | 0.049731445   |\n",
            "| ep_len_mean        | 44.9          |\n",
            "| ep_reward_mean     | 0.869         |\n",
            "| explained_variance | 0.609         |\n",
            "| fps                | 958           |\n",
            "| n_updates          | 718           |\n",
            "| policy_entropy     | 0.14224872    |\n",
            "| policy_loss        | -0.0030130907 |\n",
            "| serial_timesteps   | 367616        |\n",
            "| time_elapsed       | 3.52e+03      |\n",
            "| total_timesteps    | 2940928       |\n",
            "| value_loss         | 0.82341707    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015173445   |\n",
            "| clipfrac           | 0.060302734   |\n",
            "| ep_len_mean        | 52.2          |\n",
            "| ep_reward_mean     | 0.849         |\n",
            "| explained_variance | 0.583         |\n",
            "| fps                | 1008          |\n",
            "| n_updates          | 719           |\n",
            "| policy_entropy     | 0.17018914    |\n",
            "| policy_loss        | -0.0016655403 |\n",
            "| serial_timesteps   | 368128        |\n",
            "| time_elapsed       | 3.52e+03      |\n",
            "| total_timesteps    | 2945024       |\n",
            "| value_loss         | 0.87015134    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005710884   |\n",
            "| clipfrac           | 0.036694337   |\n",
            "| ep_len_mean        | 42.4          |\n",
            "| ep_reward_mean     | 0.876         |\n",
            "| explained_variance | 0.606         |\n",
            "| fps                | 1023          |\n",
            "| n_updates          | 720           |\n",
            "| policy_entropy     | 0.1280788     |\n",
            "| policy_loss        | -0.0030649095 |\n",
            "| serial_timesteps   | 368640        |\n",
            "| time_elapsed       | 3.52e+03      |\n",
            "| total_timesteps    | 2949120       |\n",
            "| value_loss         | 0.91562617    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2950000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.80 +/- 123.12\n",
            "-------------------------------------\n",
            "| approxkl           | 0.017817814  |\n",
            "| clipfrac           | 0.05883789   |\n",
            "| ep_len_mean        | 37.3         |\n",
            "| ep_reward_mean     | 0.892        |\n",
            "| explained_variance | 0.606        |\n",
            "| fps                | 847          |\n",
            "| n_updates          | 721          |\n",
            "| policy_entropy     | 0.14765112   |\n",
            "| policy_loss        | 0.0029031136 |\n",
            "| serial_timesteps   | 369152       |\n",
            "| time_elapsed       | 3.53e+03     |\n",
            "| total_timesteps    | 2953216      |\n",
            "| value_loss         | 0.9343255    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015901731  |\n",
            "| clipfrac           | 0.078393556  |\n",
            "| ep_len_mean        | 26.8         |\n",
            "| ep_reward_mean     | 0.926        |\n",
            "| explained_variance | 0.491        |\n",
            "| fps                | 1002         |\n",
            "| n_updates          | 722          |\n",
            "| policy_entropy     | 0.1825408    |\n",
            "| policy_loss        | 0.0026480958 |\n",
            "| serial_timesteps   | 369664       |\n",
            "| time_elapsed       | 3.53e+03     |\n",
            "| total_timesteps    | 2957312      |\n",
            "| value_loss         | 0.8254611    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2960000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.60 +/- 3.61\n",
            "-------------------------------------\n",
            "| approxkl           | 0.021301264  |\n",
            "| clipfrac           | 0.103125     |\n",
            "| ep_len_mean        | 35           |\n",
            "| ep_reward_mean     | 0.901        |\n",
            "| explained_variance | 0.296        |\n",
            "| fps                | 982          |\n",
            "| n_updates          | 723          |\n",
            "| policy_entropy     | 0.20644787   |\n",
            "| policy_loss        | -0.008364774 |\n",
            "| serial_timesteps   | 370176       |\n",
            "| time_elapsed       | 3.54e+03     |\n",
            "| total_timesteps    | 2961408      |\n",
            "| value_loss         | 0.97026634   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024317756  |\n",
            "| clipfrac           | 0.10583496   |\n",
            "| ep_len_mean        | 34.3         |\n",
            "| ep_reward_mean     | 0.904        |\n",
            "| explained_variance | 0.395        |\n",
            "| fps                | 1030         |\n",
            "| n_updates          | 724          |\n",
            "| policy_entropy     | 0.2141865    |\n",
            "| policy_loss        | -0.009352008 |\n",
            "| serial_timesteps   | 370688       |\n",
            "| time_elapsed       | 3.54e+03     |\n",
            "| total_timesteps    | 2965504      |\n",
            "| value_loss         | 0.80727464   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.024667675   |\n",
            "| clipfrac           | 0.06503906    |\n",
            "| ep_len_mean        | 38.9          |\n",
            "| ep_reward_mean     | 0.888         |\n",
            "| explained_variance | 0.538         |\n",
            "| fps                | 1022          |\n",
            "| n_updates          | 725           |\n",
            "| policy_entropy     | 0.17890494    |\n",
            "| policy_loss        | -0.0048159785 |\n",
            "| serial_timesteps   | 371200        |\n",
            "| time_elapsed       | 3.54e+03      |\n",
            "| total_timesteps    | 2969600       |\n",
            "| value_loss         | 0.80736685    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2970000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.40 +/- 3.07\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01724694   |\n",
            "| clipfrac           | 0.07751465   |\n",
            "| ep_len_mean        | 41.1         |\n",
            "| ep_reward_mean     | 0.881        |\n",
            "| explained_variance | 0.617        |\n",
            "| fps                | 967          |\n",
            "| n_updates          | 726          |\n",
            "| policy_entropy     | 0.1859508    |\n",
            "| policy_loss        | 0.0005523247 |\n",
            "| serial_timesteps   | 371712       |\n",
            "| time_elapsed       | 3.55e+03     |\n",
            "| total_timesteps    | 2973696      |\n",
            "| value_loss         | 0.81980276   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.005155719  |\n",
            "| clipfrac           | 0.0404541    |\n",
            "| ep_len_mean        | 46.1         |\n",
            "| ep_reward_mean     | 0.866        |\n",
            "| explained_variance | 0.614        |\n",
            "| fps                | 1023         |\n",
            "| n_updates          | 727          |\n",
            "| policy_entropy     | 0.16570604   |\n",
            "| policy_loss        | -0.002862499 |\n",
            "| serial_timesteps   | 372224       |\n",
            "| time_elapsed       | 3.55e+03     |\n",
            "| total_timesteps    | 2977792      |\n",
            "| value_loss         | 0.7908263    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2980000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.40 +/- 122.83\n",
            "--------------------------------------\n",
            "| approxkl           | 0.007881595   |\n",
            "| clipfrac           | 0.032202147   |\n",
            "| ep_len_mean        | 23.8          |\n",
            "| ep_reward_mean     | 0.934         |\n",
            "| explained_variance | 0.539         |\n",
            "| fps                | 832           |\n",
            "| n_updates          | 728           |\n",
            "| policy_entropy     | 0.12222695    |\n",
            "| policy_loss        | 0.00076457456 |\n",
            "| serial_timesteps   | 372736        |\n",
            "| time_elapsed       | 3.56e+03      |\n",
            "| total_timesteps    | 2981888       |\n",
            "| value_loss         | 0.84990025    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011893836   |\n",
            "| clipfrac           | 0.042407226   |\n",
            "| ep_len_mean        | 31.9          |\n",
            "| ep_reward_mean     | 0.908         |\n",
            "| explained_variance | 0.642         |\n",
            "| fps                | 993           |\n",
            "| n_updates          | 729           |\n",
            "| policy_entropy     | 0.10916047    |\n",
            "| policy_loss        | -0.0030502297 |\n",
            "| serial_timesteps   | 373248        |\n",
            "| time_elapsed       | 3.56e+03      |\n",
            "| total_timesteps    | 2985984       |\n",
            "| value_loss         | 0.8015806     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2990000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.00 +/- 124.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008841293   |\n",
            "| clipfrac           | 0.042895507   |\n",
            "| ep_len_mean        | 32.9          |\n",
            "| ep_reward_mean     | 0.906         |\n",
            "| explained_variance | 0.634         |\n",
            "| fps                | 844           |\n",
            "| n_updates          | 730           |\n",
            "| policy_entropy     | 0.104623124   |\n",
            "| policy_loss        | -0.0029524763 |\n",
            "| serial_timesteps   | 373760        |\n",
            "| time_elapsed       | 3.57e+03      |\n",
            "| total_timesteps    | 2990080       |\n",
            "| value_loss         | 0.8144023     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.032107174  |\n",
            "| clipfrac           | 0.06547852   |\n",
            "| ep_len_mean        | 24.9         |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.397        |\n",
            "| fps                | 974          |\n",
            "| n_updates          | 731          |\n",
            "| policy_entropy     | 0.11578109   |\n",
            "| policy_loss        | 0.0037570975 |\n",
            "| serial_timesteps   | 374272       |\n",
            "| time_elapsed       | 3.57e+03     |\n",
            "| total_timesteps    | 2994176      |\n",
            "| value_loss         | 0.6926516    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.006123608 |\n",
            "| clipfrac           | 0.042871095 |\n",
            "| ep_len_mean        | 33.3        |\n",
            "| ep_reward_mean     | 0.904       |\n",
            "| explained_variance | 0.654       |\n",
            "| fps                | 1028        |\n",
            "| n_updates          | 732         |\n",
            "| policy_entropy     | 0.10288706  |\n",
            "| policy_loss        | 0.001193399 |\n",
            "| serial_timesteps   | 374784      |\n",
            "| time_elapsed       | 3.57e+03    |\n",
            "| total_timesteps    | 2998272     |\n",
            "| value_loss         | 0.7257253   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3000000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 77.20 +/- 123.42\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015978921   |\n",
            "| clipfrac           | 0.05227051    |\n",
            "| ep_len_mean        | 29            |\n",
            "| ep_reward_mean     | 0.917         |\n",
            "| explained_variance | 0.587         |\n",
            "| fps                | 834           |\n",
            "| n_updates          | 733           |\n",
            "| policy_entropy     | 0.13711265    |\n",
            "| policy_loss        | 0.00039165065 |\n",
            "| serial_timesteps   | 375296        |\n",
            "| time_elapsed       | 3.58e+03      |\n",
            "| total_timesteps    | 3002368       |\n",
            "| value_loss         | 0.8612604     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.03730134    |\n",
            "| clipfrac           | 0.07211914    |\n",
            "| ep_len_mean        | 27.2          |\n",
            "| ep_reward_mean     | 0.923         |\n",
            "| explained_variance | 0.604         |\n",
            "| fps                | 1000          |\n",
            "| n_updates          | 734           |\n",
            "| policy_entropy     | 0.121898055   |\n",
            "| policy_loss        | -0.0011646838 |\n",
            "| serial_timesteps   | 375808        |\n",
            "| time_elapsed       | 3.58e+03      |\n",
            "| total_timesteps    | 3006464       |\n",
            "| value_loss         | 0.7302164     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3010000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 263.40 +/- 121.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.049506888  |\n",
            "| clipfrac           | 0.13186035   |\n",
            "| ep_len_mean        | 29.1         |\n",
            "| ep_reward_mean     | 0.918        |\n",
            "| explained_variance | 0.61         |\n",
            "| fps                | 609          |\n",
            "| n_updates          | 735          |\n",
            "| policy_entropy     | 0.17064752   |\n",
            "| policy_loss        | -0.008588423 |\n",
            "| serial_timesteps   | 376320       |\n",
            "| time_elapsed       | 3.59e+03     |\n",
            "| total_timesteps    | 3010560      |\n",
            "| value_loss         | 0.7673127    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024897486  |\n",
            "| clipfrac           | 0.093164064  |\n",
            "| ep_len_mean        | 32.4         |\n",
            "| ep_reward_mean     | 0.909        |\n",
            "| explained_variance | 0.578        |\n",
            "| fps                | 993          |\n",
            "| n_updates          | 736          |\n",
            "| policy_entropy     | 0.19459385   |\n",
            "| policy_loss        | -0.004760745 |\n",
            "| serial_timesteps   | 376832       |\n",
            "| time_elapsed       | 3.59e+03     |\n",
            "| total_timesteps    | 3014656      |\n",
            "| value_loss         | 0.67540324   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.037684884 |\n",
            "| clipfrac           | 0.08283691  |\n",
            "| ep_len_mean        | 26.1        |\n",
            "| ep_reward_mean     | 0.928       |\n",
            "| explained_variance | 0.459       |\n",
            "| fps                | 1009        |\n",
            "| n_updates          | 737         |\n",
            "| policy_entropy     | 0.1797816   |\n",
            "| policy_loss        | 0.003357203 |\n",
            "| serial_timesteps   | 377344      |\n",
            "| time_elapsed       | 3.6e+03     |\n",
            "| total_timesteps    | 3018752     |\n",
            "| value_loss         | 0.7922941   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3020000, episode_reward=0.94 +/- 0.01\n",
            "Episode length: 20.40 +/- 3.32\n",
            "-------------------------------------\n",
            "| approxkl           | 0.038618505  |\n",
            "| clipfrac           | 0.07216797   |\n",
            "| ep_len_mean        | 24.2         |\n",
            "| ep_reward_mean     | 0.933        |\n",
            "| explained_variance | 0.485        |\n",
            "| fps                | 948          |\n",
            "| n_updates          | 738          |\n",
            "| policy_entropy     | 0.15906805   |\n",
            "| policy_loss        | 0.0024190382 |\n",
            "| serial_timesteps   | 377856       |\n",
            "| time_elapsed       | 3.6e+03      |\n",
            "| total_timesteps    | 3022848      |\n",
            "| value_loss         | 0.6768628    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.017970074  |\n",
            "| clipfrac           | 0.07807617   |\n",
            "| ep_len_mean        | 45.1         |\n",
            "| ep_reward_mean     | 0.875        |\n",
            "| explained_variance | 0.375        |\n",
            "| fps                | 994          |\n",
            "| n_updates          | 739          |\n",
            "| policy_entropy     | 0.2068424    |\n",
            "| policy_loss        | 0.0011270895 |\n",
            "| serial_timesteps   | 378368       |\n",
            "| time_elapsed       | 3.61e+03     |\n",
            "| total_timesteps    | 3026944      |\n",
            "| value_loss         | 0.6484267    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3030000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.80 +/- 4.45\n",
            "------------------------------------\n",
            "| approxkl           | 0.27562442  |\n",
            "| clipfrac           | 0.082128905 |\n",
            "| ep_len_mean        | 37.8        |\n",
            "| ep_reward_mean     | 0.894       |\n",
            "| explained_variance | 0.438       |\n",
            "| fps                | 953         |\n",
            "| n_updates          | 740         |\n",
            "| policy_entropy     | 0.17254224  |\n",
            "| policy_loss        | 0.002706787 |\n",
            "| serial_timesteps   | 378880      |\n",
            "| time_elapsed       | 3.61e+03    |\n",
            "| total_timesteps    | 3031040     |\n",
            "| value_loss         | 0.64907825  |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.034765065   |\n",
            "| clipfrac           | 0.103320315   |\n",
            "| ep_len_mean        | 34.5          |\n",
            "| ep_reward_mean     | 0.904         |\n",
            "| explained_variance | 0.418         |\n",
            "| fps                | 995           |\n",
            "| n_updates          | 741           |\n",
            "| policy_entropy     | 0.21165335    |\n",
            "| policy_loss        | -0.0017792288 |\n",
            "| serial_timesteps   | 379392        |\n",
            "| time_elapsed       | 3.62e+03      |\n",
            "| total_timesteps    | 3035136       |\n",
            "| value_loss         | 0.560798      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0100647155   |\n",
            "| clipfrac           | 0.083496094    |\n",
            "| ep_len_mean        | 56.4           |\n",
            "| ep_reward_mean     | 0.842          |\n",
            "| explained_variance | 0.298          |\n",
            "| fps                | 1009           |\n",
            "| n_updates          | 742            |\n",
            "| policy_entropy     | 0.2532226      |\n",
            "| policy_loss        | -0.00015387303 |\n",
            "| serial_timesteps   | 379904         |\n",
            "| time_elapsed       | 3.62e+03       |\n",
            "| total_timesteps    | 3039232        |\n",
            "| value_loss         | 0.6910774      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=3040000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.80 +/- 3.66\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016643247  |\n",
            "| clipfrac           | 0.1076416    |\n",
            "| ep_len_mean        | 64.7         |\n",
            "| ep_reward_mean     | 0.813        |\n",
            "| explained_variance | 0.377        |\n",
            "| fps                | 955          |\n",
            "| n_updates          | 743          |\n",
            "| policy_entropy     | 0.27211624   |\n",
            "| policy_loss        | -0.008905654 |\n",
            "| serial_timesteps   | 380416       |\n",
            "| time_elapsed       | 3.62e+03     |\n",
            "| total_timesteps    | 3043328      |\n",
            "| value_loss         | 0.8060878    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.013051634   |\n",
            "| clipfrac           | 0.084350586   |\n",
            "| ep_len_mean        | 50.7          |\n",
            "| ep_reward_mean     | 0.856         |\n",
            "| explained_variance | 0.407         |\n",
            "| fps                | 1025          |\n",
            "| n_updates          | 744           |\n",
            "| policy_entropy     | 0.27241462    |\n",
            "| policy_loss        | -0.0011795069 |\n",
            "| serial_timesteps   | 380928        |\n",
            "| time_elapsed       | 3.63e+03      |\n",
            "| total_timesteps    | 3047424       |\n",
            "| value_loss         | 0.7679705     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3050000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.00 +/- 2.53\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026345516  |\n",
            "| clipfrac           | 0.07900391   |\n",
            "| ep_len_mean        | 42.2         |\n",
            "| ep_reward_mean     | 0.882        |\n",
            "| explained_variance | 0.398        |\n",
            "| fps                | 955          |\n",
            "| n_updates          | 745          |\n",
            "| policy_entropy     | 0.17996684   |\n",
            "| policy_loss        | -0.002339311 |\n",
            "| serial_timesteps   | 381440       |\n",
            "| time_elapsed       | 3.63e+03     |\n",
            "| total_timesteps    | 3051520      |\n",
            "| value_loss         | 0.62646574   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014667146   |\n",
            "| clipfrac           | 0.051635742   |\n",
            "| ep_len_mean        | 30.9          |\n",
            "| ep_reward_mean     | 0.913         |\n",
            "| explained_variance | 0.48          |\n",
            "| fps                | 1011          |\n",
            "| n_updates          | 746           |\n",
            "| policy_entropy     | 0.14197332    |\n",
            "| policy_loss        | -0.0019413851 |\n",
            "| serial_timesteps   | 381952        |\n",
            "| time_elapsed       | 3.64e+03      |\n",
            "| total_timesteps    | 3055616       |\n",
            "| value_loss         | 0.63051003    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023324754  |\n",
            "| clipfrac           | 0.104321286  |\n",
            "| ep_len_mean        | 30.7         |\n",
            "| ep_reward_mean     | 0.913        |\n",
            "| explained_variance | 0.564        |\n",
            "| fps                | 1012         |\n",
            "| n_updates          | 747          |\n",
            "| policy_entropy     | 0.16569015   |\n",
            "| policy_loss        | -0.008172149 |\n",
            "| serial_timesteps   | 382464       |\n",
            "| time_elapsed       | 3.64e+03     |\n",
            "| total_timesteps    | 3059712      |\n",
            "| value_loss         | 0.6689611    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3060000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.80 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.06795487   |\n",
            "| clipfrac           | 0.063964844  |\n",
            "| ep_len_mean        | 31.9         |\n",
            "| ep_reward_mean     | 0.911        |\n",
            "| explained_variance | 0.461        |\n",
            "| fps                | 974          |\n",
            "| n_updates          | 748          |\n",
            "| policy_entropy     | 0.13274863   |\n",
            "| policy_loss        | 0.0028857908 |\n",
            "| serial_timesteps   | 382976       |\n",
            "| time_elapsed       | 3.64e+03     |\n",
            "| total_timesteps    | 3063808      |\n",
            "| value_loss         | 0.636436     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.031283904   |\n",
            "| clipfrac           | 0.11809082    |\n",
            "| ep_len_mean        | 38.6          |\n",
            "| ep_reward_mean     | 0.892         |\n",
            "| explained_variance | 0.478         |\n",
            "| fps                | 967           |\n",
            "| n_updates          | 749           |\n",
            "| policy_entropy     | 0.19970025    |\n",
            "| policy_loss        | -0.0061368505 |\n",
            "| serial_timesteps   | 383488        |\n",
            "| time_elapsed       | 3.65e+03      |\n",
            "| total_timesteps    | 3067904       |\n",
            "| value_loss         | 0.5475258     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3070000, episode_reward=0.57 +/- 0.46\n",
            "Episode length: 141.00 +/- 149.44\n",
            "--------------------------------------\n",
            "| approxkl           | 0.02877825    |\n",
            "| clipfrac           | 0.08654785    |\n",
            "| ep_len_mean        | 34.7          |\n",
            "| ep_reward_mean     | 0.904         |\n",
            "| explained_variance | 0.441         |\n",
            "| fps                | 734           |\n",
            "| n_updates          | 750           |\n",
            "| policy_entropy     | 0.14566891    |\n",
            "| policy_loss        | -0.0069136424 |\n",
            "| serial_timesteps   | 384000        |\n",
            "| time_elapsed       | 3.65e+03      |\n",
            "| total_timesteps    | 3072000       |\n",
            "| value_loss         | 0.6076017     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.014153744    |\n",
            "| clipfrac           | 0.050683595    |\n",
            "| ep_len_mean        | 38.9           |\n",
            "| ep_reward_mean     | 0.89           |\n",
            "| explained_variance | 0.436          |\n",
            "| fps                | 984            |\n",
            "| n_updates          | 751            |\n",
            "| policy_entropy     | 0.13471183     |\n",
            "| policy_loss        | -0.00027292772 |\n",
            "| serial_timesteps   | 384512         |\n",
            "| time_elapsed       | 3.66e+03       |\n",
            "| total_timesteps    | 3076096        |\n",
            "| value_loss         | 0.7660685      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=3080000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.40 +/- 123.30\n",
            "------------------------------------\n",
            "| approxkl           | 0.02987178  |\n",
            "| clipfrac           | 0.06435547  |\n",
            "| ep_len_mean        | 32          |\n",
            "| ep_reward_mean     | 0.91        |\n",
            "| explained_variance | 0.496       |\n",
            "| fps                | 826         |\n",
            "| n_updates          | 752         |\n",
            "| policy_entropy     | 0.12815133  |\n",
            "| policy_loss        | 0.001223399 |\n",
            "| serial_timesteps   | 385024      |\n",
            "| time_elapsed       | 3.66e+03    |\n",
            "| total_timesteps    | 3080192     |\n",
            "| value_loss         | 0.6203188   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012806287   |\n",
            "| clipfrac           | 0.069042966   |\n",
            "| ep_len_mean        | 38.8          |\n",
            "| ep_reward_mean     | 0.891         |\n",
            "| explained_variance | 0.42          |\n",
            "| fps                | 984           |\n",
            "| n_updates          | 753           |\n",
            "| policy_entropy     | 0.14263013    |\n",
            "| policy_loss        | -0.0052365693 |\n",
            "| serial_timesteps   | 385536        |\n",
            "| time_elapsed       | 3.67e+03      |\n",
            "| total_timesteps    | 3084288       |\n",
            "| value_loss         | 0.6824031     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.028451603  |\n",
            "| clipfrac           | 0.05732422   |\n",
            "| ep_len_mean        | 49.9         |\n",
            "| ep_reward_mean     | 0.858        |\n",
            "| explained_variance | 0.43         |\n",
            "| fps                | 1010         |\n",
            "| n_updates          | 754          |\n",
            "| policy_entropy     | 0.10236655   |\n",
            "| policy_loss        | 0.0010741779 |\n",
            "| serial_timesteps   | 386048       |\n",
            "| time_elapsed       | 3.67e+03     |\n",
            "| total_timesteps    | 3088384      |\n",
            "| value_loss         | 0.68460697   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3090000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.80 +/- 123.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.032566566  |\n",
            "| clipfrac           | 0.11416016   |\n",
            "| ep_len_mean        | 31.4         |\n",
            "| ep_reward_mean     | 0.911        |\n",
            "| explained_variance | 0.188        |\n",
            "| fps                | 837          |\n",
            "| n_updates          | 755          |\n",
            "| policy_entropy     | 0.17062877   |\n",
            "| policy_loss        | -0.020078722 |\n",
            "| serial_timesteps   | 386560       |\n",
            "| time_elapsed       | 3.68e+03     |\n",
            "| total_timesteps    | 3092480      |\n",
            "| value_loss         | 0.99380034   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.033572096 |\n",
            "| clipfrac           | 0.091870114 |\n",
            "| ep_len_mean        | 43.3        |\n",
            "| ep_reward_mean     | 0.875       |\n",
            "| explained_variance | 0.448       |\n",
            "| fps                | 1014        |\n",
            "| n_updates          | 756         |\n",
            "| policy_entropy     | 0.22758713  |\n",
            "| policy_loss        | 0.001266239 |\n",
            "| serial_timesteps   | 387072      |\n",
            "| time_elapsed       | 3.68e+03    |\n",
            "| total_timesteps    | 3096576     |\n",
            "| value_loss         | 0.7468613   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3100000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.60 +/- 123.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023213683  |\n",
            "| clipfrac           | 0.045166016  |\n",
            "| ep_len_mean        | 57           |\n",
            "| ep_reward_mean     | 0.835        |\n",
            "| explained_variance | 0.352        |\n",
            "| fps                | 850          |\n",
            "| n_updates          | 757          |\n",
            "| policy_entropy     | 0.14959216   |\n",
            "| policy_loss        | 0.0021716356 |\n",
            "| serial_timesteps   | 387584       |\n",
            "| time_elapsed       | 3.68e+03     |\n",
            "| total_timesteps    | 3100672      |\n",
            "| value_loss         | 0.94076884   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.017105965  |\n",
            "| clipfrac           | 0.07080078   |\n",
            "| ep_len_mean        | 39.1         |\n",
            "| ep_reward_mean     | 0.889        |\n",
            "| explained_variance | 0.393        |\n",
            "| fps                | 1015         |\n",
            "| n_updates          | 758          |\n",
            "| policy_entropy     | 0.1532974    |\n",
            "| policy_loss        | -0.008267386 |\n",
            "| serial_timesteps   | 388096       |\n",
            "| time_elapsed       | 3.69e+03     |\n",
            "| total_timesteps    | 3104768      |\n",
            "| value_loss         | 0.85259885   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.039087743   |\n",
            "| clipfrac           | 0.07709961    |\n",
            "| ep_len_mean        | 41            |\n",
            "| ep_reward_mean     | 0.885         |\n",
            "| explained_variance | 0.456         |\n",
            "| fps                | 1008          |\n",
            "| n_updates          | 759           |\n",
            "| policy_entropy     | 0.162659      |\n",
            "| policy_loss        | -0.0033113523 |\n",
            "| serial_timesteps   | 388608        |\n",
            "| time_elapsed       | 3.69e+03      |\n",
            "| total_timesteps    | 3108864       |\n",
            "| value_loss         | 0.72380793    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3110000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.80 +/- 122.66\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01413326   |\n",
            "| clipfrac           | 0.1161377    |\n",
            "| ep_len_mean        | 31.1         |\n",
            "| ep_reward_mean     | 0.914        |\n",
            "| explained_variance | 0.409        |\n",
            "| fps                | 852          |\n",
            "| n_updates          | 760          |\n",
            "| policy_entropy     | 0.22715822   |\n",
            "| policy_loss        | -0.017270856 |\n",
            "| serial_timesteps   | 389120       |\n",
            "| time_elapsed       | 3.7e+03      |\n",
            "| total_timesteps    | 3112960      |\n",
            "| value_loss         | 0.55458075   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023113526  |\n",
            "| clipfrac           | 0.08164062   |\n",
            "| ep_len_mean        | 41.6         |\n",
            "| ep_reward_mean     | 0.883        |\n",
            "| explained_variance | 0.468        |\n",
            "| fps                | 970          |\n",
            "| n_updates          | 761          |\n",
            "| policy_entropy     | 0.20020816   |\n",
            "| policy_loss        | -0.003871498 |\n",
            "| serial_timesteps   | 389632       |\n",
            "| time_elapsed       | 3.7e+03      |\n",
            "| total_timesteps    | 3117056      |\n",
            "| value_loss         | 0.6805947    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3120000, episode_reward=0.58 +/- 0.47\n",
            "Episode length: 138.40 +/- 151.54\n",
            "-------------------------------------\n",
            "| approxkl           | 0.025090974  |\n",
            "| clipfrac           | 0.078271486  |\n",
            "| ep_len_mean        | 31.1         |\n",
            "| ep_reward_mean     | 0.914        |\n",
            "| explained_variance | 0.499        |\n",
            "| fps                | 738          |\n",
            "| n_updates          | 762          |\n",
            "| policy_entropy     | 0.19964555   |\n",
            "| policy_loss        | 0.0009095232 |\n",
            "| serial_timesteps   | 390144       |\n",
            "| time_elapsed       | 3.71e+03     |\n",
            "| total_timesteps    | 3121152      |\n",
            "| value_loss         | 0.59670085   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.025848333   |\n",
            "| clipfrac           | 0.10856934    |\n",
            "| ep_len_mean        | 35.9          |\n",
            "| ep_reward_mean     | 0.9           |\n",
            "| explained_variance | 0.483         |\n",
            "| fps                | 1002          |\n",
            "| n_updates          | 763           |\n",
            "| policy_entropy     | 0.20273283    |\n",
            "| policy_loss        | -0.0048326273 |\n",
            "| serial_timesteps   | 390656        |\n",
            "| time_elapsed       | 3.71e+03      |\n",
            "| total_timesteps    | 3125248       |\n",
            "| value_loss         | 0.59853476    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.022294357   |\n",
            "| clipfrac           | 0.06337891    |\n",
            "| ep_len_mean        | 24.5          |\n",
            "| ep_reward_mean     | 0.932         |\n",
            "| explained_variance | 0.48          |\n",
            "| fps                | 987           |\n",
            "| n_updates          | 764           |\n",
            "| policy_entropy     | 0.18605141    |\n",
            "| policy_loss        | -0.0033561576 |\n",
            "| serial_timesteps   | 391168        |\n",
            "| time_elapsed       | 3.72e+03      |\n",
            "| total_timesteps    | 3129344       |\n",
            "| value_loss         | 0.5915966     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3130000, episode_reward=0.93 +/- 0.06\n",
            "Episode length: 25.00 +/- 23.01\n",
            "-------------------------------------\n",
            "| approxkl           | 0.06052658   |\n",
            "| clipfrac           | 0.15114745   |\n",
            "| ep_len_mean        | 24.9         |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.366        |\n",
            "| fps                | 959          |\n",
            "| n_updates          | 765          |\n",
            "| policy_entropy     | 0.21416965   |\n",
            "| policy_loss        | -0.009222502 |\n",
            "| serial_timesteps   | 391680       |\n",
            "| time_elapsed       | 3.72e+03     |\n",
            "| total_timesteps    | 3133440      |\n",
            "| value_loss         | 0.6298346    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.031024188   |\n",
            "| clipfrac           | 0.08703613    |\n",
            "| ep_len_mean        | 44.4          |\n",
            "| ep_reward_mean     | 0.875         |\n",
            "| explained_variance | 0.371         |\n",
            "| fps                | 995           |\n",
            "| n_updates          | 766           |\n",
            "| policy_entropy     | 0.23632231    |\n",
            "| policy_loss        | 5.4034834e-05 |\n",
            "| serial_timesteps   | 392192        |\n",
            "| time_elapsed       | 3.72e+03      |\n",
            "| total_timesteps    | 3137536       |\n",
            "| value_loss         | 0.6855168     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3140000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.80 +/- 150.44\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01360514    |\n",
            "| clipfrac           | 0.0881836     |\n",
            "| ep_len_mean        | 38.9          |\n",
            "| ep_reward_mean     | 0.892         |\n",
            "| explained_variance | 0.435         |\n",
            "| fps                | 732           |\n",
            "| n_updates          | 767           |\n",
            "| policy_entropy     | 0.22689402    |\n",
            "| policy_loss        | -0.0046213637 |\n",
            "| serial_timesteps   | 392704        |\n",
            "| time_elapsed       | 3.73e+03      |\n",
            "| total_timesteps    | 3141632       |\n",
            "| value_loss         | 0.5799631     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.019121775   |\n",
            "| clipfrac           | 0.06850586    |\n",
            "| ep_len_mean        | 28.9          |\n",
            "| ep_reward_mean     | 0.92          |\n",
            "| explained_variance | 0.521         |\n",
            "| fps                | 1019          |\n",
            "| n_updates          | 768           |\n",
            "| policy_entropy     | 0.17849734    |\n",
            "| policy_loss        | -0.0034097575 |\n",
            "| serial_timesteps   | 393216        |\n",
            "| time_elapsed       | 3.73e+03      |\n",
            "| total_timesteps    | 3145728       |\n",
            "| value_loss         | 0.4947725     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010532079   |\n",
            "| clipfrac           | 0.05703125    |\n",
            "| ep_len_mean        | 35.3          |\n",
            "| ep_reward_mean     | 0.901         |\n",
            "| explained_variance | 0.528         |\n",
            "| fps                | 1000          |\n",
            "| n_updates          | 769           |\n",
            "| policy_entropy     | 0.18288018    |\n",
            "| policy_loss        | -0.0010814683 |\n",
            "| serial_timesteps   | 393728        |\n",
            "| time_elapsed       | 3.74e+03      |\n",
            "| total_timesteps    | 3149824       |\n",
            "| value_loss         | 0.59385246    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3150000, episode_reward=0.75 +/- 0.38\n",
            "Episode length: 81.40 +/- 121.31\n",
            "--------------------------------------\n",
            "| approxkl           | 0.024824034   |\n",
            "| clipfrac           | 0.06411133    |\n",
            "| ep_len_mean        | 26            |\n",
            "| ep_reward_mean     | 0.927         |\n",
            "| explained_variance | 0.576         |\n",
            "| fps                | 825           |\n",
            "| n_updates          | 770           |\n",
            "| policy_entropy     | 0.16553426    |\n",
            "| policy_loss        | -0.0029839713 |\n",
            "| serial_timesteps   | 394240        |\n",
            "| time_elapsed       | 3.74e+03      |\n",
            "| total_timesteps    | 3153920       |\n",
            "| value_loss         | 0.5761445     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.55729425  |\n",
            "| clipfrac           | 0.11835937  |\n",
            "| ep_len_mean        | 37.5        |\n",
            "| ep_reward_mean     | 0.891       |\n",
            "| explained_variance | 0.442       |\n",
            "| fps                | 998         |\n",
            "| n_updates          | 771         |\n",
            "| policy_entropy     | 0.13182643  |\n",
            "| policy_loss        | -0.01381986 |\n",
            "| serial_timesteps   | 394752      |\n",
            "| time_elapsed       | 3.75e+03    |\n",
            "| total_timesteps    | 3158016     |\n",
            "| value_loss         | 0.8471397   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3160000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 80.20 +/- 121.96\n",
            "-------------------------------------\n",
            "| approxkl           | 0.042905297  |\n",
            "| clipfrac           | 0.06496582   |\n",
            "| ep_len_mean        | 34.8         |\n",
            "| ep_reward_mean     | 0.902        |\n",
            "| explained_variance | 0.431        |\n",
            "| fps                | 856          |\n",
            "| n_updates          | 772          |\n",
            "| policy_entropy     | 0.15492824   |\n",
            "| policy_loss        | 0.0003688719 |\n",
            "| serial_timesteps   | 395264       |\n",
            "| time_elapsed       | 3.75e+03     |\n",
            "| total_timesteps    | 3162112      |\n",
            "| value_loss         | 0.6795635    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022693636  |\n",
            "| clipfrac           | 0.075854495  |\n",
            "| ep_len_mean        | 34           |\n",
            "| ep_reward_mean     | 0.904        |\n",
            "| explained_variance | 0.546        |\n",
            "| fps                | 1005         |\n",
            "| n_updates          | 773          |\n",
            "| policy_entropy     | 0.1740756    |\n",
            "| policy_loss        | -0.002136366 |\n",
            "| serial_timesteps   | 395776       |\n",
            "| time_elapsed       | 3.76e+03     |\n",
            "| total_timesteps    | 3166208      |\n",
            "| value_loss         | 0.6229188    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3170000, episode_reward=0.72 +/- 0.37\n",
            "Episode length: 94.60 +/- 119.62\n",
            "--------------------------------------\n",
            "| approxkl           | 0.021248817   |\n",
            "| clipfrac           | 0.050512694   |\n",
            "| ep_len_mean        | 25            |\n",
            "| ep_reward_mean     | 0.931         |\n",
            "| explained_variance | 0.557         |\n",
            "| fps                | 805           |\n",
            "| n_updates          | 774           |\n",
            "| policy_entropy     | 0.11911384    |\n",
            "| policy_loss        | -0.0048225364 |\n",
            "| serial_timesteps   | 396288        |\n",
            "| time_elapsed       | 3.76e+03      |\n",
            "| total_timesteps    | 3170304       |\n",
            "| value_loss         | 0.49706596    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.018466618   |\n",
            "| clipfrac           | 0.071875      |\n",
            "| ep_len_mean        | 37.7          |\n",
            "| ep_reward_mean     | 0.892         |\n",
            "| explained_variance | 0.538         |\n",
            "| fps                | 1015          |\n",
            "| n_updates          | 775           |\n",
            "| policy_entropy     | 0.18643913    |\n",
            "| policy_loss        | -0.0026008566 |\n",
            "| serial_timesteps   | 396800        |\n",
            "| time_elapsed       | 3.77e+03      |\n",
            "| total_timesteps    | 3174400       |\n",
            "| value_loss         | 0.705691      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.021131177   |\n",
            "| clipfrac           | 0.064575195   |\n",
            "| ep_len_mean        | 39.5          |\n",
            "| ep_reward_mean     | 0.889         |\n",
            "| explained_variance | 0.521         |\n",
            "| fps                | 994           |\n",
            "| n_updates          | 776           |\n",
            "| policy_entropy     | 0.18613155    |\n",
            "| policy_loss        | -0.0030158905 |\n",
            "| serial_timesteps   | 397312        |\n",
            "| time_elapsed       | 3.77e+03      |\n",
            "| total_timesteps    | 3178496       |\n",
            "| value_loss         | 0.6852713     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3180000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.00 +/- 2.68\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01812111    |\n",
            "| clipfrac           | 0.06530762    |\n",
            "| ep_len_mean        | 33.6          |\n",
            "| ep_reward_mean     | 0.904         |\n",
            "| explained_variance | 0.537         |\n",
            "| fps                | 975           |\n",
            "| n_updates          | 777           |\n",
            "| policy_entropy     | 0.16722342    |\n",
            "| policy_loss        | -7.738146e-05 |\n",
            "| serial_timesteps   | 397824        |\n",
            "| time_elapsed       | 3.77e+03      |\n",
            "| total_timesteps    | 3182592       |\n",
            "| value_loss         | 0.6489757     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010832251   |\n",
            "| clipfrac           | 0.05986328    |\n",
            "| ep_len_mean        | 30.6          |\n",
            "| ep_reward_mean     | 0.914         |\n",
            "| explained_variance | 0.506         |\n",
            "| fps                | 1015          |\n",
            "| n_updates          | 778           |\n",
            "| policy_entropy     | 0.16263473    |\n",
            "| policy_loss        | -0.0018589055 |\n",
            "| serial_timesteps   | 398336        |\n",
            "| time_elapsed       | 3.78e+03      |\n",
            "| total_timesteps    | 3186688       |\n",
            "| value_loss         | 0.66623276    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3190000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.80 +/- 122.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.029330647   |\n",
            "| clipfrac           | 0.06506348    |\n",
            "| ep_len_mean        | 34.3          |\n",
            "| ep_reward_mean     | 0.904         |\n",
            "| explained_variance | 0.528         |\n",
            "| fps                | 835           |\n",
            "| n_updates          | 779           |\n",
            "| policy_entropy     | 0.122823216   |\n",
            "| policy_loss        | -0.0014258235 |\n",
            "| serial_timesteps   | 398848        |\n",
            "| time_elapsed       | 3.78e+03      |\n",
            "| total_timesteps    | 3190784       |\n",
            "| value_loss         | 0.6622336     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014827184  |\n",
            "| clipfrac           | 0.054296874  |\n",
            "| ep_len_mean        | 29.9         |\n",
            "| ep_reward_mean     | 0.917        |\n",
            "| explained_variance | 0.501        |\n",
            "| fps                | 997          |\n",
            "| n_updates          | 780          |\n",
            "| policy_entropy     | 0.13833986   |\n",
            "| policy_loss        | 0.0012033095 |\n",
            "| serial_timesteps   | 399360       |\n",
            "| time_elapsed       | 3.79e+03     |\n",
            "| total_timesteps    | 3194880      |\n",
            "| value_loss         | 0.5834899    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010742706  |\n",
            "| clipfrac           | 0.03881836   |\n",
            "| ep_len_mean        | 25.6         |\n",
            "| ep_reward_mean     | 0.928        |\n",
            "| explained_variance | 0.518        |\n",
            "| fps                | 1026         |\n",
            "| n_updates          | 781          |\n",
            "| policy_entropy     | 0.10860423   |\n",
            "| policy_loss        | 0.0016484989 |\n",
            "| serial_timesteps   | 399872       |\n",
            "| time_elapsed       | 3.79e+03     |\n",
            "| total_timesteps    | 3198976      |\n",
            "| value_loss         | 0.6410226    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3200000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.20 +/- 3.71\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0076631033 |\n",
            "| clipfrac           | 0.04082031   |\n",
            "| ep_len_mean        | 32.3         |\n",
            "| ep_reward_mean     | 0.907        |\n",
            "| explained_variance | 0.549        |\n",
            "| fps                | 985          |\n",
            "| n_updates          | 782          |\n",
            "| policy_entropy     | 0.10102768   |\n",
            "| policy_loss        | -0.001807192 |\n",
            "| serial_timesteps   | 400384       |\n",
            "| time_elapsed       | 3.79e+03     |\n",
            "| total_timesteps    | 3203072      |\n",
            "| value_loss         | 0.6800973    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.027202364 |\n",
            "| clipfrac           | 0.112695314 |\n",
            "| ep_len_mean        | 18.7        |\n",
            "| ep_reward_mean     | 0.948       |\n",
            "| explained_variance | 0.542       |\n",
            "| fps                | 1010        |\n",
            "| n_updates          | 783         |\n",
            "| policy_entropy     | 0.12185146  |\n",
            "| policy_loss        | -0.01800659 |\n",
            "| serial_timesteps   | 400896      |\n",
            "| time_elapsed       | 3.8e+03     |\n",
            "| total_timesteps    | 3207168     |\n",
            "| value_loss         | 0.52525216  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3210000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.40 +/- 122.35\n",
            "--------------------------------------\n",
            "| approxkl           | 0.057591826   |\n",
            "| clipfrac           | 0.097753905   |\n",
            "| ep_len_mean        | 28.7          |\n",
            "| ep_reward_mean     | 0.918         |\n",
            "| explained_variance | 0.552         |\n",
            "| fps                | 840           |\n",
            "| n_updates          | 784           |\n",
            "| policy_entropy     | 0.13813046    |\n",
            "| policy_loss        | -0.0030675642 |\n",
            "| serial_timesteps   | 401408        |\n",
            "| time_elapsed       | 3.8e+03       |\n",
            "| total_timesteps    | 3211264       |\n",
            "| value_loss         | 0.6766428     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023771852  |\n",
            "| clipfrac           | 0.07180176   |\n",
            "| ep_len_mean        | 32.2         |\n",
            "| ep_reward_mean     | 0.907        |\n",
            "| explained_variance | 0.56         |\n",
            "| fps                | 1001         |\n",
            "| n_updates          | 785          |\n",
            "| policy_entropy     | 0.14178964   |\n",
            "| policy_loss        | 0.0041095973 |\n",
            "| serial_timesteps   | 401920       |\n",
            "| time_elapsed       | 3.81e+03     |\n",
            "| total_timesteps    | 3215360      |\n",
            "| value_loss         | 0.7780352    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03337267   |\n",
            "| clipfrac           | 0.064404294  |\n",
            "| ep_len_mean        | 25           |\n",
            "| ep_reward_mean     | 0.929        |\n",
            "| explained_variance | 0.454        |\n",
            "| fps                | 1012         |\n",
            "| n_updates          | 786          |\n",
            "| policy_entropy     | 0.105505966  |\n",
            "| policy_loss        | -0.002399549 |\n",
            "| serial_timesteps   | 402432       |\n",
            "| time_elapsed       | 3.81e+03     |\n",
            "| total_timesteps    | 3219456      |\n",
            "| value_loss         | 0.6906208    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3220000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.80 +/- 3.92\n",
            "------------------------------------\n",
            "| approxkl           | 0.0608614   |\n",
            "| clipfrac           | 0.06286621  |\n",
            "| ep_len_mean        | 25.9        |\n",
            "| ep_reward_mean     | 0.928       |\n",
            "| explained_variance | 0.325       |\n",
            "| fps                | 976         |\n",
            "| n_updates          | 787         |\n",
            "| policy_entropy     | 0.096154    |\n",
            "| policy_loss        | 0.008296603 |\n",
            "| serial_timesteps   | 402944      |\n",
            "| time_elapsed       | 3.82e+03    |\n",
            "| total_timesteps    | 3223552     |\n",
            "| value_loss         | 0.6202827   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026801128  |\n",
            "| clipfrac           | 0.09846191   |\n",
            "| ep_len_mean        | 35.2         |\n",
            "| ep_reward_mean     | 0.899        |\n",
            "| explained_variance | 0.507        |\n",
            "| fps                | 976          |\n",
            "| n_updates          | 788          |\n",
            "| policy_entropy     | 0.12554747   |\n",
            "| policy_loss        | -0.007570827 |\n",
            "| serial_timesteps   | 403456       |\n",
            "| time_elapsed       | 3.82e+03     |\n",
            "| total_timesteps    | 3227648      |\n",
            "| value_loss         | 0.77020305   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3230000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.40 +/- 3.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020515602  |\n",
            "| clipfrac           | 0.052685548  |\n",
            "| ep_len_mean        | 27.8         |\n",
            "| ep_reward_mean     | 0.921        |\n",
            "| explained_variance | 0.483        |\n",
            "| fps                | 966          |\n",
            "| n_updates          | 789          |\n",
            "| policy_entropy     | 0.10480885   |\n",
            "| policy_loss        | 0.0016670407 |\n",
            "| serial_timesteps   | 403968       |\n",
            "| time_elapsed       | 3.82e+03     |\n",
            "| total_timesteps    | 3231744      |\n",
            "| value_loss         | 0.63148177   |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.012263038    |\n",
            "| clipfrac           | 0.039404295    |\n",
            "| ep_len_mean        | 28.4           |\n",
            "| ep_reward_mean     | 0.918          |\n",
            "| explained_variance | 0.569          |\n",
            "| fps                | 999            |\n",
            "| n_updates          | 790            |\n",
            "| policy_entropy     | 0.0964867      |\n",
            "| policy_loss        | -0.00017012891 |\n",
            "| serial_timesteps   | 404480         |\n",
            "| time_elapsed       | 3.83e+03       |\n",
            "| total_timesteps    | 3235840        |\n",
            "| value_loss         | 0.6822747      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.018088728   |\n",
            "| clipfrac           | 0.052001953   |\n",
            "| ep_len_mean        | 29.9          |\n",
            "| ep_reward_mean     | 0.915         |\n",
            "| explained_variance | 0.456         |\n",
            "| fps                | 1015          |\n",
            "| n_updates          | 791           |\n",
            "| policy_entropy     | 0.10795426    |\n",
            "| policy_loss        | -0.0010966093 |\n",
            "| serial_timesteps   | 404992        |\n",
            "| time_elapsed       | 3.83e+03      |\n",
            "| total_timesteps    | 3239936       |\n",
            "| value_loss         | 0.6630253     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3240000, episode_reward=0.82 +/- 0.26\n",
            "Episode length: 63.60 +/- 95.21\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009848855  |\n",
            "| clipfrac           | 0.047167968  |\n",
            "| ep_len_mean        | 37.4         |\n",
            "| ep_reward_mean     | 0.892        |\n",
            "| explained_variance | 0.567        |\n",
            "| fps                | 858          |\n",
            "| n_updates          | 792          |\n",
            "| policy_entropy     | 0.12038096   |\n",
            "| policy_loss        | 0.0024591458 |\n",
            "| serial_timesteps   | 405504       |\n",
            "| time_elapsed       | 3.84e+03     |\n",
            "| total_timesteps    | 3244032      |\n",
            "| value_loss         | 0.6516398    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.011657793    |\n",
            "| clipfrac           | 0.03261719     |\n",
            "| ep_len_mean        | 24.8           |\n",
            "| ep_reward_mean     | 0.93           |\n",
            "| explained_variance | 0.591          |\n",
            "| fps                | 1010           |\n",
            "| n_updates          | 793            |\n",
            "| policy_entropy     | 0.07859118     |\n",
            "| policy_loss        | -0.00084436574 |\n",
            "| serial_timesteps   | 406016         |\n",
            "| time_elapsed       | 3.84e+03       |\n",
            "| total_timesteps    | 3248128        |\n",
            "| value_loss         | 0.55738497     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=3250000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 262.80 +/- 122.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03887884   |\n",
            "| clipfrac           | 0.14589843   |\n",
            "| ep_len_mean        | 59.6         |\n",
            "| ep_reward_mean     | 0.822        |\n",
            "| explained_variance | 0.31         |\n",
            "| fps                | 601          |\n",
            "| n_updates          | 794          |\n",
            "| policy_entropy     | 0.22059986   |\n",
            "| policy_loss        | -0.006842667 |\n",
            "| serial_timesteps   | 406528       |\n",
            "| time_elapsed       | 3.85e+03     |\n",
            "| total_timesteps    | 3252224      |\n",
            "| value_loss         | 1.0445869    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.027117666  |\n",
            "| clipfrac           | 0.22832032   |\n",
            "| ep_len_mean        | 88.9         |\n",
            "| ep_reward_mean     | 0.732        |\n",
            "| explained_variance | 0.123        |\n",
            "| fps                | 983          |\n",
            "| n_updates          | 795          |\n",
            "| policy_entropy     | 0.25879097   |\n",
            "| policy_loss        | -0.014830561 |\n",
            "| serial_timesteps   | 407040       |\n",
            "| time_elapsed       | 3.85e+03     |\n",
            "| total_timesteps    | 3256320      |\n",
            "| value_loss         | 0.96555173   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3260000, episode_reward=0.38 +/- 0.47\n",
            "Episode length: 201.60 +/- 149.91\n",
            "-------------------------------------\n",
            "| approxkl           | 0.044721503  |\n",
            "| clipfrac           | 0.11860351   |\n",
            "| ep_len_mean        | 128          |\n",
            "| ep_reward_mean     | 0.612        |\n",
            "| explained_variance | -0.141       |\n",
            "| fps                | 658          |\n",
            "| n_updates          | 796          |\n",
            "| policy_entropy     | 0.3305578    |\n",
            "| policy_loss        | 0.0064943023 |\n",
            "| serial_timesteps   | 407552       |\n",
            "| time_elapsed       | 3.86e+03     |\n",
            "| total_timesteps    | 3260416      |\n",
            "| value_loss         | 1.024276     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022005633  |\n",
            "| clipfrac           | 0.09658203   |\n",
            "| ep_len_mean        | 160          |\n",
            "| ep_reward_mean     | 0.514        |\n",
            "| explained_variance | 0.106        |\n",
            "| fps                | 945          |\n",
            "| n_updates          | 797          |\n",
            "| policy_entropy     | 0.30935213   |\n",
            "| policy_loss        | 0.0005300021 |\n",
            "| serial_timesteps   | 408064       |\n",
            "| time_elapsed       | 3.86e+03     |\n",
            "| total_timesteps    | 3264512      |\n",
            "| value_loss         | 0.73606217   |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.009903634    |\n",
            "| clipfrac           | 0.06970215     |\n",
            "| ep_len_mean        | 191            |\n",
            "| ep_reward_mean     | 0.417          |\n",
            "| explained_variance | 0.0551         |\n",
            "| fps                | 996            |\n",
            "| n_updates          | 798            |\n",
            "| policy_entropy     | 0.32169768     |\n",
            "| policy_loss        | -0.00072089717 |\n",
            "| serial_timesteps   | 408576         |\n",
            "| time_elapsed       | 3.87e+03       |\n",
            "| total_timesteps    | 3268608        |\n",
            "| value_loss         | 0.8146976      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=3270000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 138.80 +/- 151.22\n",
            "--------------------------------------\n",
            "| approxkl           | 0.047798388   |\n",
            "| clipfrac           | 0.07961426    |\n",
            "| ep_len_mean        | 177           |\n",
            "| ep_reward_mean     | 0.461         |\n",
            "| explained_variance | 0.491         |\n",
            "| fps                | 735           |\n",
            "| n_updates          | 799           |\n",
            "| policy_entropy     | 0.2577895     |\n",
            "| policy_loss        | -0.0018149086 |\n",
            "| serial_timesteps   | 409088        |\n",
            "| time_elapsed       | 3.87e+03      |\n",
            "| total_timesteps    | 3272704       |\n",
            "| value_loss         | 0.67554057    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.013845299   |\n",
            "| clipfrac           | 0.07111816    |\n",
            "| ep_len_mean        | 185           |\n",
            "| ep_reward_mean     | 0.436         |\n",
            "| explained_variance | 0.406         |\n",
            "| fps                | 984           |\n",
            "| n_updates          | 800           |\n",
            "| policy_entropy     | 0.26195592    |\n",
            "| policy_loss        | -0.0008291129 |\n",
            "| serial_timesteps   | 409600        |\n",
            "| time_elapsed       | 3.88e+03      |\n",
            "| total_timesteps    | 3276800       |\n",
            "| value_loss         | 0.52792907    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3280000, episode_reward=0.38 +/- 0.46\n",
            "Episode length: 203.00 +/- 148.21\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008926083   |\n",
            "| clipfrac           | 0.083178714   |\n",
            "| ep_len_mean        | 176           |\n",
            "| ep_reward_mean     | 0.464         |\n",
            "| explained_variance | 0.331         |\n",
            "| fps                | 675           |\n",
            "| n_updates          | 801           |\n",
            "| policy_entropy     | 0.3335294     |\n",
            "| policy_loss        | -0.0051385043 |\n",
            "| serial_timesteps   | 410112        |\n",
            "| time_elapsed       | 3.88e+03      |\n",
            "| total_timesteps    | 3280896       |\n",
            "| value_loss         | 0.60032564    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014053459   |\n",
            "| clipfrac           | 0.070043944   |\n",
            "| ep_len_mean        | 170           |\n",
            "| ep_reward_mean     | 0.485         |\n",
            "| explained_variance | 0.348         |\n",
            "| fps                | 998           |\n",
            "| n_updates          | 802           |\n",
            "| policy_entropy     | 0.34056538    |\n",
            "| policy_loss        | -0.0026809946 |\n",
            "| serial_timesteps   | 410624        |\n",
            "| time_elapsed       | 3.89e+03      |\n",
            "| total_timesteps    | 3284992       |\n",
            "| value_loss         | 0.58729637    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005175754   |\n",
            "| clipfrac           | 0.04423828    |\n",
            "| ep_len_mean        | 177           |\n",
            "| ep_reward_mean     | 0.463         |\n",
            "| explained_variance | 0.439         |\n",
            "| fps                | 998           |\n",
            "| n_updates          | 803           |\n",
            "| policy_entropy     | 0.30202785    |\n",
            "| policy_loss        | -0.0016219491 |\n",
            "| serial_timesteps   | 411136        |\n",
            "| time_elapsed       | 3.89e+03      |\n",
            "| total_timesteps    | 3289088       |\n",
            "| value_loss         | 0.52782005    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3290000, episode_reward=0.38 +/- 0.46\n",
            "Episode length: 202.80 +/- 148.46\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0044471184  |\n",
            "| clipfrac           | 0.045654297   |\n",
            "| ep_len_mean        | 179           |\n",
            "| ep_reward_mean     | 0.457         |\n",
            "| explained_variance | 0.604         |\n",
            "| fps                | 664           |\n",
            "| n_updates          | 804           |\n",
            "| policy_entropy     | 0.27624002    |\n",
            "| policy_loss        | -0.0025396075 |\n",
            "| serial_timesteps   | 411648        |\n",
            "| time_elapsed       | 3.9e+03       |\n",
            "| total_timesteps    | 3293184       |\n",
            "| value_loss         | 0.46167603    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01201222   |\n",
            "| clipfrac           | 0.10361328   |\n",
            "| ep_len_mean        | 183          |\n",
            "| ep_reward_mean     | 0.444        |\n",
            "| explained_variance | 0.505        |\n",
            "| fps                | 973          |\n",
            "| n_updates          | 805          |\n",
            "| policy_entropy     | 0.45486435   |\n",
            "| policy_loss        | -0.007676995 |\n",
            "| serial_timesteps   | 412160       |\n",
            "| time_elapsed       | 3.9e+03      |\n",
            "| total_timesteps    | 3297280      |\n",
            "| value_loss         | 0.3939975    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3300000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 263.40 +/- 121.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.030260837  |\n",
            "| clipfrac           | 0.060546875  |\n",
            "| ep_len_mean        | 162          |\n",
            "| ep_reward_mean     | 0.51         |\n",
            "| explained_variance | 0.618        |\n",
            "| fps                | 616          |\n",
            "| n_updates          | 806          |\n",
            "| policy_entropy     | 0.3451124    |\n",
            "| policy_loss        | -0.004213745 |\n",
            "| serial_timesteps   | 412672       |\n",
            "| time_elapsed       | 3.91e+03     |\n",
            "| total_timesteps    | 3301376      |\n",
            "| value_loss         | 0.6733422    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.40849528   |\n",
            "| clipfrac           | 0.17253418   |\n",
            "| ep_len_mean        | 92.4         |\n",
            "| ep_reward_mean     | 0.727        |\n",
            "| explained_variance | 0.336        |\n",
            "| fps                | 1014         |\n",
            "| n_updates          | 807          |\n",
            "| policy_entropy     | 0.41885224   |\n",
            "| policy_loss        | -0.005874784 |\n",
            "| serial_timesteps   | 413184       |\n",
            "| time_elapsed       | 3.91e+03     |\n",
            "| total_timesteps    | 3305472      |\n",
            "| value_loss         | 1.2400297    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.027783439  |\n",
            "| clipfrac           | 0.109838866  |\n",
            "| ep_len_mean        | 78.3         |\n",
            "| ep_reward_mean     | 0.771        |\n",
            "| explained_variance | 0.325        |\n",
            "| fps                | 1015         |\n",
            "| n_updates          | 808          |\n",
            "| policy_entropy     | 0.4413858    |\n",
            "| policy_loss        | 0.0017896785 |\n",
            "| serial_timesteps   | 413696       |\n",
            "| time_elapsed       | 3.92e+03     |\n",
            "| total_timesteps    | 3309568      |\n",
            "| value_loss         | 1.1694812    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3310000, episode_reward=0.75 +/- 0.38\n",
            "Episode length: 81.80 +/- 121.19\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013704424  |\n",
            "| clipfrac           | 0.08896484   |\n",
            "| ep_len_mean        | 82.9         |\n",
            "| ep_reward_mean     | 0.756        |\n",
            "| explained_variance | 0.378        |\n",
            "| fps                | 840          |\n",
            "| n_updates          | 809          |\n",
            "| policy_entropy     | 0.36971414   |\n",
            "| policy_loss        | -0.010270715 |\n",
            "| serial_timesteps   | 414208       |\n",
            "| time_elapsed       | 3.92e+03     |\n",
            "| total_timesteps    | 3313664      |\n",
            "| value_loss         | 0.93058264   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015306538   |\n",
            "| clipfrac           | 0.049194336   |\n",
            "| ep_len_mean        | 109           |\n",
            "| ep_reward_mean     | 0.675         |\n",
            "| explained_variance | 0.313         |\n",
            "| fps                | 1008          |\n",
            "| n_updates          | 810           |\n",
            "| policy_entropy     | 0.2776538     |\n",
            "| policy_loss        | -0.0024000357 |\n",
            "| serial_timesteps   | 414720        |\n",
            "| time_elapsed       | 3.93e+03      |\n",
            "| total_timesteps    | 3317760       |\n",
            "| value_loss         | 0.77319026    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3320000, episode_reward=0.38 +/- 0.46\n",
            "Episode length: 201.80 +/- 149.69\n",
            "--------------------------------------\n",
            "| approxkl           | 0.020901252   |\n",
            "| clipfrac           | 0.06875       |\n",
            "| ep_len_mean        | 129           |\n",
            "| ep_reward_mean     | 0.612         |\n",
            "| explained_variance | 0.43          |\n",
            "| fps                | 678           |\n",
            "| n_updates          | 811           |\n",
            "| policy_entropy     | 0.37951872    |\n",
            "| policy_loss        | -0.0011950076 |\n",
            "| serial_timesteps   | 415232        |\n",
            "| time_elapsed       | 3.93e+03      |\n",
            "| total_timesteps    | 3321856       |\n",
            "| value_loss         | 0.796564      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026425382  |\n",
            "| clipfrac           | 0.095263675  |\n",
            "| ep_len_mean        | 108          |\n",
            "| ep_reward_mean     | 0.68         |\n",
            "| explained_variance | 0.227        |\n",
            "| fps                | 1008         |\n",
            "| n_updates          | 812          |\n",
            "| policy_entropy     | 0.39494833   |\n",
            "| policy_loss        | 0.0008829989 |\n",
            "| serial_timesteps   | 415744       |\n",
            "| time_elapsed       | 3.94e+03     |\n",
            "| total_timesteps    | 3325952      |\n",
            "| value_loss         | 1.0448896    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3330000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.026599184   |\n",
            "| clipfrac           | 0.093969725   |\n",
            "| ep_len_mean        | 77.3          |\n",
            "| ep_reward_mean     | 0.778         |\n",
            "| explained_variance | 0.287         |\n",
            "| fps                | 549           |\n",
            "| n_updates          | 813           |\n",
            "| policy_entropy     | 0.34530568    |\n",
            "| policy_loss        | 0.00062563305 |\n",
            "| serial_timesteps   | 416256        |\n",
            "| time_elapsed       | 3.94e+03      |\n",
            "| total_timesteps    | 3330048       |\n",
            "| value_loss         | 0.9854108     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012291948   |\n",
            "| clipfrac           | 0.0887207     |\n",
            "| ep_len_mean        | 77.9          |\n",
            "| ep_reward_mean     | 0.776         |\n",
            "| explained_variance | 0.236         |\n",
            "| fps                | 973           |\n",
            "| n_updates          | 814           |\n",
            "| policy_entropy     | 0.36695522    |\n",
            "| policy_loss        | -0.0035842904 |\n",
            "| serial_timesteps   | 416768        |\n",
            "| time_elapsed       | 3.95e+03      |\n",
            "| total_timesteps    | 3334144       |\n",
            "| value_loss         | 0.91881895    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010987263  |\n",
            "| clipfrac           | 0.08625488   |\n",
            "| ep_len_mean        | 78.5         |\n",
            "| ep_reward_mean     | 0.771        |\n",
            "| explained_variance | 0.306        |\n",
            "| fps                | 990          |\n",
            "| n_updates          | 815          |\n",
            "| policy_entropy     | 0.40479922   |\n",
            "| policy_loss        | -0.002149748 |\n",
            "| serial_timesteps   | 417280       |\n",
            "| time_elapsed       | 3.95e+03     |\n",
            "| total_timesteps    | 3338240      |\n",
            "| value_loss         | 1.0039347    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3340000, episode_reward=0.11 +/- 0.23\n",
            "Episode length: 290.60 +/- 66.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014734696   |\n",
            "| clipfrac           | 0.09074707    |\n",
            "| ep_len_mean        | 64.7          |\n",
            "| ep_reward_mean     | 0.813         |\n",
            "| explained_variance | 0.391         |\n",
            "| fps                | 572           |\n",
            "| n_updates          | 816           |\n",
            "| policy_entropy     | 0.39930177    |\n",
            "| policy_loss        | -0.0030883611 |\n",
            "| serial_timesteps   | 417792        |\n",
            "| time_elapsed       | 3.96e+03      |\n",
            "| total_timesteps    | 3342336       |\n",
            "| value_loss         | 0.9184561     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01086869   |\n",
            "| clipfrac           | 0.072705075  |\n",
            "| ep_len_mean        | 62.9         |\n",
            "| ep_reward_mean     | 0.818        |\n",
            "| explained_variance | 0.446        |\n",
            "| fps                | 971          |\n",
            "| n_updates          | 817          |\n",
            "| policy_entropy     | 0.36795333   |\n",
            "| policy_loss        | -0.002088628 |\n",
            "| serial_timesteps   | 418304       |\n",
            "| time_elapsed       | 3.96e+03     |\n",
            "| total_timesteps    | 3346432      |\n",
            "| value_loss         | 0.84944266   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3350000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 138.80 +/- 151.22\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016989408  |\n",
            "| clipfrac           | 0.10837402   |\n",
            "| ep_len_mean        | 61.6         |\n",
            "| ep_reward_mean     | 0.821        |\n",
            "| explained_variance | 0.431        |\n",
            "| fps                | 744          |\n",
            "| n_updates          | 818          |\n",
            "| policy_entropy     | 0.3715803    |\n",
            "| policy_loss        | -0.006948142 |\n",
            "| serial_timesteps   | 418816       |\n",
            "| time_elapsed       | 3.97e+03     |\n",
            "| total_timesteps    | 3350528      |\n",
            "| value_loss         | 0.97911054   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.037395447   |\n",
            "| clipfrac           | 0.0675293     |\n",
            "| ep_len_mean        | 61.8          |\n",
            "| ep_reward_mean     | 0.82          |\n",
            "| explained_variance | 0.301         |\n",
            "| fps                | 987           |\n",
            "| n_updates          | 819           |\n",
            "| policy_entropy     | 0.2157367     |\n",
            "| policy_loss        | -0.0010095006 |\n",
            "| serial_timesteps   | 419328        |\n",
            "| time_elapsed       | 3.97e+03      |\n",
            "| total_timesteps    | 3354624       |\n",
            "| value_loss         | 0.80948496    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.018825205   |\n",
            "| clipfrac           | 0.07495117    |\n",
            "| ep_len_mean        | 85.5          |\n",
            "| ep_reward_mean     | 0.747         |\n",
            "| explained_variance | 0.472         |\n",
            "| fps                | 1014          |\n",
            "| n_updates          | 820           |\n",
            "| policy_entropy     | 0.27765858    |\n",
            "| policy_loss        | -0.0016891811 |\n",
            "| serial_timesteps   | 419840        |\n",
            "| time_elapsed       | 3.98e+03      |\n",
            "| total_timesteps    | 3358720       |\n",
            "| value_loss         | 0.7253315     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3360000, episode_reward=0.90 +/- 0.10\n",
            "Episode length: 36.20 +/- 37.02\n",
            "--------------------------------------\n",
            "| approxkl           | 0.019307734   |\n",
            "| clipfrac           | 0.122802734   |\n",
            "| ep_len_mean        | 104           |\n",
            "| ep_reward_mean     | 0.691         |\n",
            "| explained_variance | 0.493         |\n",
            "| fps                | 898           |\n",
            "| n_updates          | 821           |\n",
            "| policy_entropy     | 0.23872551    |\n",
            "| policy_loss        | -0.0066102184 |\n",
            "| serial_timesteps   | 420352        |\n",
            "| time_elapsed       | 3.98e+03      |\n",
            "| total_timesteps    | 3362816       |\n",
            "| value_loss         | 0.8051833     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.061362434  |\n",
            "| clipfrac           | 0.12915039   |\n",
            "| ep_len_mean        | 107          |\n",
            "| ep_reward_mean     | 0.681        |\n",
            "| explained_variance | 0.311        |\n",
            "| fps                | 1003         |\n",
            "| n_updates          | 822          |\n",
            "| policy_entropy     | 0.22347243   |\n",
            "| policy_loss        | -0.008164584 |\n",
            "| serial_timesteps   | 420864       |\n",
            "| time_elapsed       | 3.98e+03     |\n",
            "| total_timesteps    | 3366912      |\n",
            "| value_loss         | 0.88126105   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3370000, episode_reward=0.38 +/- 0.47\n",
            "Episode length: 200.40 +/- 151.38\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012101987   |\n",
            "| clipfrac           | 0.061157227   |\n",
            "| ep_len_mean        | 110           |\n",
            "| ep_reward_mean     | 0.676         |\n",
            "| explained_variance | 0.227         |\n",
            "| fps                | 657           |\n",
            "| n_updates          | 823           |\n",
            "| policy_entropy     | 0.25160798    |\n",
            "| policy_loss        | -0.0012572771 |\n",
            "| serial_timesteps   | 421376        |\n",
            "| time_elapsed       | 3.99e+03      |\n",
            "| total_timesteps    | 3371008       |\n",
            "| value_loss         | 0.8670875     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015428717   |\n",
            "| clipfrac           | 0.079272464   |\n",
            "| ep_len_mean        | 139           |\n",
            "| ep_reward_mean     | 0.584         |\n",
            "| explained_variance | 0.271         |\n",
            "| fps                | 1017          |\n",
            "| n_updates          | 824           |\n",
            "| policy_entropy     | 0.31062534    |\n",
            "| policy_loss        | -0.0027553227 |\n",
            "| serial_timesteps   | 421888        |\n",
            "| time_elapsed       | 4e+03         |\n",
            "| total_timesteps    | 3375104       |\n",
            "| value_loss         | 0.6365541     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.009340319   |\n",
            "| clipfrac           | 0.06875       |\n",
            "| ep_len_mean        | 113           |\n",
            "| ep_reward_mean     | 0.666         |\n",
            "| explained_variance | 0.406         |\n",
            "| fps                | 1009          |\n",
            "| n_updates          | 825           |\n",
            "| policy_entropy     | 0.33379403    |\n",
            "| policy_loss        | -0.0014182578 |\n",
            "| serial_timesteps   | 422400        |\n",
            "| time_elapsed       | 4e+03         |\n",
            "| total_timesteps    | 3379200       |\n",
            "| value_loss         | 0.971863      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3380000, episode_reward=0.56 +/- 0.46\n",
            "Episode length: 143.60 +/- 147.31\n",
            "--------------------------------------\n",
            "| approxkl           | 0.018385533   |\n",
            "| clipfrac           | 0.061181642   |\n",
            "| ep_len_mean        | 72.1          |\n",
            "| ep_reward_mean     | 0.791         |\n",
            "| explained_variance | 0.474         |\n",
            "| fps                | 750           |\n",
            "| n_updates          | 826           |\n",
            "| policy_entropy     | 0.28166837    |\n",
            "| policy_loss        | -0.0035832755 |\n",
            "| serial_timesteps   | 422912        |\n",
            "| time_elapsed       | 4e+03         |\n",
            "| total_timesteps    | 3383296       |\n",
            "| value_loss         | 0.9876874     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01624098   |\n",
            "| clipfrac           | 0.06877442   |\n",
            "| ep_len_mean        | 72.8         |\n",
            "| ep_reward_mean     | 0.788        |\n",
            "| explained_variance | 0.529        |\n",
            "| fps                | 1017         |\n",
            "| n_updates          | 827          |\n",
            "| policy_entropy     | 0.24666068   |\n",
            "| policy_loss        | -0.003522794 |\n",
            "| serial_timesteps   | 423424       |\n",
            "| time_elapsed       | 4.01e+03     |\n",
            "| total_timesteps    | 3387392      |\n",
            "| value_loss         | 0.8557607    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3390000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.60 +/- 150.58\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011904861  |\n",
            "| clipfrac           | 0.07353516   |\n",
            "| ep_len_mean        | 62.6         |\n",
            "| ep_reward_mean     | 0.818        |\n",
            "| explained_variance | 0.545        |\n",
            "| fps                | 731          |\n",
            "| n_updates          | 828          |\n",
            "| policy_entropy     | 0.29261535   |\n",
            "| policy_loss        | -0.003229015 |\n",
            "| serial_timesteps   | 423936       |\n",
            "| time_elapsed       | 4.01e+03     |\n",
            "| total_timesteps    | 3391488      |\n",
            "| value_loss         | 0.9633996    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008611495   |\n",
            "| clipfrac           | 0.057495117   |\n",
            "| ep_len_mean        | 55.1          |\n",
            "| ep_reward_mean     | 0.84          |\n",
            "| explained_variance | 0.507         |\n",
            "| fps                | 1001          |\n",
            "| n_updates          | 829           |\n",
            "| policy_entropy     | 0.28767243    |\n",
            "| policy_loss        | -0.0002655633 |\n",
            "| serial_timesteps   | 424448        |\n",
            "| time_elapsed       | 4.02e+03      |\n",
            "| total_timesteps    | 3395584       |\n",
            "| value_loss         | 1.0891669     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02698582   |\n",
            "| clipfrac           | 0.07600097   |\n",
            "| ep_len_mean        | 73.8         |\n",
            "| ep_reward_mean     | 0.783        |\n",
            "| explained_variance | 0.28         |\n",
            "| fps                | 963          |\n",
            "| n_updates          | 830          |\n",
            "| policy_entropy     | 0.17025623   |\n",
            "| policy_loss        | -0.011021939 |\n",
            "| serial_timesteps   | 424960       |\n",
            "| time_elapsed       | 4.02e+03     |\n",
            "| total_timesteps    | 3399680      |\n",
            "| value_loss         | 0.92321044   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3400000, episode_reward=0.48 +/- 0.43\n",
            "Episode length: 171.00 +/- 138.46\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012614267   |\n",
            "| clipfrac           | 0.08439942    |\n",
            "| ep_len_mean        | 97.4          |\n",
            "| ep_reward_mean     | 0.714         |\n",
            "| explained_variance | 0.345         |\n",
            "| fps                | 715           |\n",
            "| n_updates          | 831           |\n",
            "| policy_entropy     | 0.34193853    |\n",
            "| policy_loss        | -0.0045168153 |\n",
            "| serial_timesteps   | 425472        |\n",
            "| time_elapsed       | 4.03e+03      |\n",
            "| total_timesteps    | 3403776       |\n",
            "| value_loss         | 0.8784858     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013048535  |\n",
            "| clipfrac           | 0.08654785   |\n",
            "| ep_len_mean        | 93           |\n",
            "| ep_reward_mean     | 0.729        |\n",
            "| explained_variance | 0.362        |\n",
            "| fps                | 1011         |\n",
            "| n_updates          | 832          |\n",
            "| policy_entropy     | 0.36001384   |\n",
            "| policy_loss        | -0.002270392 |\n",
            "| serial_timesteps   | 425984       |\n",
            "| time_elapsed       | 4.03e+03     |\n",
            "| total_timesteps    | 3407872      |\n",
            "| value_loss         | 0.8637088    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3410000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 262.00 +/- 124.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.04092538    |\n",
            "| clipfrac           | 0.09309082    |\n",
            "| ep_len_mean        | 107           |\n",
            "| ep_reward_mean     | 0.684         |\n",
            "| explained_variance | -0.219        |\n",
            "| fps                | 607           |\n",
            "| n_updates          | 833           |\n",
            "| policy_entropy     | 0.25526544    |\n",
            "| policy_loss        | -0.0017394973 |\n",
            "| serial_timesteps   | 426496        |\n",
            "| time_elapsed       | 4.04e+03      |\n",
            "| total_timesteps    | 3411968       |\n",
            "| value_loss         | 0.7333222     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.014003662 |\n",
            "| clipfrac           | 0.08364258  |\n",
            "| ep_len_mean        | 134         |\n",
            "| ep_reward_mean     | 0.599       |\n",
            "| explained_variance | 0.00713     |\n",
            "| fps                | 992         |\n",
            "| n_updates          | 834         |\n",
            "| policy_entropy     | 0.2464648   |\n",
            "| policy_loss        | -0.00580871 |\n",
            "| serial_timesteps   | 427008      |\n",
            "| time_elapsed       | 4.04e+03    |\n",
            "| total_timesteps    | 3416064     |\n",
            "| value_loss         | 0.5764811   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3420000, episode_reward=0.38 +/- 0.46\n",
            "Episode length: 203.40 +/- 147.78\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01654758    |\n",
            "| clipfrac           | 0.10637207    |\n",
            "| ep_len_mean        | 160           |\n",
            "| ep_reward_mean     | 0.52          |\n",
            "| explained_variance | -0.0851       |\n",
            "| fps                | 663           |\n",
            "| n_updates          | 835           |\n",
            "| policy_entropy     | 0.3364834     |\n",
            "| policy_loss        | -0.0066673546 |\n",
            "| serial_timesteps   | 427520        |\n",
            "| time_elapsed       | 4.05e+03      |\n",
            "| total_timesteps    | 3420160       |\n",
            "| value_loss         | 0.5302512     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011640552   |\n",
            "| clipfrac           | 0.06359863    |\n",
            "| ep_len_mean        | 178           |\n",
            "| ep_reward_mean     | 0.463         |\n",
            "| explained_variance | 0.425         |\n",
            "| fps                | 1024          |\n",
            "| n_updates          | 836           |\n",
            "| policy_entropy     | 0.38927525    |\n",
            "| policy_loss        | -0.0033363726 |\n",
            "| serial_timesteps   | 428032        |\n",
            "| time_elapsed       | 4.05e+03      |\n",
            "| total_timesteps    | 3424256       |\n",
            "| value_loss         | 0.6487992     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012214855   |\n",
            "| clipfrac           | 0.07600097    |\n",
            "| ep_len_mean        | 79.1          |\n",
            "| ep_reward_mean     | 0.768         |\n",
            "| explained_variance | 0.445         |\n",
            "| fps                | 1007          |\n",
            "| n_updates          | 837           |\n",
            "| policy_entropy     | 0.3442281     |\n",
            "| policy_loss        | -0.0022107395 |\n",
            "| serial_timesteps   | 428544        |\n",
            "| time_elapsed       | 4.06e+03      |\n",
            "| total_timesteps    | 3428352       |\n",
            "| value_loss         | 1.2594793     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3430000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 138.60 +/- 151.38\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018771607  |\n",
            "| clipfrac           | 0.09436035   |\n",
            "| ep_len_mean        | 55           |\n",
            "| ep_reward_mean     | 0.844        |\n",
            "| explained_variance | 0.425        |\n",
            "| fps                | 734          |\n",
            "| n_updates          | 838          |\n",
            "| policy_entropy     | 0.38731298   |\n",
            "| policy_loss        | -0.006809334 |\n",
            "| serial_timesteps   | 429056       |\n",
            "| time_elapsed       | 4.06e+03     |\n",
            "| total_timesteps    | 3432448      |\n",
            "| value_loss         | 1.2024791    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012485075   |\n",
            "| clipfrac           | 0.10397949    |\n",
            "| ep_len_mean        | 47.7          |\n",
            "| ep_reward_mean     | 0.865         |\n",
            "| explained_variance | 0.398         |\n",
            "| fps                | 995           |\n",
            "| n_updates          | 839           |\n",
            "| policy_entropy     | 0.42848       |\n",
            "| policy_loss        | -0.0071480284 |\n",
            "| serial_timesteps   | 429568        |\n",
            "| time_elapsed       | 4.07e+03      |\n",
            "| total_timesteps    | 3436544       |\n",
            "| value_loss         | 1.365108      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3440000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.40 +/- 123.33\n",
            "--------------------------------------\n",
            "| approxkl           | 0.023654459   |\n",
            "| clipfrac           | 0.08774414    |\n",
            "| ep_len_mean        | 36.9          |\n",
            "| ep_reward_mean     | 0.898         |\n",
            "| explained_variance | 0.374         |\n",
            "| fps                | 849           |\n",
            "| n_updates          | 840           |\n",
            "| policy_entropy     | 0.3810001     |\n",
            "| policy_loss        | -0.0012244142 |\n",
            "| serial_timesteps   | 430080        |\n",
            "| time_elapsed       | 4.07e+03      |\n",
            "| total_timesteps    | 3440640       |\n",
            "| value_loss         | 1.4612288     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008474846   |\n",
            "| clipfrac           | 0.07856445    |\n",
            "| ep_len_mean        | 43.7          |\n",
            "| ep_reward_mean     | 0.878         |\n",
            "| explained_variance | 0.389         |\n",
            "| fps                | 1007          |\n",
            "| n_updates          | 841           |\n",
            "| policy_entropy     | 0.33067834    |\n",
            "| policy_loss        | -0.0029191817 |\n",
            "| serial_timesteps   | 430592        |\n",
            "| time_elapsed       | 4.08e+03      |\n",
            "| total_timesteps    | 3444736       |\n",
            "| value_loss         | 1.1154331     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010582759   |\n",
            "| clipfrac           | 0.06447754    |\n",
            "| ep_len_mean        | 52.3          |\n",
            "| ep_reward_mean     | 0.852         |\n",
            "| explained_variance | 0.46          |\n",
            "| fps                | 1011          |\n",
            "| n_updates          | 842           |\n",
            "| policy_entropy     | 0.29774207    |\n",
            "| policy_loss        | -0.0019403497 |\n",
            "| serial_timesteps   | 431104        |\n",
            "| time_elapsed       | 4.08e+03      |\n",
            "| total_timesteps    | 3448832       |\n",
            "| value_loss         | 0.943087      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3450000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 262.20 +/- 123.60\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01955635    |\n",
            "| clipfrac           | 0.084545895   |\n",
            "| ep_len_mean        | 52.9          |\n",
            "| ep_reward_mean     | 0.851         |\n",
            "| explained_variance | 0.435         |\n",
            "| fps                | 607           |\n",
            "| n_updates          | 843           |\n",
            "| policy_entropy     | 0.34525752    |\n",
            "| policy_loss        | -0.0034177017 |\n",
            "| serial_timesteps   | 431616        |\n",
            "| time_elapsed       | 4.08e+03      |\n",
            "| total_timesteps    | 3452928       |\n",
            "| value_loss         | 1.0086713     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.02151997    |\n",
            "| clipfrac           | 0.110522464   |\n",
            "| ep_len_mean        | 43.6          |\n",
            "| ep_reward_mean     | 0.879         |\n",
            "| explained_variance | 0.432         |\n",
            "| fps                | 1011          |\n",
            "| n_updates          | 844           |\n",
            "| policy_entropy     | 0.37746242    |\n",
            "| policy_loss        | -0.0055027343 |\n",
            "| serial_timesteps   | 432128        |\n",
            "| time_elapsed       | 4.09e+03      |\n",
            "| total_timesteps    | 3457024       |\n",
            "| value_loss         | 0.83898723    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3460000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.05466028    |\n",
            "| clipfrac           | 0.13884278    |\n",
            "| ep_len_mean        | 50.2          |\n",
            "| ep_reward_mean     | 0.859         |\n",
            "| explained_variance | 0.488         |\n",
            "| fps                | 548           |\n",
            "| n_updates          | 845           |\n",
            "| policy_entropy     | 0.34980914    |\n",
            "| policy_loss        | -0.0033299099 |\n",
            "| serial_timesteps   | 432640        |\n",
            "| time_elapsed       | 4.09e+03      |\n",
            "| total_timesteps    | 3461120       |\n",
            "| value_loss         | 0.7149929     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.025307218 |\n",
            "| clipfrac           | 0.12036133  |\n",
            "| ep_len_mean        | 60.1        |\n",
            "| ep_reward_mean     | 0.828       |\n",
            "| explained_variance | 0.438       |\n",
            "| fps                | 1012        |\n",
            "| n_updates          | 846         |\n",
            "| policy_entropy     | 0.3512986   |\n",
            "| policy_loss        | -0.0040147  |\n",
            "| serial_timesteps   | 433152      |\n",
            "| time_elapsed       | 4.1e+03     |\n",
            "| total_timesteps    | 3465216     |\n",
            "| value_loss         | 0.91122484  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01285624   |\n",
            "| clipfrac           | 0.06770019   |\n",
            "| ep_len_mean        | 50.2         |\n",
            "| ep_reward_mean     | 0.858        |\n",
            "| explained_variance | 0.43         |\n",
            "| fps                | 1005         |\n",
            "| n_updates          | 847          |\n",
            "| policy_entropy     | 0.25080457   |\n",
            "| policy_loss        | -0.002155932 |\n",
            "| serial_timesteps   | 433664       |\n",
            "| time_elapsed       | 4.11e+03     |\n",
            "| total_timesteps    | 3469312      |\n",
            "| value_loss         | 0.91870725   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3470000, episode_reward=0.38 +/- 0.47\n",
            "Episode length: 201.60 +/- 149.91\n",
            "--------------------------------------\n",
            "| approxkl           | 0.007944151   |\n",
            "| clipfrac           | 0.062329102   |\n",
            "| ep_len_mean        | 64.1          |\n",
            "| ep_reward_mean     | 0.817         |\n",
            "| explained_variance | 0.413         |\n",
            "| fps                | 673           |\n",
            "| n_updates          | 848           |\n",
            "| policy_entropy     | 0.26806667    |\n",
            "| policy_loss        | -0.0018996885 |\n",
            "| serial_timesteps   | 434176        |\n",
            "| time_elapsed       | 4.11e+03      |\n",
            "| total_timesteps    | 3473408       |\n",
            "| value_loss         | 0.77260864    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0179149     |\n",
            "| clipfrac           | 0.096142575   |\n",
            "| ep_len_mean        | 72.3          |\n",
            "| ep_reward_mean     | 0.793         |\n",
            "| explained_variance | 0.413         |\n",
            "| fps                | 1016          |\n",
            "| n_updates          | 849           |\n",
            "| policy_entropy     | 0.2830694     |\n",
            "| policy_loss        | -0.0069707474 |\n",
            "| serial_timesteps   | 434688        |\n",
            "| time_elapsed       | 4.12e+03      |\n",
            "| total_timesteps    | 3477504       |\n",
            "| value_loss         | 0.7757342     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3480000, episode_reward=0.78 +/- 0.34\n",
            "Episode length: 79.80 +/- 121.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012905726   |\n",
            "| clipfrac           | 0.09414063    |\n",
            "| ep_len_mean        | 57.4          |\n",
            "| ep_reward_mean     | 0.839         |\n",
            "| explained_variance | 0.345         |\n",
            "| fps                | 845           |\n",
            "| n_updates          | 850           |\n",
            "| policy_entropy     | 0.2644877     |\n",
            "| policy_loss        | -0.0063431426 |\n",
            "| serial_timesteps   | 435200        |\n",
            "| time_elapsed       | 4.12e+03      |\n",
            "| total_timesteps    | 3481600       |\n",
            "| value_loss         | 0.94190663    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015885899   |\n",
            "| clipfrac           | 0.06962891    |\n",
            "| ep_len_mean        | 52.3          |\n",
            "| ep_reward_mean     | 0.853         |\n",
            "| explained_variance | 0.486         |\n",
            "| fps                | 1017          |\n",
            "| n_updates          | 851           |\n",
            "| policy_entropy     | 0.2608958     |\n",
            "| policy_loss        | -0.0023854647 |\n",
            "| serial_timesteps   | 435712        |\n",
            "| time_elapsed       | 4.13e+03      |\n",
            "| total_timesteps    | 3485696       |\n",
            "| value_loss         | 0.86813104    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014537705   |\n",
            "| clipfrac           | 0.084228516   |\n",
            "| ep_len_mean        | 40.3          |\n",
            "| ep_reward_mean     | 0.887         |\n",
            "| explained_variance | 0.483         |\n",
            "| fps                | 1007          |\n",
            "| n_updates          | 852           |\n",
            "| policy_entropy     | 0.29140958    |\n",
            "| policy_loss        | -0.0034667463 |\n",
            "| serial_timesteps   | 436224        |\n",
            "| time_elapsed       | 4.13e+03      |\n",
            "| total_timesteps    | 3489792       |\n",
            "| value_loss         | 0.92912996    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3490000, episode_reward=0.56 +/- 0.46\n",
            "Episode length: 144.20 +/- 146.95\n",
            "------------------------------------\n",
            "| approxkl           | 0.019816536 |\n",
            "| clipfrac           | 0.084423825 |\n",
            "| ep_len_mean        | 32.3        |\n",
            "| ep_reward_mean     | 0.91        |\n",
            "| explained_variance | 0.408       |\n",
            "| fps                | 727         |\n",
            "| n_updates          | 853         |\n",
            "| policy_entropy     | 0.24328363  |\n",
            "| policy_loss        | 0.009549152 |\n",
            "| serial_timesteps   | 436736      |\n",
            "| time_elapsed       | 4.13e+03    |\n",
            "| total_timesteps    | 3493888     |\n",
            "| value_loss         | 0.9207452   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016845025  |\n",
            "| clipfrac           | 0.095654294  |\n",
            "| ep_len_mean        | 43           |\n",
            "| ep_reward_mean     | 0.878        |\n",
            "| explained_variance | 0.363        |\n",
            "| fps                | 986          |\n",
            "| n_updates          | 854          |\n",
            "| policy_entropy     | 0.3021493    |\n",
            "| policy_loss        | 0.0035467923 |\n",
            "| serial_timesteps   | 437248       |\n",
            "| time_elapsed       | 4.14e+03     |\n",
            "| total_timesteps    | 3497984      |\n",
            "| value_loss         | 0.84650457   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3500000, episode_reward=0.38 +/- 0.47\n",
            "Episode length: 201.60 +/- 149.91\n",
            "------------------------------------\n",
            "| approxkl           | 0.011444027 |\n",
            "| clipfrac           | 0.088208005 |\n",
            "| ep_len_mean        | 43.7        |\n",
            "| ep_reward_mean     | 0.878       |\n",
            "| explained_variance | 0.371       |\n",
            "| fps                | 666         |\n",
            "| n_updates          | 855         |\n",
            "| policy_entropy     | 0.32080734  |\n",
            "| policy_loss        | 0.002201498 |\n",
            "| serial_timesteps   | 437760      |\n",
            "| time_elapsed       | 4.14e+03    |\n",
            "| total_timesteps    | 3502080     |\n",
            "| value_loss         | 0.7815557   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.038995873  |\n",
            "| clipfrac           | 0.07956543   |\n",
            "| ep_len_mean        | 49.8         |\n",
            "| ep_reward_mean     | 0.859        |\n",
            "| explained_variance | 0.496        |\n",
            "| fps                | 999          |\n",
            "| n_updates          | 856          |\n",
            "| policy_entropy     | 0.20392592   |\n",
            "| policy_loss        | -0.003414262 |\n",
            "| serial_timesteps   | 438272       |\n",
            "| time_elapsed       | 4.15e+03     |\n",
            "| total_timesteps    | 3506176      |\n",
            "| value_loss         | 0.6717162    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3510000, episode_reward=0.58 +/- 0.47\n",
            "Episode length: 138.00 +/- 151.87\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014810453  |\n",
            "| clipfrac           | 0.061572265  |\n",
            "| ep_len_mean        | 63.9         |\n",
            "| ep_reward_mean     | 0.815        |\n",
            "| explained_variance | 0.518        |\n",
            "| fps                | 739          |\n",
            "| n_updates          | 857          |\n",
            "| policy_entropy     | 0.17605284   |\n",
            "| policy_loss        | -0.005929788 |\n",
            "| serial_timesteps   | 438784       |\n",
            "| time_elapsed       | 4.15e+03     |\n",
            "| total_timesteps    | 3510272      |\n",
            "| value_loss         | 0.8332655    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.013674416   |\n",
            "| clipfrac           | 0.09990235    |\n",
            "| ep_len_mean        | 52.5          |\n",
            "| ep_reward_mean     | 0.851         |\n",
            "| explained_variance | 0.211         |\n",
            "| fps                | 989           |\n",
            "| n_updates          | 858           |\n",
            "| policy_entropy     | 0.30594316    |\n",
            "| policy_loss        | -0.0015492471 |\n",
            "| serial_timesteps   | 439296        |\n",
            "| time_elapsed       | 4.16e+03      |\n",
            "| total_timesteps    | 3514368       |\n",
            "| value_loss         | 0.82944936    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022072552  |\n",
            "| clipfrac           | 0.086035155  |\n",
            "| ep_len_mean        | 42.8         |\n",
            "| ep_reward_mean     | 0.88         |\n",
            "| explained_variance | 0.432        |\n",
            "| fps                | 1019         |\n",
            "| n_updates          | 859          |\n",
            "| policy_entropy     | 0.27220985   |\n",
            "| policy_loss        | -0.002090837 |\n",
            "| serial_timesteps   | 439808       |\n",
            "| time_elapsed       | 4.16e+03     |\n",
            "| total_timesteps    | 3518464      |\n",
            "| value_loss         | 0.65219533   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3520000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.40 +/- 150.74\n",
            "--------------------------------------\n",
            "| approxkl           | 0.017063776   |\n",
            "| clipfrac           | 0.07583008    |\n",
            "| ep_len_mean        | 57.9          |\n",
            "| ep_reward_mean     | 0.834         |\n",
            "| explained_variance | 0.365         |\n",
            "| fps                | 749           |\n",
            "| n_updates          | 860           |\n",
            "| policy_entropy     | 0.29725385    |\n",
            "| policy_loss        | 0.00055717037 |\n",
            "| serial_timesteps   | 440320        |\n",
            "| time_elapsed       | 4.17e+03      |\n",
            "| total_timesteps    | 3522560       |\n",
            "| value_loss         | 0.75902903    |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.06578343  |\n",
            "| clipfrac           | 0.17038575  |\n",
            "| ep_len_mean        | 50.4        |\n",
            "| ep_reward_mean     | 0.856       |\n",
            "| explained_variance | 0.42        |\n",
            "| fps                | 1008        |\n",
            "| n_updates          | 861         |\n",
            "| policy_entropy     | 0.252616    |\n",
            "| policy_loss        | -0.01882757 |\n",
            "| serial_timesteps   | 440832      |\n",
            "| time_elapsed       | 4.17e+03    |\n",
            "| total_timesteps    | 3526656     |\n",
            "| value_loss         | 0.7728604   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3530000, episode_reward=0.61 +/- 0.43\n",
            "Episode length: 134.00 +/- 145.54\n",
            "--------------------------------------\n",
            "| approxkl           | 0.02791367    |\n",
            "| clipfrac           | 0.09868164    |\n",
            "| ep_len_mean        | 55.7          |\n",
            "| ep_reward_mean     | 0.84          |\n",
            "| explained_variance | 0.28          |\n",
            "| fps                | 762           |\n",
            "| n_updates          | 862           |\n",
            "| policy_entropy     | 0.2386766     |\n",
            "| policy_loss        | -0.0003280355 |\n",
            "| serial_timesteps   | 441344        |\n",
            "| time_elapsed       | 4.18e+03      |\n",
            "| total_timesteps    | 3530752       |\n",
            "| value_loss         | 0.8575295     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.023498524   |\n",
            "| clipfrac           | 0.08310547    |\n",
            "| ep_len_mean        | 39.3          |\n",
            "| ep_reward_mean     | 0.891         |\n",
            "| explained_variance | 0.475         |\n",
            "| fps                | 1009          |\n",
            "| n_updates          | 863           |\n",
            "| policy_entropy     | 0.2534673     |\n",
            "| policy_loss        | -0.0033122245 |\n",
            "| serial_timesteps   | 441856        |\n",
            "| time_elapsed       | 4.18e+03      |\n",
            "| total_timesteps    | 3534848       |\n",
            "| value_loss         | 0.6159235     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.030530464   |\n",
            "| clipfrac           | 0.093774416   |\n",
            "| ep_len_mean        | 33.2          |\n",
            "| ep_reward_mean     | 0.908         |\n",
            "| explained_variance | 0.475         |\n",
            "| fps                | 1021          |\n",
            "| n_updates          | 864           |\n",
            "| policy_entropy     | 0.2666383     |\n",
            "| policy_loss        | -0.0037079095 |\n",
            "| serial_timesteps   | 442368        |\n",
            "| time_elapsed       | 4.19e+03      |\n",
            "| total_timesteps    | 3538944       |\n",
            "| value_loss         | 0.59151936    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3540000, episode_reward=0.75 +/- 0.38\n",
            "Episode length: 83.20 +/- 121.09\n",
            "--------------------------------------\n",
            "| approxkl           | 0.035028998   |\n",
            "| clipfrac           | 0.101586916   |\n",
            "| ep_len_mean        | 44.2          |\n",
            "| ep_reward_mean     | 0.876         |\n",
            "| explained_variance | 0.479         |\n",
            "| fps                | 844           |\n",
            "| n_updates          | 865           |\n",
            "| policy_entropy     | 0.25646636    |\n",
            "| policy_loss        | -0.0035469248 |\n",
            "| serial_timesteps   | 442880        |\n",
            "| time_elapsed       | 4.19e+03      |\n",
            "| total_timesteps    | 3543040       |\n",
            "| value_loss         | 0.50052893    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.046294488  |\n",
            "| clipfrac           | 0.10100098   |\n",
            "| ep_len_mean        | 34.9         |\n",
            "| ep_reward_mean     | 0.903        |\n",
            "| explained_variance | 0.405        |\n",
            "| fps                | 1007         |\n",
            "| n_updates          | 866          |\n",
            "| policy_entropy     | 0.23489818   |\n",
            "| policy_loss        | -0.008114213 |\n",
            "| serial_timesteps   | 443392       |\n",
            "| time_elapsed       | 4.2e+03      |\n",
            "| total_timesteps    | 3547136      |\n",
            "| value_loss         | 0.5320321    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3550000, episode_reward=0.90 +/- 0.09\n",
            "Episode length: 35.00 +/- 34.11\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0219884    |\n",
            "| clipfrac           | 0.09238281   |\n",
            "| ep_len_mean        | 33.7         |\n",
            "| ep_reward_mean     | 0.906        |\n",
            "| explained_variance | 0.512        |\n",
            "| fps                | 910          |\n",
            "| n_updates          | 867          |\n",
            "| policy_entropy     | 0.26372716   |\n",
            "| policy_loss        | -0.002362849 |\n",
            "| serial_timesteps   | 443904       |\n",
            "| time_elapsed       | 4.2e+03      |\n",
            "| total_timesteps    | 3551232      |\n",
            "| value_loss         | 0.44178963   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009073464  |\n",
            "| clipfrac           | 0.062451173  |\n",
            "| ep_len_mean        | 35.6         |\n",
            "| ep_reward_mean     | 0.901        |\n",
            "| explained_variance | 0.457        |\n",
            "| fps                | 988          |\n",
            "| n_updates          | 868          |\n",
            "| policy_entropy     | 0.1974584    |\n",
            "| policy_loss        | -0.003477214 |\n",
            "| serial_timesteps   | 444416       |\n",
            "| time_elapsed       | 4.2e+03      |\n",
            "| total_timesteps    | 3555328      |\n",
            "| value_loss         | 0.48481894   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023938615  |\n",
            "| clipfrac           | 0.09624024   |\n",
            "| ep_len_mean        | 36.4         |\n",
            "| ep_reward_mean     | 0.897        |\n",
            "| explained_variance | 0.45         |\n",
            "| fps                | 1014         |\n",
            "| n_updates          | 869          |\n",
            "| policy_entropy     | 0.24596798   |\n",
            "| policy_loss        | -0.003968649 |\n",
            "| serial_timesteps   | 444928       |\n",
            "| time_elapsed       | 4.21e+03     |\n",
            "| total_timesteps    | 3559424      |\n",
            "| value_loss         | 0.5904285    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3560000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 77.20 +/- 123.41\n",
            "--------------------------------------\n",
            "| approxkl           | 0.018408298   |\n",
            "| clipfrac           | 0.07495117    |\n",
            "| ep_len_mean        | 34.2          |\n",
            "| ep_reward_mean     | 0.905         |\n",
            "| explained_variance | 0.482         |\n",
            "| fps                | 853           |\n",
            "| n_updates          | 870           |\n",
            "| policy_entropy     | 0.21417668    |\n",
            "| policy_loss        | -0.0018391259 |\n",
            "| serial_timesteps   | 445440        |\n",
            "| time_elapsed       | 4.21e+03      |\n",
            "| total_timesteps    | 3563520       |\n",
            "| value_loss         | 0.46109337    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.020001339   |\n",
            "| clipfrac           | 0.07248535    |\n",
            "| ep_len_mean        | 34.3          |\n",
            "| ep_reward_mean     | 0.905         |\n",
            "| explained_variance | 0.479         |\n",
            "| fps                | 985           |\n",
            "| n_updates          | 871           |\n",
            "| policy_entropy     | 0.20318937    |\n",
            "| policy_loss        | 0.00063683384 |\n",
            "| serial_timesteps   | 445952        |\n",
            "| time_elapsed       | 4.22e+03      |\n",
            "| total_timesteps    | 3567616       |\n",
            "| value_loss         | 0.46538132    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3570000, episode_reward=0.75 +/- 0.38\n",
            "Episode length: 82.00 +/- 121.21\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01720908    |\n",
            "| clipfrac           | 0.08774414    |\n",
            "| ep_len_mean        | 35.8          |\n",
            "| ep_reward_mean     | 0.898         |\n",
            "| explained_variance | 0.506         |\n",
            "| fps                | 815           |\n",
            "| n_updates          | 872           |\n",
            "| policy_entropy     | 0.23695846    |\n",
            "| policy_loss        | -0.0065900185 |\n",
            "| serial_timesteps   | 446464        |\n",
            "| time_elapsed       | 4.22e+03      |\n",
            "| total_timesteps    | 3571712       |\n",
            "| value_loss         | 0.52459747    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012229144   |\n",
            "| clipfrac           | 0.06325684    |\n",
            "| ep_len_mean        | 50.8          |\n",
            "| ep_reward_mean     | 0.854         |\n",
            "| explained_variance | 0.387         |\n",
            "| fps                | 985           |\n",
            "| n_updates          | 873           |\n",
            "| policy_entropy     | 0.23407733    |\n",
            "| policy_loss        | -0.0015345721 |\n",
            "| serial_timesteps   | 446976        |\n",
            "| time_elapsed       | 4.23e+03      |\n",
            "| total_timesteps    | 3575808       |\n",
            "| value_loss         | 0.7547342     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015425337   |\n",
            "| clipfrac           | 0.073632814   |\n",
            "| ep_len_mean        | 42.7          |\n",
            "| ep_reward_mean     | 0.88          |\n",
            "| explained_variance | 0.381         |\n",
            "| fps                | 987           |\n",
            "| n_updates          | 874           |\n",
            "| policy_entropy     | 0.1882007     |\n",
            "| policy_loss        | -0.0036947876 |\n",
            "| serial_timesteps   | 447488        |\n",
            "| time_elapsed       | 4.23e+03      |\n",
            "| total_timesteps    | 3579904       |\n",
            "| value_loss         | 0.56488746    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3580000, episode_reward=0.57 +/- 0.46\n",
            "Episode length: 141.60 +/- 148.96\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012194691   |\n",
            "| clipfrac           | 0.074438475   |\n",
            "| ep_len_mean        | 42            |\n",
            "| ep_reward_mean     | 0.883         |\n",
            "| explained_variance | 0.413         |\n",
            "| fps                | 736           |\n",
            "| n_updates          | 875           |\n",
            "| policy_entropy     | 0.2012945     |\n",
            "| policy_loss        | -0.0035851896 |\n",
            "| serial_timesteps   | 448000        |\n",
            "| time_elapsed       | 4.23e+03      |\n",
            "| total_timesteps    | 3584000       |\n",
            "| value_loss         | 0.5062367     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.024130194 |\n",
            "| clipfrac           | 0.13486329  |\n",
            "| ep_len_mean        | 52.5        |\n",
            "| ep_reward_mean     | 0.853       |\n",
            "| explained_variance | 0.368       |\n",
            "| fps                | 1003        |\n",
            "| n_updates          | 876         |\n",
            "| policy_entropy     | 0.3041286   |\n",
            "| policy_loss        | 0.053613376 |\n",
            "| serial_timesteps   | 448512      |\n",
            "| time_elapsed       | 4.24e+03    |\n",
            "| total_timesteps    | 3588096     |\n",
            "| value_loss         | 0.6268838   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3590000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 140.20 +/- 150.08\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016967172  |\n",
            "| clipfrac           | 0.063647464  |\n",
            "| ep_len_mean        | 49           |\n",
            "| ep_reward_mean     | 0.863        |\n",
            "| explained_variance | 0.387        |\n",
            "| fps                | 742          |\n",
            "| n_updates          | 877          |\n",
            "| policy_entropy     | 0.26973206   |\n",
            "| policy_loss        | -0.002949014 |\n",
            "| serial_timesteps   | 449024       |\n",
            "| time_elapsed       | 4.24e+03     |\n",
            "| total_timesteps    | 3592192      |\n",
            "| value_loss         | 0.605955     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.019682871   |\n",
            "| clipfrac           | 0.07297363    |\n",
            "| ep_len_mean        | 41.3          |\n",
            "| ep_reward_mean     | 0.885         |\n",
            "| explained_variance | 0.43          |\n",
            "| fps                | 1014          |\n",
            "| n_updates          | 878           |\n",
            "| policy_entropy     | 0.19283244    |\n",
            "| policy_loss        | -0.0010497568 |\n",
            "| serial_timesteps   | 449536        |\n",
            "| time_elapsed       | 4.25e+03      |\n",
            "| total_timesteps    | 3596288       |\n",
            "| value_loss         | 0.49449787    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3600000, episode_reward=0.57 +/- 0.46\n",
            "Episode length: 140.80 +/- 149.65\n",
            "-------------------------------------\n",
            "| approxkl           | 0.053024102  |\n",
            "| clipfrac           | 0.082373045  |\n",
            "| ep_len_mean        | 28.1         |\n",
            "| ep_reward_mean     | 0.922        |\n",
            "| explained_variance | 0.529        |\n",
            "| fps                | 745          |\n",
            "| n_updates          | 879          |\n",
            "| policy_entropy     | 0.16837046   |\n",
            "| policy_loss        | -0.008720148 |\n",
            "| serial_timesteps   | 450048       |\n",
            "| time_elapsed       | 4.25e+03     |\n",
            "| total_timesteps    | 3600384      |\n",
            "| value_loss         | 0.422923     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.017480234  |\n",
            "| clipfrac           | 0.09919433   |\n",
            "| ep_len_mean        | 27.8         |\n",
            "| ep_reward_mean     | 0.923        |\n",
            "| explained_variance | 0.499        |\n",
            "| fps                | 1014         |\n",
            "| n_updates          | 880          |\n",
            "| policy_entropy     | 0.17975417   |\n",
            "| policy_loss        | -0.010404134 |\n",
            "| serial_timesteps   | 450560       |\n",
            "| time_elapsed       | 4.26e+03     |\n",
            "| total_timesteps    | 3604480      |\n",
            "| value_loss         | 0.41175646   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0119582545 |\n",
            "| clipfrac           | 0.06833496   |\n",
            "| ep_len_mean        | 30.4         |\n",
            "| ep_reward_mean     | 0.916        |\n",
            "| explained_variance | 0.521        |\n",
            "| fps                | 1005         |\n",
            "| n_updates          | 881          |\n",
            "| policy_entropy     | 0.16030481   |\n",
            "| policy_loss        | -0.006783156 |\n",
            "| serial_timesteps   | 451072       |\n",
            "| time_elapsed       | 4.26e+03     |\n",
            "| total_timesteps    | 3608576      |\n",
            "| value_loss         | 0.36134118   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3610000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.00 +/- 151.06\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015887795  |\n",
            "| clipfrac           | 0.078222655  |\n",
            "| ep_len_mean        | 34.3         |\n",
            "| ep_reward_mean     | 0.905        |\n",
            "| explained_variance | 0.507        |\n",
            "| fps                | 714          |\n",
            "| n_updates          | 882          |\n",
            "| policy_entropy     | 0.17903528   |\n",
            "| policy_loss        | -0.003050695 |\n",
            "| serial_timesteps   | 451584       |\n",
            "| time_elapsed       | 4.27e+03     |\n",
            "| total_timesteps    | 3612672      |\n",
            "| value_loss         | 0.3306189    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.017003773   |\n",
            "| clipfrac           | 0.0836914     |\n",
            "| ep_len_mean        | 33.5          |\n",
            "| ep_reward_mean     | 0.907         |\n",
            "| explained_variance | 0.52          |\n",
            "| fps                | 1005          |\n",
            "| n_updates          | 883           |\n",
            "| policy_entropy     | 0.19416502    |\n",
            "| policy_loss        | -0.0011428792 |\n",
            "| serial_timesteps   | 452096        |\n",
            "| time_elapsed       | 4.27e+03      |\n",
            "| total_timesteps    | 3616768       |\n",
            "| value_loss         | 0.3441454     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3620000, episode_reward=0.75 +/- 0.37\n",
            "Episode length: 83.60 +/- 120.34\n",
            "--------------------------------------\n",
            "| approxkl           | 0.017710475   |\n",
            "| clipfrac           | 0.07143555    |\n",
            "| ep_len_mean        | 26.5          |\n",
            "| ep_reward_mean     | 0.926         |\n",
            "| explained_variance | 0.484         |\n",
            "| fps                | 810           |\n",
            "| n_updates          | 884           |\n",
            "| policy_entropy     | 0.16677824    |\n",
            "| policy_loss        | 0.00053650606 |\n",
            "| serial_timesteps   | 452608        |\n",
            "| time_elapsed       | 4.28e+03      |\n",
            "| total_timesteps    | 3620864       |\n",
            "| value_loss         | 0.34356076    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.025426377   |\n",
            "| clipfrac           | 0.072265625   |\n",
            "| ep_len_mean        | 33.4          |\n",
            "| ep_reward_mean     | 0.907         |\n",
            "| explained_variance | 0.552         |\n",
            "| fps                | 993           |\n",
            "| n_updates          | 885           |\n",
            "| policy_entropy     | 0.16664608    |\n",
            "| policy_loss        | -0.0032541049 |\n",
            "| serial_timesteps   | 453120        |\n",
            "| time_elapsed       | 4.28e+03      |\n",
            "| total_timesteps    | 3624960       |\n",
            "| value_loss         | 0.30712578    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.02295537    |\n",
            "| clipfrac           | 0.082250975   |\n",
            "| ep_len_mean        | 27.4          |\n",
            "| ep_reward_mean     | 0.924         |\n",
            "| explained_variance | 0.462         |\n",
            "| fps                | 1009          |\n",
            "| n_updates          | 886           |\n",
            "| policy_entropy     | 0.1789363     |\n",
            "| policy_loss        | -0.0040217033 |\n",
            "| serial_timesteps   | 453632        |\n",
            "| time_elapsed       | 4.29e+03      |\n",
            "| total_timesteps    | 3629056       |\n",
            "| value_loss         | 0.3292566     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3630000, episode_reward=0.93 +/- 0.02\n",
            "Episode length: 24.20 +/- 8.38\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012748596   |\n",
            "| clipfrac           | 0.062402345   |\n",
            "| ep_len_mean        | 25.6          |\n",
            "| ep_reward_mean     | 0.929         |\n",
            "| explained_variance | 0.491         |\n",
            "| fps                | 955           |\n",
            "| n_updates          | 887           |\n",
            "| policy_entropy     | 0.14455509    |\n",
            "| policy_loss        | -0.0018134393 |\n",
            "| serial_timesteps   | 454144        |\n",
            "| time_elapsed       | 4.29e+03      |\n",
            "| total_timesteps    | 3633152       |\n",
            "| value_loss         | 0.29131994    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024146792  |\n",
            "| clipfrac           | 0.084179685  |\n",
            "| ep_len_mean        | 24.8         |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.512        |\n",
            "| fps                | 995          |\n",
            "| n_updates          | 888          |\n",
            "| policy_entropy     | 0.16192916   |\n",
            "| policy_loss        | -0.004027345 |\n",
            "| serial_timesteps   | 454656       |\n",
            "| time_elapsed       | 4.29e+03     |\n",
            "| total_timesteps    | 3637248      |\n",
            "| value_loss         | 0.2347173    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3640000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.80 +/- 4.26\n",
            "--------------------------------------\n",
            "| approxkl           | 0.031465337   |\n",
            "| clipfrac           | 0.06032715    |\n",
            "| ep_len_mean        | 23.9          |\n",
            "| ep_reward_mean     | 0.934         |\n",
            "| explained_variance | 0.536         |\n",
            "| fps                | 952           |\n",
            "| n_updates          | 889           |\n",
            "| policy_entropy     | 0.14516081    |\n",
            "| policy_loss        | -0.0042280536 |\n",
            "| serial_timesteps   | 455168        |\n",
            "| time_elapsed       | 4.3e+03       |\n",
            "| total_timesteps    | 3641344       |\n",
            "| value_loss         | 0.22293606    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018896203  |\n",
            "| clipfrac           | 0.08452149   |\n",
            "| ep_len_mean        | 28.4         |\n",
            "| ep_reward_mean     | 0.921        |\n",
            "| explained_variance | 0.522        |\n",
            "| fps                | 944          |\n",
            "| n_updates          | 890          |\n",
            "| policy_entropy     | 0.20144124   |\n",
            "| policy_loss        | -0.001129259 |\n",
            "| serial_timesteps   | 455680       |\n",
            "| time_elapsed       | 4.3e+03      |\n",
            "| total_timesteps    | 3645440      |\n",
            "| value_loss         | 0.31022018   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012183453   |\n",
            "| clipfrac           | 0.06125488    |\n",
            "| ep_len_mean        | 33.8          |\n",
            "| ep_reward_mean     | 0.906         |\n",
            "| explained_variance | 0.503         |\n",
            "| fps                | 996           |\n",
            "| n_updates          | 891           |\n",
            "| policy_entropy     | 0.1719375     |\n",
            "| policy_loss        | -0.0005718178 |\n",
            "| serial_timesteps   | 456192        |\n",
            "| time_elapsed       | 4.31e+03      |\n",
            "| total_timesteps    | 3649536       |\n",
            "| value_loss         | 0.3458589     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3650000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.40 +/- 3.44\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014800778  |\n",
            "| clipfrac           | 0.066430666  |\n",
            "| ep_len_mean        | 29.8         |\n",
            "| ep_reward_mean     | 0.917        |\n",
            "| explained_variance | 0.527        |\n",
            "| fps                | 945          |\n",
            "| n_updates          | 892          |\n",
            "| policy_entropy     | 0.1877212    |\n",
            "| policy_loss        | 0.0011897144 |\n",
            "| serial_timesteps   | 456704       |\n",
            "| time_elapsed       | 4.31e+03     |\n",
            "| total_timesteps    | 3653632      |\n",
            "| value_loss         | 0.328275     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.020402122    |\n",
            "| clipfrac           | 0.057275392    |\n",
            "| ep_len_mean        | 23.8           |\n",
            "| ep_reward_mean     | 0.934          |\n",
            "| explained_variance | 0.565          |\n",
            "| fps                | 992            |\n",
            "| n_updates          | 893            |\n",
            "| policy_entropy     | 0.14730771     |\n",
            "| policy_loss        | -0.00047193834 |\n",
            "| serial_timesteps   | 457216         |\n",
            "| time_elapsed       | 4.32e+03       |\n",
            "| total_timesteps    | 3657728        |\n",
            "| value_loss         | 0.21534626     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=3660000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.60 +/- 2.87\n",
            "--------------------------------------\n",
            "| approxkl           | 0.031211179   |\n",
            "| clipfrac           | 0.091308594   |\n",
            "| ep_len_mean        | 26.3          |\n",
            "| ep_reward_mean     | 0.927         |\n",
            "| explained_variance | 0.63          |\n",
            "| fps                | 941           |\n",
            "| n_updates          | 894           |\n",
            "| policy_entropy     | 0.18287352    |\n",
            "| policy_loss        | -0.0026922084 |\n",
            "| serial_timesteps   | 457728        |\n",
            "| time_elapsed       | 4.32e+03      |\n",
            "| total_timesteps    | 3661824       |\n",
            "| value_loss         | 0.25020805    |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.009004698 |\n",
            "| clipfrac           | 0.048754882 |\n",
            "| ep_len_mean        | 25.9        |\n",
            "| ep_reward_mean     | 0.928       |\n",
            "| explained_variance | 0.541       |\n",
            "| fps                | 1011        |\n",
            "| n_updates          | 895         |\n",
            "| policy_entropy     | 0.17036599  |\n",
            "| policy_loss        | 0.000816801 |\n",
            "| serial_timesteps   | 458240      |\n",
            "| time_elapsed       | 4.32e+03    |\n",
            "| total_timesteps    | 3665920     |\n",
            "| value_loss         | 0.23858967  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3670000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.60 +/- 122.26\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02328014   |\n",
            "| clipfrac           | 0.109521486  |\n",
            "| ep_len_mean        | 26.8         |\n",
            "| ep_reward_mean     | 0.926        |\n",
            "| explained_variance | 0.492        |\n",
            "| fps                | 839          |\n",
            "| n_updates          | 896          |\n",
            "| policy_entropy     | 0.19801784   |\n",
            "| policy_loss        | -0.008833302 |\n",
            "| serial_timesteps   | 458752       |\n",
            "| time_elapsed       | 4.33e+03     |\n",
            "| total_timesteps    | 3670016      |\n",
            "| value_loss         | 0.24166206   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014616676   |\n",
            "| clipfrac           | 0.072827145   |\n",
            "| ep_len_mean        | 30.1          |\n",
            "| ep_reward_mean     | 0.916         |\n",
            "| explained_variance | 0.449         |\n",
            "| fps                | 1014          |\n",
            "| n_updates          | 897           |\n",
            "| policy_entropy     | 0.19172825    |\n",
            "| policy_loss        | -0.0042649573 |\n",
            "| serial_timesteps   | 459264        |\n",
            "| time_elapsed       | 4.33e+03      |\n",
            "| total_timesteps    | 3674112       |\n",
            "| value_loss         | 0.23877272    |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.048175655 |\n",
            "| clipfrac           | 0.13010255  |\n",
            "| ep_len_mean        | 38.4        |\n",
            "| ep_reward_mean     | 0.892       |\n",
            "| explained_variance | 0.362       |\n",
            "| fps                | 999         |\n",
            "| n_updates          | 898         |\n",
            "| policy_entropy     | 0.2811221   |\n",
            "| policy_loss        | 0.062471174 |\n",
            "| serial_timesteps   | 459776      |\n",
            "| time_elapsed       | 4.34e+03    |\n",
            "| total_timesteps    | 3678208     |\n",
            "| value_loss         | 0.43045145  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3680000, episode_reward=0.93 +/- 0.01\n",
            "Episode length: 24.20 +/- 4.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011249195   |\n",
            "| clipfrac           | 0.05595703    |\n",
            "| ep_len_mean        | 33.2          |\n",
            "| ep_reward_mean     | 0.907         |\n",
            "| explained_variance | 0.465         |\n",
            "| fps                | 938           |\n",
            "| n_updates          | 899           |\n",
            "| policy_entropy     | 0.1728628     |\n",
            "| policy_loss        | -0.0029054743 |\n",
            "| serial_timesteps   | 460288        |\n",
            "| time_elapsed       | 4.34e+03      |\n",
            "| total_timesteps    | 3682304       |\n",
            "| value_loss         | 0.387753      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.022573425    |\n",
            "| clipfrac           | 0.08400879     |\n",
            "| ep_len_mean        | 37.2           |\n",
            "| ep_reward_mean     | 0.895          |\n",
            "| explained_variance | 0.454          |\n",
            "| fps                | 1011           |\n",
            "| n_updates          | 900            |\n",
            "| policy_entropy     | 0.30007252     |\n",
            "| policy_loss        | -0.00096509233 |\n",
            "| serial_timesteps   | 460800         |\n",
            "| time_elapsed       | 4.35e+03       |\n",
            "| total_timesteps    | 3686400        |\n",
            "| value_loss         | 0.48546463     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=3690000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.80 +/- 2.64\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015637696   |\n",
            "| clipfrac           | 0.06499024    |\n",
            "| ep_len_mean        | 27.7          |\n",
            "| ep_reward_mean     | 0.923         |\n",
            "| explained_variance | 0.555         |\n",
            "| fps                | 968           |\n",
            "| n_updates          | 901           |\n",
            "| policy_entropy     | 0.1683981     |\n",
            "| policy_loss        | -0.0060803816 |\n",
            "| serial_timesteps   | 461312        |\n",
            "| time_elapsed       | 4.35e+03      |\n",
            "| total_timesteps    | 3690496       |\n",
            "| value_loss         | 0.326598      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011278324  |\n",
            "| clipfrac           | 0.058129884  |\n",
            "| ep_len_mean        | 28.5         |\n",
            "| ep_reward_mean     | 0.921        |\n",
            "| explained_variance | 0.59         |\n",
            "| fps                | 998          |\n",
            "| n_updates          | 902          |\n",
            "| policy_entropy     | 0.13983746   |\n",
            "| policy_loss        | -0.005199066 |\n",
            "| serial_timesteps   | 461824       |\n",
            "| time_elapsed       | 4.35e+03     |\n",
            "| total_timesteps    | 3694592      |\n",
            "| value_loss         | 0.30631477   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008771121   |\n",
            "| clipfrac           | 0.03640137    |\n",
            "| ep_len_mean        | 23.5          |\n",
            "| ep_reward_mean     | 0.935         |\n",
            "| explained_variance | 0.506         |\n",
            "| fps                | 988           |\n",
            "| n_updates          | 903           |\n",
            "| policy_entropy     | 0.11244325    |\n",
            "| policy_loss        | -0.0008374827 |\n",
            "| serial_timesteps   | 462336        |\n",
            "| time_elapsed       | 4.36e+03      |\n",
            "| total_timesteps    | 3698688       |\n",
            "| value_loss         | 0.22761872    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3700000, episode_reward=0.94 +/- 0.02\n",
            "Episode length: 22.00 +/- 5.51\n",
            "------------------------------------\n",
            "| approxkl           | 0.010507507 |\n",
            "| clipfrac           | 0.040771484 |\n",
            "| ep_len_mean        | 30.7        |\n",
            "| ep_reward_mean     | 0.915       |\n",
            "| explained_variance | 0.591       |\n",
            "| fps                | 907         |\n",
            "| n_updates          | 904         |\n",
            "| policy_entropy     | 0.113664985 |\n",
            "| policy_loss        | 0.003872895 |\n",
            "| serial_timesteps   | 462848      |\n",
            "| time_elapsed       | 4.36e+03    |\n",
            "| total_timesteps    | 3702784     |\n",
            "| value_loss         | 0.34140936  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011593883  |\n",
            "| clipfrac           | 0.04194336   |\n",
            "| ep_len_mean        | 26           |\n",
            "| ep_reward_mean     | 0.928        |\n",
            "| explained_variance | 0.582        |\n",
            "| fps                | 999          |\n",
            "| n_updates          | 905          |\n",
            "| policy_entropy     | 0.13658914   |\n",
            "| policy_loss        | 0.0014656145 |\n",
            "| serial_timesteps   | 463360       |\n",
            "| time_elapsed       | 4.37e+03     |\n",
            "| total_timesteps    | 3706880      |\n",
            "| value_loss         | 0.2564623    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3710000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 77.20 +/- 123.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011809047   |\n",
            "| clipfrac           | 0.04790039    |\n",
            "| ep_len_mean        | 23.1          |\n",
            "| ep_reward_mean     | 0.936         |\n",
            "| explained_variance | 0.483         |\n",
            "| fps                | 845           |\n",
            "| n_updates          | 906           |\n",
            "| policy_entropy     | 0.115301624   |\n",
            "| policy_loss        | -0.0027204012 |\n",
            "| serial_timesteps   | 463872        |\n",
            "| time_elapsed       | 4.37e+03      |\n",
            "| total_timesteps    | 3710976       |\n",
            "| value_loss         | 0.24515805    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014039224   |\n",
            "| clipfrac           | 0.06086426    |\n",
            "| ep_len_mean        | 24.8          |\n",
            "| ep_reward_mean     | 0.931         |\n",
            "| explained_variance | 0.445         |\n",
            "| fps                | 1000          |\n",
            "| n_updates          | 907           |\n",
            "| policy_entropy     | 0.11508248    |\n",
            "| policy_loss        | -0.0075803623 |\n",
            "| serial_timesteps   | 464384        |\n",
            "| time_elapsed       | 4.38e+03      |\n",
            "| total_timesteps    | 3715072       |\n",
            "| value_loss         | 0.33645478    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019646073  |\n",
            "| clipfrac           | 0.06486817   |\n",
            "| ep_len_mean        | 26.5         |\n",
            "| ep_reward_mean     | 0.924        |\n",
            "| explained_variance | 0.362        |\n",
            "| fps                | 1013         |\n",
            "| n_updates          | 908          |\n",
            "| policy_entropy     | 0.155961     |\n",
            "| policy_loss        | 0.0013549357 |\n",
            "| serial_timesteps   | 464896       |\n",
            "| time_elapsed       | 4.38e+03     |\n",
            "| total_timesteps    | 3719168      |\n",
            "| value_loss         | 0.41872668   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3720000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 140.00 +/- 150.24\n",
            "-------------------------------------\n",
            "| approxkl           | 0.007168072  |\n",
            "| clipfrac           | 0.025170898  |\n",
            "| ep_len_mean        | 26.4         |\n",
            "| ep_reward_mean     | 0.927        |\n",
            "| explained_variance | 0.5          |\n",
            "| fps                | 739          |\n",
            "| n_updates          | 909          |\n",
            "| policy_entropy     | 0.097892396  |\n",
            "| policy_loss        | 0.0014237144 |\n",
            "| serial_timesteps   | 465408       |\n",
            "| time_elapsed       | 4.38e+03     |\n",
            "| total_timesteps    | 3723264      |\n",
            "| value_loss         | 0.26810578   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.07966478    |\n",
            "| clipfrac           | 0.07729492    |\n",
            "| ep_len_mean        | 29.4          |\n",
            "| ep_reward_mean     | 0.916         |\n",
            "| explained_variance | 0.514         |\n",
            "| fps                | 998           |\n",
            "| n_updates          | 910           |\n",
            "| policy_entropy     | 0.17740126    |\n",
            "| policy_loss        | -0.0047353506 |\n",
            "| serial_timesteps   | 465920        |\n",
            "| time_elapsed       | 4.39e+03      |\n",
            "| total_timesteps    | 3727360       |\n",
            "| value_loss         | 0.47252148    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3730000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 18.00 +/- 2.76\n",
            "--------------------------------------\n",
            "| approxkl           | 0.022839297   |\n",
            "| clipfrac           | 0.041308593   |\n",
            "| ep_len_mean        | 23.4          |\n",
            "| ep_reward_mean     | 0.935         |\n",
            "| explained_variance | 0.503         |\n",
            "| fps                | 948           |\n",
            "| n_updates          | 911           |\n",
            "| policy_entropy     | 0.1166964     |\n",
            "| policy_loss        | 0.00051864923 |\n",
            "| serial_timesteps   | 466432        |\n",
            "| time_elapsed       | 4.39e+03      |\n",
            "| total_timesteps    | 3731456       |\n",
            "| value_loss         | 0.4172097     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.046493713 |\n",
            "| clipfrac           | 0.10871582  |\n",
            "| ep_len_mean        | 29.2        |\n",
            "| ep_reward_mean     | 0.918       |\n",
            "| explained_variance | 0.411       |\n",
            "| fps                | 1004        |\n",
            "| n_updates          | 912         |\n",
            "| policy_entropy     | 0.1598714   |\n",
            "| policy_loss        | 0.029749062 |\n",
            "| serial_timesteps   | 466944      |\n",
            "| time_elapsed       | 4.4e+03     |\n",
            "| total_timesteps    | 3735552     |\n",
            "| value_loss         | 0.36496836  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.008992044 |\n",
            "| clipfrac           | 0.04482422  |\n",
            "| ep_len_mean        | 22.4        |\n",
            "| ep_reward_mean     | 0.938       |\n",
            "| explained_variance | 0.404       |\n",
            "| fps                | 996         |\n",
            "| n_updates          | 913         |\n",
            "| policy_entropy     | 0.1101926   |\n",
            "| policy_loss        | 0.009213081 |\n",
            "| serial_timesteps   | 467456      |\n",
            "| time_elapsed       | 4.4e+03     |\n",
            "| total_timesteps    | 3739648     |\n",
            "| value_loss         | 0.24162197  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3740000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.80 +/- 122.13\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01951663    |\n",
            "| clipfrac           | 0.06022949    |\n",
            "| ep_len_mean        | 25.5          |\n",
            "| ep_reward_mean     | 0.929         |\n",
            "| explained_variance | 0.589         |\n",
            "| fps                | 849           |\n",
            "| n_updates          | 914           |\n",
            "| policy_entropy     | 0.16338512    |\n",
            "| policy_loss        | -0.0055957027 |\n",
            "| serial_timesteps   | 467968        |\n",
            "| time_elapsed       | 4.41e+03      |\n",
            "| total_timesteps    | 3743744       |\n",
            "| value_loss         | 0.30173522    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.017085312  |\n",
            "| clipfrac           | 0.066186525  |\n",
            "| ep_len_mean        | 28           |\n",
            "| ep_reward_mean     | 0.921        |\n",
            "| explained_variance | 0.492        |\n",
            "| fps                | 1024         |\n",
            "| n_updates          | 915          |\n",
            "| policy_entropy     | 0.14975321   |\n",
            "| policy_loss        | -0.002563654 |\n",
            "| serial_timesteps   | 468480       |\n",
            "| time_elapsed       | 4.41e+03     |\n",
            "| total_timesteps    | 3747840      |\n",
            "| value_loss         | 0.4463767    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3750000, episode_reward=0.94 +/- 0.02\n",
            "Episode length: 19.80 +/- 5.42\n",
            "--------------------------------------\n",
            "| approxkl           | 0.009320533   |\n",
            "| clipfrac           | 0.038305663   |\n",
            "| ep_len_mean        | 25.3          |\n",
            "| ep_reward_mean     | 0.93          |\n",
            "| explained_variance | 0.528         |\n",
            "| fps                | 951           |\n",
            "| n_updates          | 916           |\n",
            "| policy_entropy     | 0.11977074    |\n",
            "| policy_loss        | -0.0013284361 |\n",
            "| serial_timesteps   | 468992        |\n",
            "| time_elapsed       | 4.41e+03      |\n",
            "| total_timesteps    | 3751936       |\n",
            "| value_loss         | 0.3002294     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.012978491  |\n",
            "| clipfrac           | 0.06796875   |\n",
            "| ep_len_mean        | 28.3         |\n",
            "| ep_reward_mean     | 0.92         |\n",
            "| explained_variance | 0.517        |\n",
            "| fps                | 1002         |\n",
            "| n_updates          | 917          |\n",
            "| policy_entropy     | 0.14403428   |\n",
            "| policy_loss        | 0.0014420606 |\n",
            "| serial_timesteps   | 469504       |\n",
            "| time_elapsed       | 4.42e+03     |\n",
            "| total_timesteps    | 3756032      |\n",
            "| value_loss         | 0.32222295   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3760000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.60 +/- 3.01\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008158755  |\n",
            "| clipfrac           | 0.04472656   |\n",
            "| ep_len_mean        | 30.4         |\n",
            "| ep_reward_mean     | 0.916        |\n",
            "| explained_variance | 0.495        |\n",
            "| fps                | 937          |\n",
            "| n_updates          | 918          |\n",
            "| policy_entropy     | 0.14428511   |\n",
            "| policy_loss        | -0.004125454 |\n",
            "| serial_timesteps   | 470016       |\n",
            "| time_elapsed       | 4.42e+03     |\n",
            "| total_timesteps    | 3760128      |\n",
            "| value_loss         | 0.28763783   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.009316817   |\n",
            "| clipfrac           | 0.058007814   |\n",
            "| ep_len_mean        | 23.8          |\n",
            "| ep_reward_mean     | 0.934         |\n",
            "| explained_variance | 0.553         |\n",
            "| fps                | 1010          |\n",
            "| n_updates          | 919           |\n",
            "| policy_entropy     | 0.14996928    |\n",
            "| policy_loss        | -0.0055971565 |\n",
            "| serial_timesteps   | 470528        |\n",
            "| time_elapsed       | 4.43e+03      |\n",
            "| total_timesteps    | 3764224       |\n",
            "| value_loss         | 0.21704721    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012969549   |\n",
            "| clipfrac           | 0.0659668     |\n",
            "| ep_len_mean        | 27.1          |\n",
            "| ep_reward_mean     | 0.925         |\n",
            "| explained_variance | 0.537         |\n",
            "| fps                | 990           |\n",
            "| n_updates          | 920           |\n",
            "| policy_entropy     | 0.15587601    |\n",
            "| policy_loss        | -0.0047763484 |\n",
            "| serial_timesteps   | 471040        |\n",
            "| time_elapsed       | 4.43e+03      |\n",
            "| total_timesteps    | 3768320       |\n",
            "| value_loss         | 0.22252794    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3770000, episode_reward=0.57 +/- 0.46\n",
            "Episode length: 141.60 +/- 148.96\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011761572   |\n",
            "| clipfrac           | 0.055419922   |\n",
            "| ep_len_mean        | 26.1          |\n",
            "| ep_reward_mean     | 0.928         |\n",
            "| explained_variance | 0.566         |\n",
            "| fps                | 734           |\n",
            "| n_updates          | 921           |\n",
            "| policy_entropy     | 0.12889211    |\n",
            "| policy_loss        | -0.0045608827 |\n",
            "| serial_timesteps   | 471552        |\n",
            "| time_elapsed       | 4.44e+03      |\n",
            "| total_timesteps    | 3772416       |\n",
            "| value_loss         | 0.21062715    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012617774   |\n",
            "| clipfrac           | 0.04802246    |\n",
            "| ep_len_mean        | 23.1          |\n",
            "| ep_reward_mean     | 0.936         |\n",
            "| explained_variance | 0.598         |\n",
            "| fps                | 1010          |\n",
            "| n_updates          | 922           |\n",
            "| policy_entropy     | 0.10489871    |\n",
            "| policy_loss        | -0.0049572503 |\n",
            "| serial_timesteps   | 472064        |\n",
            "| time_elapsed       | 4.44e+03      |\n",
            "| total_timesteps    | 3776512       |\n",
            "| value_loss         | 0.16590966    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3780000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 79.00 +/- 122.51\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011118727   |\n",
            "| clipfrac           | 0.05683594    |\n",
            "| ep_len_mean        | 25.3          |\n",
            "| ep_reward_mean     | 0.93          |\n",
            "| explained_variance | 0.532         |\n",
            "| fps                | 845           |\n",
            "| n_updates          | 923           |\n",
            "| policy_entropy     | 0.13469169    |\n",
            "| policy_loss        | 0.00045493673 |\n",
            "| serial_timesteps   | 472576        |\n",
            "| time_elapsed       | 4.44e+03      |\n",
            "| total_timesteps    | 3780608       |\n",
            "| value_loss         | 0.22804633    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0081511205  |\n",
            "| clipfrac           | 0.047143556   |\n",
            "| ep_len_mean        | 21.7          |\n",
            "| ep_reward_mean     | 0.94          |\n",
            "| explained_variance | 0.597         |\n",
            "| fps                | 1020          |\n",
            "| n_updates          | 924           |\n",
            "| policy_entropy     | 0.099456266   |\n",
            "| policy_loss        | -0.0021386421 |\n",
            "| serial_timesteps   | 473088        |\n",
            "| time_elapsed       | 4.45e+03      |\n",
            "| total_timesteps    | 3784704       |\n",
            "| value_loss         | 0.1310585     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019480359  |\n",
            "| clipfrac           | 0.06933594   |\n",
            "| ep_len_mean        | 25.4         |\n",
            "| ep_reward_mean     | 0.929        |\n",
            "| explained_variance | 0.549        |\n",
            "| fps                | 1006         |\n",
            "| n_updates          | 925          |\n",
            "| policy_entropy     | 0.13273442   |\n",
            "| policy_loss        | -0.007018529 |\n",
            "| serial_timesteps   | 473600       |\n",
            "| time_elapsed       | 4.45e+03     |\n",
            "| total_timesteps    | 3788800      |\n",
            "| value_loss         | 0.16067573   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3790000, episode_reward=0.75 +/- 0.37\n",
            "Episode length: 83.40 +/- 120.51\n",
            "---------------------------------------\n",
            "| approxkl           | 0.009092713    |\n",
            "| clipfrac           | 0.056811523    |\n",
            "| ep_len_mean        | 22.7           |\n",
            "| ep_reward_mean     | 0.937          |\n",
            "| explained_variance | 0.579          |\n",
            "| fps                | 834            |\n",
            "| n_updates          | 926            |\n",
            "| policy_entropy     | 0.12427342     |\n",
            "| policy_loss        | -7.5103014e-07 |\n",
            "| serial_timesteps   | 474112         |\n",
            "| time_elapsed       | 4.46e+03       |\n",
            "| total_timesteps    | 3792896        |\n",
            "| value_loss         | 0.1428403      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01325311   |\n",
            "| clipfrac           | 0.059033204  |\n",
            "| ep_len_mean        | 23.7         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.602        |\n",
            "| fps                | 997          |\n",
            "| n_updates          | 927          |\n",
            "| policy_entropy     | 0.11786439   |\n",
            "| policy_loss        | -0.003753156 |\n",
            "| serial_timesteps   | 474624       |\n",
            "| time_elapsed       | 4.46e+03     |\n",
            "| total_timesteps    | 3796992      |\n",
            "| value_loss         | 0.15714203   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3800000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 17.20 +/- 1.47\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016668463  |\n",
            "| clipfrac           | 0.060668945  |\n",
            "| ep_len_mean        | 22.6         |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.577        |\n",
            "| fps                | 927          |\n",
            "| n_updates          | 928          |\n",
            "| policy_entropy     | 0.12601383   |\n",
            "| policy_loss        | -0.003898499 |\n",
            "| serial_timesteps   | 475136       |\n",
            "| time_elapsed       | 4.47e+03     |\n",
            "| total_timesteps    | 3801088      |\n",
            "| value_loss         | 0.15038067   |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.012977896    |\n",
            "| clipfrac           | 0.049951173    |\n",
            "| ep_len_mean        | 31.3           |\n",
            "| ep_reward_mean     | 0.911          |\n",
            "| explained_variance | 0.531          |\n",
            "| fps                | 1010           |\n",
            "| n_updates          | 929            |\n",
            "| policy_entropy     | 0.13064715     |\n",
            "| policy_loss        | -0.00014862088 |\n",
            "| serial_timesteps   | 475648         |\n",
            "| time_elapsed       | 4.47e+03       |\n",
            "| total_timesteps    | 3805184        |\n",
            "| value_loss         | 0.42794418     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0071368976  |\n",
            "| clipfrac           | 0.04909668    |\n",
            "| ep_len_mean        | 33.1          |\n",
            "| ep_reward_mean     | 0.906         |\n",
            "| explained_variance | 0.509         |\n",
            "| fps                | 1007          |\n",
            "| n_updates          | 930           |\n",
            "| policy_entropy     | 0.15163198    |\n",
            "| policy_loss        | -0.0012166544 |\n",
            "| serial_timesteps   | 476160        |\n",
            "| time_elapsed       | 4.48e+03      |\n",
            "| total_timesteps    | 3809280       |\n",
            "| value_loss         | 0.46639752    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3810000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 140.20 +/- 150.08\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01609575   |\n",
            "| clipfrac           | 0.08588867   |\n",
            "| ep_len_mean        | 34.3         |\n",
            "| ep_reward_mean     | 0.903        |\n",
            "| explained_variance | 0.514        |\n",
            "| fps                | 742          |\n",
            "| n_updates          | 931          |\n",
            "| policy_entropy     | 0.16513869   |\n",
            "| policy_loss        | -0.007134655 |\n",
            "| serial_timesteps   | 476672       |\n",
            "| time_elapsed       | 4.48e+03     |\n",
            "| total_timesteps    | 3813376      |\n",
            "| value_loss         | 0.4358119    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0062122047   |\n",
            "| clipfrac           | 0.04135742     |\n",
            "| ep_len_mean        | 26             |\n",
            "| ep_reward_mean     | 0.928          |\n",
            "| explained_variance | 0.591          |\n",
            "| fps                | 994            |\n",
            "| n_updates          | 932            |\n",
            "| policy_entropy     | 0.12096194     |\n",
            "| policy_loss        | -0.00066198024 |\n",
            "| serial_timesteps   | 477184         |\n",
            "| time_elapsed       | 4.48e+03       |\n",
            "| total_timesteps    | 3817472        |\n",
            "| value_loss         | 0.32409832     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=3820000, episode_reward=0.94 +/- 0.01\n",
            "Episode length: 20.00 +/- 3.69\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012973027   |\n",
            "| clipfrac           | 0.041064452   |\n",
            "| ep_len_mean        | 23.8          |\n",
            "| ep_reward_mean     | 0.934         |\n",
            "| explained_variance | 0.528         |\n",
            "| fps                | 934           |\n",
            "| n_updates          | 933           |\n",
            "| policy_entropy     | 0.09242108    |\n",
            "| policy_loss        | -0.0030362164 |\n",
            "| serial_timesteps   | 477696        |\n",
            "| time_elapsed       | 4.49e+03      |\n",
            "| total_timesteps    | 3821568       |\n",
            "| value_loss         | 0.26747426    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018359587  |\n",
            "| clipfrac           | 0.041430663  |\n",
            "| ep_len_mean        | 22.5         |\n",
            "| ep_reward_mean     | 0.938        |\n",
            "| explained_variance | 0.59         |\n",
            "| fps                | 993          |\n",
            "| n_updates          | 934          |\n",
            "| policy_entropy     | 0.10040361   |\n",
            "| policy_loss        | -0.003482724 |\n",
            "| serial_timesteps   | 478208       |\n",
            "| time_elapsed       | 4.49e+03     |\n",
            "| total_timesteps    | 3825664      |\n",
            "| value_loss         | 0.23241727   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010918906   |\n",
            "| clipfrac           | 0.03330078    |\n",
            "| ep_len_mean        | 21.6          |\n",
            "| ep_reward_mean     | 0.94          |\n",
            "| explained_variance | 0.575         |\n",
            "| fps                | 1001          |\n",
            "| n_updates          | 935           |\n",
            "| policy_entropy     | 0.109320246   |\n",
            "| policy_loss        | -0.0020596832 |\n",
            "| serial_timesteps   | 478720        |\n",
            "| time_elapsed       | 4.5e+03       |\n",
            "| total_timesteps    | 3829760       |\n",
            "| value_loss         | 0.1958759     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3830000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 16.40 +/- 0.80\n",
            "-------------------------------------\n",
            "| approxkl           | 0.012952614  |\n",
            "| clipfrac           | 0.0421875    |\n",
            "| ep_len_mean        | 22.8         |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.594        |\n",
            "| fps                | 960          |\n",
            "| n_updates          | 936          |\n",
            "| policy_entropy     | 0.099641755  |\n",
            "| policy_loss        | -0.002569145 |\n",
            "| serial_timesteps   | 479232       |\n",
            "| time_elapsed       | 4.5e+03      |\n",
            "| total_timesteps    | 3833856      |\n",
            "| value_loss         | 0.16022627   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.05910232   |\n",
            "| clipfrac           | 0.0909668    |\n",
            "| ep_len_mean        | 36.7         |\n",
            "| ep_reward_mean     | 0.897        |\n",
            "| explained_variance | 0.345        |\n",
            "| fps                | 998          |\n",
            "| n_updates          | 937          |\n",
            "| policy_entropy     | 0.15224843   |\n",
            "| policy_loss        | 0.0040781763 |\n",
            "| serial_timesteps   | 479744       |\n",
            "| time_elapsed       | 4.51e+03     |\n",
            "| total_timesteps    | 3837952      |\n",
            "| value_loss         | 0.39975357   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3840000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 19.00 +/- 3.41\n",
            "--------------------------------------\n",
            "| approxkl           | 0.025513675   |\n",
            "| clipfrac           | 0.071704105   |\n",
            "| ep_len_mean        | 25.4          |\n",
            "| ep_reward_mean     | 0.929         |\n",
            "| explained_variance | 0.522         |\n",
            "| fps                | 973           |\n",
            "| n_updates          | 938           |\n",
            "| policy_entropy     | 0.13452452    |\n",
            "| policy_loss        | -0.0051192306 |\n",
            "| serial_timesteps   | 480256        |\n",
            "| time_elapsed       | 4.51e+03      |\n",
            "| total_timesteps    | 3842048       |\n",
            "| value_loss         | 0.20702234    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0121062305  |\n",
            "| clipfrac           | 0.065014645   |\n",
            "| ep_len_mean        | 23.9          |\n",
            "| ep_reward_mean     | 0.934         |\n",
            "| explained_variance | 0.577         |\n",
            "| fps                | 983           |\n",
            "| n_updates          | 939           |\n",
            "| policy_entropy     | 0.1437943     |\n",
            "| policy_loss        | -0.0024981578 |\n",
            "| serial_timesteps   | 480768        |\n",
            "| time_elapsed       | 4.51e+03      |\n",
            "| total_timesteps    | 3846144       |\n",
            "| value_loss         | 0.1725314     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3850000, episode_reward=0.94 +/- 0.03\n",
            "Episode length: 23.00 +/- 9.03\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0141649395 |\n",
            "| clipfrac           | 0.077905275  |\n",
            "| ep_len_mean        | 26.1         |\n",
            "| ep_reward_mean     | 0.927        |\n",
            "| explained_variance | 0.492        |\n",
            "| fps                | 929          |\n",
            "| n_updates          | 940          |\n",
            "| policy_entropy     | 0.14969508   |\n",
            "| policy_loss        | -0.006727914 |\n",
            "| serial_timesteps   | 481280       |\n",
            "| time_elapsed       | 4.52e+03     |\n",
            "| total_timesteps    | 3850240      |\n",
            "| value_loss         | 0.19649805   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011219194   |\n",
            "| clipfrac           | 0.06906738    |\n",
            "| ep_len_mean        | 34.9          |\n",
            "| ep_reward_mean     | 0.902         |\n",
            "| explained_variance | 0.425         |\n",
            "| fps                | 1009          |\n",
            "| n_updates          | 941           |\n",
            "| policy_entropy     | 0.1630744     |\n",
            "| policy_loss        | -0.0037953425 |\n",
            "| serial_timesteps   | 481792        |\n",
            "| time_elapsed       | 4.52e+03      |\n",
            "| total_timesteps    | 3854336       |\n",
            "| value_loss         | 0.38291305    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0070882724  |\n",
            "| clipfrac           | 0.040112305   |\n",
            "| ep_len_mean        | 23.9          |\n",
            "| ep_reward_mean     | 0.934         |\n",
            "| explained_variance | 0.569         |\n",
            "| fps                | 997           |\n",
            "| n_updates          | 942           |\n",
            "| policy_entropy     | 0.10332588    |\n",
            "| policy_loss        | 0.00018025801 |\n",
            "| serial_timesteps   | 482304        |\n",
            "| time_elapsed       | 4.53e+03      |\n",
            "| total_timesteps    | 3858432       |\n",
            "| value_loss         | 0.17643495    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3860000, episode_reward=0.75 +/- 0.38\n",
            "Episode length: 82.40 +/- 120.87\n",
            "--------------------------------------\n",
            "| approxkl           | 0.015259802   |\n",
            "| clipfrac           | 0.05546875    |\n",
            "| ep_len_mean        | 29.4          |\n",
            "| ep_reward_mean     | 0.917         |\n",
            "| explained_variance | 0.547         |\n",
            "| fps                | 839           |\n",
            "| n_updates          | 943           |\n",
            "| policy_entropy     | 0.13514352    |\n",
            "| policy_loss        | -0.0026915749 |\n",
            "| serial_timesteps   | 482816        |\n",
            "| time_elapsed       | 4.53e+03      |\n",
            "| total_timesteps    | 3862528       |\n",
            "| value_loss         | 0.3048985     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010631805   |\n",
            "| clipfrac           | 0.0390625     |\n",
            "| ep_len_mean        | 22.4          |\n",
            "| ep_reward_mean     | 0.938         |\n",
            "| explained_variance | 0.52          |\n",
            "| fps                | 1010          |\n",
            "| n_updates          | 944           |\n",
            "| policy_entropy     | 0.08442257    |\n",
            "| policy_loss        | -0.0012696503 |\n",
            "| serial_timesteps   | 483328        |\n",
            "| time_elapsed       | 4.54e+03      |\n",
            "| total_timesteps    | 3866624       |\n",
            "| value_loss         | 0.16208048    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3870000, episode_reward=0.38 +/- 0.47\n",
            "Episode length: 200.80 +/- 150.89\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011426001  |\n",
            "| clipfrac           | 0.048046876  |\n",
            "| ep_len_mean        | 23.4         |\n",
            "| ep_reward_mean     | 0.935        |\n",
            "| explained_variance | 0.583        |\n",
            "| fps                | 675          |\n",
            "| n_updates          | 945          |\n",
            "| policy_entropy     | 0.10267937   |\n",
            "| policy_loss        | 0.0002552608 |\n",
            "| serial_timesteps   | 483840       |\n",
            "| time_elapsed       | 4.54e+03     |\n",
            "| total_timesteps    | 3870720      |\n",
            "| value_loss         | 0.17505926   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.014131129   |\n",
            "| clipfrac           | 0.047924805   |\n",
            "| ep_len_mean        | 23.9          |\n",
            "| ep_reward_mean     | 0.934         |\n",
            "| explained_variance | 0.622         |\n",
            "| fps                | 978           |\n",
            "| n_updates          | 946           |\n",
            "| policy_entropy     | 0.1054608     |\n",
            "| policy_loss        | -0.0017251751 |\n",
            "| serial_timesteps   | 484352        |\n",
            "| time_elapsed       | 4.55e+03      |\n",
            "| total_timesteps    | 3874816       |\n",
            "| value_loss         | 0.16474697    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0063882186 |\n",
            "| clipfrac           | 0.03161621   |\n",
            "| ep_len_mean        | 24.4         |\n",
            "| ep_reward_mean     | 0.932        |\n",
            "| explained_variance | 0.612        |\n",
            "| fps                | 997          |\n",
            "| n_updates          | 947          |\n",
            "| policy_entropy     | 0.09942258   |\n",
            "| policy_loss        | 0.0044957506 |\n",
            "| serial_timesteps   | 484864       |\n",
            "| time_elapsed       | 4.55e+03     |\n",
            "| total_timesteps    | 3878912      |\n",
            "| value_loss         | 0.170077     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3880000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.40 +/- 122.81\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018365793  |\n",
            "| clipfrac           | 0.05632324   |\n",
            "| ep_len_mean        | 23.1         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.592        |\n",
            "| fps                | 835          |\n",
            "| n_updates          | 948          |\n",
            "| policy_entropy     | 0.11080875   |\n",
            "| policy_loss        | -0.007776321 |\n",
            "| serial_timesteps   | 485376       |\n",
            "| time_elapsed       | 4.55e+03     |\n",
            "| total_timesteps    | 3883008      |\n",
            "| value_loss         | 0.17817363   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008792374   |\n",
            "| clipfrac           | 0.04230957    |\n",
            "| ep_len_mean        | 22.7          |\n",
            "| ep_reward_mean     | 0.937         |\n",
            "| explained_variance | 0.531         |\n",
            "| fps                | 990           |\n",
            "| n_updates          | 949           |\n",
            "| policy_entropy     | 0.10130286    |\n",
            "| policy_loss        | -0.0014426957 |\n",
            "| serial_timesteps   | 485888        |\n",
            "| time_elapsed       | 4.56e+03      |\n",
            "| total_timesteps    | 3887104       |\n",
            "| value_loss         | 0.16856422    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3890000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 140.40 +/- 149.92\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0073549612  |\n",
            "| clipfrac           | 0.038549803   |\n",
            "| ep_len_mean        | 23.4          |\n",
            "| ep_reward_mean     | 0.935         |\n",
            "| explained_variance | 0.613         |\n",
            "| fps                | 730           |\n",
            "| n_updates          | 950           |\n",
            "| policy_entropy     | 0.0976186     |\n",
            "| policy_loss        | -0.0022991563 |\n",
            "| serial_timesteps   | 486400        |\n",
            "| time_elapsed       | 4.56e+03      |\n",
            "| total_timesteps    | 3891200       |\n",
            "| value_loss         | 0.15654151    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012673835   |\n",
            "| clipfrac           | 0.047851562   |\n",
            "| ep_len_mean        | 27.2          |\n",
            "| ep_reward_mean     | 0.924         |\n",
            "| explained_variance | 0.598         |\n",
            "| fps                | 995           |\n",
            "| n_updates          | 951           |\n",
            "| policy_entropy     | 0.11571076    |\n",
            "| policy_loss        | -0.0034534521 |\n",
            "| serial_timesteps   | 486912        |\n",
            "| time_elapsed       | 4.57e+03      |\n",
            "| total_timesteps    | 3895296       |\n",
            "| value_loss         | 0.22855397    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009691731  |\n",
            "| clipfrac           | 0.037036132  |\n",
            "| ep_len_mean        | 24.8         |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.552        |\n",
            "| fps                | 1020         |\n",
            "| n_updates          | 952          |\n",
            "| policy_entropy     | 0.07972278   |\n",
            "| policy_loss        | -0.002013099 |\n",
            "| serial_timesteps   | 487424       |\n",
            "| time_elapsed       | 4.57e+03     |\n",
            "| total_timesteps    | 3899392      |\n",
            "| value_loss         | 0.18730912   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3900000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.00 +/- 123.05\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0058494196  |\n",
            "| clipfrac           | 0.028100586   |\n",
            "| ep_len_mean        | 23.6          |\n",
            "| ep_reward_mean     | 0.935         |\n",
            "| explained_variance | 0.561         |\n",
            "| fps                | 827           |\n",
            "| n_updates          | 953           |\n",
            "| policy_entropy     | 0.07061259    |\n",
            "| policy_loss        | -0.0022507473 |\n",
            "| serial_timesteps   | 487936        |\n",
            "| time_elapsed       | 4.58e+03      |\n",
            "| total_timesteps    | 3903488       |\n",
            "| value_loss         | 0.1638786     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005669136   |\n",
            "| clipfrac           | 0.029199218   |\n",
            "| ep_len_mean        | 22.9          |\n",
            "| ep_reward_mean     | 0.936         |\n",
            "| explained_variance | 0.585         |\n",
            "| fps                | 971           |\n",
            "| n_updates          | 954           |\n",
            "| policy_entropy     | 0.08057554    |\n",
            "| policy_loss        | -0.0035853472 |\n",
            "| serial_timesteps   | 488448        |\n",
            "| time_elapsed       | 4.58e+03      |\n",
            "| total_timesteps    | 3907584       |\n",
            "| value_loss         | 0.15879712    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3910000, episode_reward=0.94 +/- 0.01\n",
            "Episode length: 21.80 +/- 4.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.03291048    |\n",
            "| clipfrac           | 0.048754882   |\n",
            "| ep_len_mean        | 22.9          |\n",
            "| ep_reward_mean     | 0.936         |\n",
            "| explained_variance | 0.583         |\n",
            "| fps                | 937           |\n",
            "| n_updates          | 955           |\n",
            "| policy_entropy     | 0.09598963    |\n",
            "| policy_loss        | -0.0028624106 |\n",
            "| serial_timesteps   | 488960        |\n",
            "| time_elapsed       | 4.59e+03      |\n",
            "| total_timesteps    | 3911680       |\n",
            "| value_loss         | 0.113980494   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.18433455   |\n",
            "| clipfrac           | 0.10783692   |\n",
            "| ep_len_mean        | 29.2         |\n",
            "| ep_reward_mean     | 0.918        |\n",
            "| explained_variance | 0.412        |\n",
            "| fps                | 994          |\n",
            "| n_updates          | 956          |\n",
            "| policy_entropy     | 0.13809246   |\n",
            "| policy_loss        | -0.021945268 |\n",
            "| serial_timesteps   | 489472       |\n",
            "| time_elapsed       | 4.59e+03     |\n",
            "| total_timesteps    | 3915776      |\n",
            "| value_loss         | 0.3053476    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012242952   |\n",
            "| clipfrac           | 0.049243163   |\n",
            "| ep_len_mean        | 23.7          |\n",
            "| ep_reward_mean     | 0.934         |\n",
            "| explained_variance | 0.474         |\n",
            "| fps                | 1008          |\n",
            "| n_updates          | 957           |\n",
            "| policy_entropy     | 0.12552887    |\n",
            "| policy_loss        | -0.0036388435 |\n",
            "| serial_timesteps   | 489984        |\n",
            "| time_elapsed       | 4.59e+03      |\n",
            "| total_timesteps    | 3919872       |\n",
            "| value_loss         | 0.21488622    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3920000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.80 +/- 150.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009757342  |\n",
            "| clipfrac           | 0.035668947  |\n",
            "| ep_len_mean        | 18.8         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.602        |\n",
            "| fps                | 741          |\n",
            "| n_updates          | 958          |\n",
            "| policy_entropy     | 0.08858773   |\n",
            "| policy_loss        | 0.0002796167 |\n",
            "| serial_timesteps   | 490496       |\n",
            "| time_elapsed       | 4.6e+03      |\n",
            "| total_timesteps    | 3923968      |\n",
            "| value_loss         | 0.098474845  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.07705427  |\n",
            "| clipfrac           | 0.095214844 |\n",
            "| ep_len_mean        | 31.7        |\n",
            "| ep_reward_mean     | 0.91        |\n",
            "| explained_variance | 0.335       |\n",
            "| fps                | 1021        |\n",
            "| n_updates          | 959         |\n",
            "| policy_entropy     | 0.21615429  |\n",
            "| policy_loss        | 0.019698977 |\n",
            "| serial_timesteps   | 491008      |\n",
            "| time_elapsed       | 4.6e+03     |\n",
            "| total_timesteps    | 3928064     |\n",
            "| value_loss         | 0.4120081   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3930000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.20 +/- 2.64\n",
            "--------------------------------------\n",
            "| approxkl           | 0.044419557   |\n",
            "| clipfrac           | 0.07780762    |\n",
            "| ep_len_mean        | 28.4          |\n",
            "| ep_reward_mean     | 0.921         |\n",
            "| explained_variance | 0.492         |\n",
            "| fps                | 965           |\n",
            "| n_updates          | 960           |\n",
            "| policy_entropy     | 0.16448864    |\n",
            "| policy_loss        | 0.00048741285 |\n",
            "| serial_timesteps   | 491520        |\n",
            "| time_elapsed       | 4.61e+03      |\n",
            "| total_timesteps    | 3932160       |\n",
            "| value_loss         | 0.35809222    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.13231654   |\n",
            "| clipfrac           | 0.119702145  |\n",
            "| ep_len_mean        | 33.9         |\n",
            "| ep_reward_mean     | 0.905        |\n",
            "| explained_variance | 0.456        |\n",
            "| fps                | 992          |\n",
            "| n_updates          | 961          |\n",
            "| policy_entropy     | 0.19545023   |\n",
            "| policy_loss        | -0.009584641 |\n",
            "| serial_timesteps   | 492032       |\n",
            "| time_elapsed       | 4.61e+03     |\n",
            "| total_timesteps    | 3936256      |\n",
            "| value_loss         | 0.30408612   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3940000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.80 +/- 123.61\n",
            "---------------------------------------\n",
            "| approxkl           | 0.026057657    |\n",
            "| clipfrac           | 0.0668457      |\n",
            "| ep_len_mean        | 29.1           |\n",
            "| ep_reward_mean     | 0.918          |\n",
            "| explained_variance | 0.574          |\n",
            "| fps                | 819            |\n",
            "| n_updates          | 962            |\n",
            "| policy_entropy     | 0.15826623     |\n",
            "| policy_loss        | -0.00047342904 |\n",
            "| serial_timesteps   | 492544         |\n",
            "| time_elapsed       | 4.62e+03       |\n",
            "| total_timesteps    | 3940352        |\n",
            "| value_loss         | 0.28432757     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.012858276    |\n",
            "| clipfrac           | 0.053808592    |\n",
            "| ep_len_mean        | 24.9           |\n",
            "| ep_reward_mean     | 0.931          |\n",
            "| explained_variance | 0.577          |\n",
            "| fps                | 976            |\n",
            "| n_updates          | 963            |\n",
            "| policy_entropy     | 0.13558047     |\n",
            "| policy_loss        | -0.00074640085 |\n",
            "| serial_timesteps   | 493056         |\n",
            "| time_elapsed       | 4.62e+03       |\n",
            "| total_timesteps    | 3944448        |\n",
            "| value_loss         | 0.24717267     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.017066438   |\n",
            "| clipfrac           | 0.051708985   |\n",
            "| ep_len_mean        | 24            |\n",
            "| ep_reward_mean     | 0.933         |\n",
            "| explained_variance | 0.6           |\n",
            "| fps                | 988           |\n",
            "| n_updates          | 964           |\n",
            "| policy_entropy     | 0.13378768    |\n",
            "| policy_loss        | -0.0041758893 |\n",
            "| serial_timesteps   | 493568        |\n",
            "| time_elapsed       | 4.63e+03      |\n",
            "| total_timesteps    | 3948544       |\n",
            "| value_loss         | 0.17328122    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3950000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.40 +/- 122.84\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0095643755  |\n",
            "| clipfrac           | 0.045166016   |\n",
            "| ep_len_mean        | 23.3          |\n",
            "| ep_reward_mean     | 0.935         |\n",
            "| explained_variance | 0.581         |\n",
            "| fps                | 822           |\n",
            "| n_updates          | 965           |\n",
            "| policy_entropy     | 0.13625726    |\n",
            "| policy_loss        | -0.0038918525 |\n",
            "| serial_timesteps   | 494080        |\n",
            "| time_elapsed       | 4.63e+03      |\n",
            "| total_timesteps    | 3952640       |\n",
            "| value_loss         | 0.19175318    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.017211564   |\n",
            "| clipfrac           | 0.054174803   |\n",
            "| ep_len_mean        | 22.3          |\n",
            "| ep_reward_mean     | 0.938         |\n",
            "| explained_variance | 0.577         |\n",
            "| fps                | 993           |\n",
            "| n_updates          | 966           |\n",
            "| policy_entropy     | 0.11976962    |\n",
            "| policy_loss        | -0.0052544116 |\n",
            "| serial_timesteps   | 494592        |\n",
            "| time_elapsed       | 4.63e+03      |\n",
            "| total_timesteps    | 3956736       |\n",
            "| value_loss         | 0.12967259    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3960000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 3.12\n",
            "------------------------------------\n",
            "| approxkl           | 0.020024497 |\n",
            "| clipfrac           | 0.061450195 |\n",
            "| ep_len_mean        | 24.2        |\n",
            "| ep_reward_mean     | 0.933       |\n",
            "| explained_variance | 0.523       |\n",
            "| fps                | 959         |\n",
            "| n_updates          | 967         |\n",
            "| policy_entropy     | 0.12956308  |\n",
            "| policy_loss        | 0.003639639 |\n",
            "| serial_timesteps   | 495104      |\n",
            "| time_elapsed       | 4.64e+03    |\n",
            "| total_timesteps    | 3960832     |\n",
            "| value_loss         | 0.14580664  |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.028307926   |\n",
            "| clipfrac           | 0.052856445   |\n",
            "| ep_len_mean        | 21            |\n",
            "| ep_reward_mean     | 0.942         |\n",
            "| explained_variance | 0.531         |\n",
            "| fps                | 1007          |\n",
            "| n_updates          | 968           |\n",
            "| policy_entropy     | 0.11592819    |\n",
            "| policy_loss        | -0.0020894986 |\n",
            "| serial_timesteps   | 495616        |\n",
            "| time_elapsed       | 4.64e+03      |\n",
            "| total_timesteps    | 3964928       |\n",
            "| value_loss         | 0.13894175    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0927124    |\n",
            "| clipfrac           | 0.20261231   |\n",
            "| ep_len_mean        | 30.9         |\n",
            "| ep_reward_mean     | 0.912        |\n",
            "| explained_variance | 0.361        |\n",
            "| fps                | 1016         |\n",
            "| n_updates          | 969          |\n",
            "| policy_entropy     | 0.245253     |\n",
            "| policy_loss        | -0.004149004 |\n",
            "| serial_timesteps   | 496128       |\n",
            "| time_elapsed       | 4.65e+03     |\n",
            "| total_timesteps    | 3969024      |\n",
            "| value_loss         | 0.4419869    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3970000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.80 +/- 122.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0138151925 |\n",
            "| clipfrac           | 0.09226074   |\n",
            "| ep_len_mean        | 24           |\n",
            "| ep_reward_mean     | 0.933        |\n",
            "| explained_variance | 0.516        |\n",
            "| fps                | 843          |\n",
            "| n_updates          | 970          |\n",
            "| policy_entropy     | 0.20651722   |\n",
            "| policy_loss        | -0.008312422 |\n",
            "| serial_timesteps   | 496640       |\n",
            "| time_elapsed       | 4.65e+03     |\n",
            "| total_timesteps    | 3973120      |\n",
            "| value_loss         | 0.14488809   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.053194165   |\n",
            "| clipfrac           | 0.07551269    |\n",
            "| ep_len_mean        | 24.1          |\n",
            "| ep_reward_mean     | 0.933         |\n",
            "| explained_variance | 0.537         |\n",
            "| fps                | 1020          |\n",
            "| n_updates          | 971           |\n",
            "| policy_entropy     | 0.18322569    |\n",
            "| policy_loss        | -0.0030348417 |\n",
            "| serial_timesteps   | 497152        |\n",
            "| time_elapsed       | 4.66e+03      |\n",
            "| total_timesteps    | 3977216       |\n",
            "| value_loss         | 0.15496236    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3980000, episode_reward=0.95 +/- 0.02\n",
            "Episode length: 18.00 +/- 5.51\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01091206   |\n",
            "| clipfrac           | 0.0574707    |\n",
            "| ep_len_mean        | 24.7         |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.589        |\n",
            "| fps                | 973          |\n",
            "| n_updates          | 972          |\n",
            "| policy_entropy     | 0.17863129   |\n",
            "| policy_loss        | -0.001697577 |\n",
            "| serial_timesteps   | 497664       |\n",
            "| time_elapsed       | 4.66e+03     |\n",
            "| total_timesteps    | 3981312      |\n",
            "| value_loss         | 0.14069965   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011598285   |\n",
            "| clipfrac           | 0.049438477   |\n",
            "| ep_len_mean        | 21.5          |\n",
            "| ep_reward_mean     | 0.94          |\n",
            "| explained_variance | 0.598         |\n",
            "| fps                | 1016          |\n",
            "| n_updates          | 973           |\n",
            "| policy_entropy     | 0.15294759    |\n",
            "| policy_loss        | -0.0019898196 |\n",
            "| serial_timesteps   | 498176        |\n",
            "| time_elapsed       | 4.66e+03      |\n",
            "| total_timesteps    | 3985408       |\n",
            "| value_loss         | 0.118513264   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013867706  |\n",
            "| clipfrac           | 0.057617188  |\n",
            "| ep_len_mean        | 24.1         |\n",
            "| ep_reward_mean     | 0.933        |\n",
            "| explained_variance | 0.558        |\n",
            "| fps                | 997          |\n",
            "| n_updates          | 974          |\n",
            "| policy_entropy     | 0.15873533   |\n",
            "| policy_loss        | -0.001196564 |\n",
            "| serial_timesteps   | 498688       |\n",
            "| time_elapsed       | 4.67e+03     |\n",
            "| total_timesteps    | 3989504      |\n",
            "| value_loss         | 0.13467923   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3990000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.80 +/- 2.79\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0177216      |\n",
            "| clipfrac           | 0.06508789     |\n",
            "| ep_len_mean        | 21.4           |\n",
            "| ep_reward_mean     | 0.94           |\n",
            "| explained_variance | 0.603          |\n",
            "| fps                | 962            |\n",
            "| n_updates          | 975            |\n",
            "| policy_entropy     | 0.14235337     |\n",
            "| policy_loss        | -0.00016053565 |\n",
            "| serial_timesteps   | 499200         |\n",
            "| time_elapsed       | 4.67e+03       |\n",
            "| total_timesteps    | 3993600        |\n",
            "| value_loss         | 0.10865797     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005730438   |\n",
            "| clipfrac           | 0.035498045   |\n",
            "| ep_len_mean        | 21.9          |\n",
            "| ep_reward_mean     | 0.939         |\n",
            "| explained_variance | 0.6           |\n",
            "| fps                | 1000          |\n",
            "| n_updates          | 976           |\n",
            "| policy_entropy     | 0.12643866    |\n",
            "| policy_loss        | -0.0020944078 |\n",
            "| serial_timesteps   | 499712        |\n",
            "| time_elapsed       | 4.68e+03      |\n",
            "| total_timesteps    | 3997696       |\n",
            "| value_loss         | 0.109974705   |\n",
            "--------------------------------------\n",
            "Saving to logs/ppo2/MiniGrid-SimpleCrossingS9N1-v0_2\n",
            "\u001b[0m[821dccaef3a6:02172] *** Process received signal ***\n",
            "[821dccaef3a6:02172] Signal: Segmentation fault (11)\n",
            "[821dccaef3a6:02172] Signal code: Address not mapped (1)\n",
            "[821dccaef3a6:02172] Failing at address: 0x7fbf4f55b20d\n",
            "[821dccaef3a6:02172] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fbf52000980]\n",
            "[821dccaef3a6:02172] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fbf51c3f8a5]\n",
            "[821dccaef3a6:02172] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fbf524aae44]\n",
            "[821dccaef3a6:02172] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fbf51c40735]\n",
            "[821dccaef3a6:02172] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fbf524a8cb3]\n",
            "[821dccaef3a6:02172] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fHBq73665yD"
      },
      "source": [
        "#### Evaluate trained agent\n",
        "\n",
        "\n",
        "You can remove the `--folder logs/` to evaluate pretrained agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8YuEgU6bT3",
        "outputId": "1216c13d-837b-42ac-c15a-85ec3126ad10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python enjoy.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --no-render --n-timesteps 5000 --folder logs/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"enjoy.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"enjoy.py\", line 77, in main\n",
            "    model_path = find_saved_model(algo, log_path, env_id, load_best=args.load_best)\n",
            "  File \"/content/rl-baselines-zoo/utils/utils.py\", line 369, in find_saved_model\n",
            "    raise ValueError(\"No model found for {} on {}, path: {}\".format(algo, env_id, model_path))\n",
            "ValueError: No model found for ppo2 on MiniGrid-SimpleCrossingS9N1-v0, path: logs/ppo2/MiniGrid-SimpleCrossingS9N1-v0.zip\n",
            "[821dccaef3a6:03091] *** Process received signal ***\n",
            "[821dccaef3a6:03091] Signal: Segmentation fault (11)\n",
            "[821dccaef3a6:03091] Signal code: Address not mapped (1)\n",
            "[821dccaef3a6:03091] Failing at address: 0x7fec1431620d\n",
            "[821dccaef3a6:03091] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fec16dbb980]\n",
            "[821dccaef3a6:03091] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fec169fa8a5]\n",
            "[821dccaef3a6:03091] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fec17265e44]\n",
            "[821dccaef3a6:03091] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fec169fb735]\n",
            "[821dccaef3a6:03091] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fec17263cb3]\n",
            "[821dccaef3a6:03091] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Il2J0VHPLC"
      },
      "source": [
        "#### Tune Hyperparameters\n",
        "\n",
        "We use [Optuna](https://optuna.org/) for optimizing the hyperparameters.\n",
        "\n",
        "Tune the hyperparameters for PPO2, using a tpe sampler and median pruner, 2 parallels jobs,\n",
        "with a budget of 1000 trials and a maximum of 50000 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sC22eGHTH-"
      },
      "source": [
        "!python -m train.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --gym-packages gym_minigrid -n 5000 -optimize --n-trials 100 --n-jobs 8 --sampler tpe --pruner median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Record  a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS1VBBaQ_emT"
      },
      "source": [
        "!pip install pyglet==1.3.1  # pyglet v1.4.1 throws an error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip3AauLzwNGP"
      },
      "source": [
        "!python -m utils.record_video --algo a2c --env CartPole-v1 --exp-id 0 -f logs/ -n 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBuUfnzI8DN6"
      },
      "source": [
        "### Display the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3OTfpf8CXu"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOjFuwK9HI0"
      },
      "source": [
        "show_videos(prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjdpP0HE8D2p"
      },
      "source": [
        "### Continue Training\n",
        "\n",
        "Here, we will continue training of the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgMZQJJF6u1C"
      },
      "source": [
        "!python train.py --algo a2c --env CartPole-v1 --n-timesteps 50000 -i logs/a2c/CartPole-v1.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSaoyiAE8cVj"
      },
      "source": [
        "!python enjoy.py --algo a2c --env CartPole-v1 --no-render --n-timesteps 1000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9u4I1H-48O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}