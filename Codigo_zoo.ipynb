{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codigo zoo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpg/RL-Agent-for-Unreal-Engine/blob/main/Codigo_zoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy9QoDC7XA7"
      },
      "source": [
        "# RL Baselines Zoo: Training in Colab\n",
        "\n",
        "\n",
        "\n",
        "Github Repo: [https://github.com/araffin/rl-baselines-zoo](https://github.com/araffin/rl-baselines-zoo)\n",
        "\n",
        "Stable-Baselines Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
        "\n",
        "# Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXVDDlTn02M9",
        "outputId": "d70aa2d2-7821-42f7-ce65-c455e6a6b662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt-get update\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg freeglut3-dev xvfb\n",
        "!pip install stable-baselines[mpi] --upgrade\n",
        "!pip install pybullet\n",
        "!pip install box2d box2d-kengz pyyaml pytablewriter optuna scikit-optimize\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [407 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [58.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,688 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,781 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,129 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [222 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,365 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [247 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [46.3 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,198 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.6 kB]\n",
            "Fetched 11.4 MB in 4s (2,771 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0 xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 1,884 kB of archives.\n",
            "After this operation, 8,089 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.7 [783 kB]\n",
            "Fetched 1,884 kB in 1s (1,358 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 144786 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.7_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines[mpi]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl (240kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.15.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.1\n",
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/61/2fc2c19327966ca4e4133211be3f4dcc56c1ee6f392d71d0da8c6c1a4cba/pybullet-3.0.6-cp36-cp36m-manylinux1_x86_64.whl (102.2MB)\n",
            "\u001b[K     |████████████████████████████████| 102.2MB 62kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.0.6\n",
            "Collecting box2d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 8.3MB/s \n",
            "\u001b[?25hCollecting box2d-kengz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Collecting pytablewriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.1MB/s \n",
            "\u001b[?25hCollecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 19.7MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.6/dist-packages (from pytablewriter) (50.3.2)\n",
            "Collecting tabledata<2,>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/df/b2/264d9707502f0259a3eb82ec48064df98b1735d5a5f315b6a1d7105263f4/tabledata-1.1.3-py3-none-any.whl\n",
            "Collecting msgfy<1,>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl\n",
            "Collecting mbstrdecoder<2,>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/57/3bb55beafe0a5e9883621f01a560d16bcef6d4f844dc2dd40caa0a8d9182/mbstrdecoder-1.0.0-py3-none-any.whl\n",
            "Collecting DataProperty<2,>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/2d/e5413965af992f4e489b6f5eebf52db9c17953c772962d1223d434b05cef/DataProperty-0.50.0-py3-none-any.whl\n",
            "Collecting tcolorpy<1,>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/02/51/bbb0cc7f30771c285c354634bf83653a2871d58c6923bd29bfddeb9c9cb1/tcolorpy-0.0.8-py3-none-any.whl\n",
            "Collecting typepy[datetime]<2,>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/55/a1111b2eb1f4096c28b14645ca62aec560b1768338af21620e470b60872f/typepy-1.1.1-py3-none-any.whl\n",
            "Collecting pathvalidate<3,>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/fa/1a951084aa93940399800e37ed6f096ad5c0de3c26604be62f9464a39fc1/pathvalidate-2.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.4)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/61/5b64d73b01c1218f55c894b5ec0fb89b32c6960b7f7b3ad9f5ac0c373b9d/cliff-3.5.0-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c8/c16d30bbed11a1722060014c246d124582d1f781b26f5859d8dacc3e08e1/colorlog-4.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.17.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.20)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 30.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/3c/06c76ec8b54b9b1fad7f35e903fd25010fe3e0d41bd94cea5e6f12e0d651/cmaes-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.4 in /usr/local/lib/python3.6/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2018.9; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 38.4MB/s \n",
            "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/54/af6e2703f064485d717cb311d3f9440cd302a823ba6d80a020b59eae166d/cmd2-1.4.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 36.2MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/a1/004f04ba411a8002b02aadb089fd6868116c12ddc9f6d576175e89d07587/stevedore-3.2.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.5MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.2.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.4.0)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp36-none-any.whl size=359761 sha256=62c53c39d4fa02ebd478a7610cf44211f77bd12e773c2d2f47a42713bd568a99\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: box2d-kengz, PrettyTable, pyperclip\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp36-cp36m-linux_x86_64.whl size=2027205 sha256=c5464d32112865387eab07bf8006b08a297b048f5157737fe0f638e93531f4c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-cp36-none-any.whl size=13700 sha256=d7ebd9c7413d678feafe953b00d3bbe186b5037c9d80849fe8f10e5c0f5a6b21\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11119 sha256=cb23e61b9c2ebc9c06ca4e4055a3a8b5bc18bbb8d5d4f7a2d62b79998f8bc807\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built box2d-kengz PrettyTable pyperclip\n",
            "Installing collected packages: box2d, box2d-kengz, mbstrdecoder, typepy, DataProperty, tabledata, msgfy, tcolorpy, pathvalidate, pytablewriter, pbr, PrettyTable, colorama, pyperclip, cmd2, stevedore, cliff, colorlog, Mako, python-editor, alembic, cmaes, optuna, pyaml, scikit-optimize\n",
            "  Found existing installation: prettytable 1.0.1\n",
            "    Uninstalling prettytable-1.0.1:\n",
            "      Successfully uninstalled prettytable-1.0.1\n",
            "Successfully installed DataProperty-0.50.0 Mako-1.1.3 PrettyTable-0.7.2 alembic-1.4.3 box2d-2.3.10 box2d-kengz-2.3.3 cliff-3.5.0 cmaes-0.7.0 cmd2-1.4.0 colorama-0.4.4 colorlog-4.6.2 mbstrdecoder-1.0.0 msgfy-0.1.0 optuna-2.3.0 pathvalidate-2.3.0 pbr-5.5.1 pyaml-20.4.0 pyperclip-1.8.1 pytablewriter-0.58.0 python-editor-1.0.4 scikit-optimize-0.8.1 stevedore-3.2.2 tabledata-1.1.3 tcolorpy-0.0.8 typepy-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjF3qRg7oGH"
      },
      "source": [
        "## Clone RL Baselines Zoo Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjGikdT1DFy",
        "outputId": "0da49241-3b29-458c-a7fa-66d877e2c00b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines-zoo'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1829 (delta 12), reused 18 (delta 8), pack-reused 1796\u001b[K\n",
            "Receiving objects: 100% (1829/1829), 375.67 MiB | 35.49 MiB/s, done.\n",
            "Resolving deltas: 100% (1077/1077), done.\n",
            "Checking out files: 100% (333/333), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REMQlh-ezyVt",
        "outputId": "e8828374-42d7-4ae9-808f-bcc067d46043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd rl-baselines-zoo/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rl-baselines-zoo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ-pAbF7zRZ"
      },
      "source": [
        "## Train an RL Agent\n",
        "\n",
        "\n",
        "The train agent can be found in the `logs/` folder.\n",
        "\n",
        "Here we will train A2C on CartPole-v1 environment for 100 000 steps. \n",
        "\n",
        "\n",
        "To train it on Pong (Atari), you just have to pass `--env PongNoFrameskip-v4`\n",
        "\n",
        "Note: You need to update `hyperparams/algo.yml` to support new environments. You can access it in the side panel of Google Colab. (see https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34lXM8ZfMQfG",
        "outputId": "af9a21b8-c102-4468-cc5c-fd579abc2247",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gym-minigrid\n",
        "# go to gym-minigrid in \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs\" and replace the modeled crossing env"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-minigrid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/57/15171eff6222dd012cc89c001f5c50ad9e11b8cef385873db3b4c0d89aff/gym_minigrid-1.0.1-py3-none-any.whl (47kB)\n",
            "\r\u001b[K     |███████                         | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 20kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 40kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (1.18.5)\n",
            "Requirement already satisfied: gym>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.9.6->gym-minigrid) (0.16.0)\n",
            "Installing collected packages: gym-minigrid\n",
            "Successfully installed gym-minigrid-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmsGh9A8p4NO",
        "outputId": "ce3d5cbe-9828-4a57-9d5d-bcec9ab9f7a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# moving the mod crossing file (U MAZE MODELED) to the destination path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%rm \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs/crossing.py\"\n",
        "%cp \"/content/drive/My Drive/Colab Notebooks/crossing.py\" \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs\"\n",
        "\n",
        "#SIMULTANEAMENTE SE DEBE REEMPLAZAR LA INFORMACIÓN DEL ENTORNO EN LOS HIPERPARAMETROS PPO2 DEL PAQUETE ZOO\n",
        "%rm \"/content/rl-baselines-zoo/hyperparams/ppo2.yml\"\n",
        "%cp \"/content/drive/My Drive/Colab Notebooks/ppo2.yml\" \"/content/rl-baselines-zoo/hyperparams\"\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bIR_N7R11XI",
        "outputId": "0987edd7-dcbf-4ef3-e0da-dd12f093a068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingEnvUmaze-v0 --gym-packages gym_minigrid --n-timesteps 1000000\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MiniGrid-SimpleCrossingEnvUmaze-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('cliprange', 0.2),\n",
            "             ('ent_coef', 0.0),\n",
            "             ('env_wrapper', 'gym_minigrid.wrappers.FlatObsWrapper'),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.95),\n",
            "             ('learning_rate', 0.00025),\n",
            "             ('n_envs', 8),\n",
            "             ('n_steps', 512),\n",
            "             ('n_timesteps', 4000000.0),\n",
            "             ('nminibatches', 32),\n",
            "             ('noptepochs', 10),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 8 environments\n",
            "Overwriting n_timesteps with n=1000000\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MiniGrid-SimpleCrossingEnvUmaze-v0_2\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0060737464 |\n",
            "| clipfrac           | 0.07253418   |\n",
            "| ep_len_mean        | 308          |\n",
            "| ep_reward_mean     | 0.0676       |\n",
            "| explained_variance | -0.834       |\n",
            "| fps                | 638          |\n",
            "| n_updates          | 1            |\n",
            "| policy_entropy     | 1.9400218    |\n",
            "| policy_loss        | -0.013225389 |\n",
            "| serial_timesteps   | 512          |\n",
            "| time_elapsed       | 2.07e-05     |\n",
            "| total_timesteps    | 4096         |\n",
            "| value_loss         | 0.33766252   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.007478407  |\n",
            "| clipfrac           | 0.09350586   |\n",
            "| ep_len_mean        | 318          |\n",
            "| ep_reward_mean     | 0.0253       |\n",
            "| explained_variance | -1.42        |\n",
            "| fps                | 877          |\n",
            "| n_updates          | 2            |\n",
            "| policy_entropy     | 1.9305732    |\n",
            "| policy_loss        | -0.018929217 |\n",
            "| serial_timesteps   | 1024         |\n",
            "| time_elapsed       | 6.41         |\n",
            "| total_timesteps    | 8192         |\n",
            "| value_loss         | 0.018343262  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009221307  |\n",
            "| clipfrac           | 0.1260254    |\n",
            "| ep_len_mean        | 320          |\n",
            "| ep_reward_mean     | 0.0184       |\n",
            "| explained_variance | -3.25        |\n",
            "| fps                | 494          |\n",
            "| n_updates          | 3            |\n",
            "| policy_entropy     | 1.9170004    |\n",
            "| policy_loss        | -0.018619332 |\n",
            "| serial_timesteps   | 1536         |\n",
            "| time_elapsed       | 11.1         |\n",
            "| total_timesteps    | 12288        |\n",
            "| value_loss         | 0.0064198375 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009356292  |\n",
            "| clipfrac           | 0.13256836   |\n",
            "| ep_len_mean        | 321          |\n",
            "| ep_reward_mean     | 0.0127       |\n",
            "| explained_variance | -3.3         |\n",
            "| fps                | 875          |\n",
            "| n_updates          | 4            |\n",
            "| policy_entropy     | 1.9103873    |\n",
            "| policy_loss        | -0.014810279 |\n",
            "| serial_timesteps   | 2048         |\n",
            "| time_elapsed       | 19.4         |\n",
            "| total_timesteps    | 16384        |\n",
            "| value_loss         | 0.007553087  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.00903547   |\n",
            "| clipfrac           | 0.10649414   |\n",
            "| ep_len_mean        | 321          |\n",
            "| ep_reward_mean     | 0.0107       |\n",
            "| explained_variance | -3.69        |\n",
            "| fps                | 498          |\n",
            "| n_updates          | 5            |\n",
            "| policy_entropy     | 1.8947719    |\n",
            "| policy_loss        | -0.016452862 |\n",
            "| serial_timesteps   | 2560         |\n",
            "| time_elapsed       | 24           |\n",
            "| total_timesteps    | 20480        |\n",
            "| value_loss         | 0.007161244  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.007495214  |\n",
            "| clipfrac           | 0.097753905  |\n",
            "| ep_len_mean        | 322          |\n",
            "| ep_reward_mean     | 0.00995      |\n",
            "| explained_variance | -0.0881      |\n",
            "| fps                | 884          |\n",
            "| n_updates          | 6            |\n",
            "| policy_entropy     | 1.8756769    |\n",
            "| policy_loss        | -0.008868044 |\n",
            "| serial_timesteps   | 3072         |\n",
            "| time_elapsed       | 32.3         |\n",
            "| total_timesteps    | 24576        |\n",
            "| value_loss         | 0.10300364   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011481977  |\n",
            "| clipfrac           | 0.14758301   |\n",
            "| ep_len_mean        | 322          |\n",
            "| ep_reward_mean     | 0.00814      |\n",
            "| explained_variance | -3.75        |\n",
            "| fps                | 889          |\n",
            "| n_updates          | 7            |\n",
            "| policy_entropy     | 1.8595483    |\n",
            "| policy_loss        | -0.012647149 |\n",
            "| serial_timesteps   | 3584         |\n",
            "| time_elapsed       | 36.9         |\n",
            "| total_timesteps    | 28672        |\n",
            "| value_loss         | 0.0023754658 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "------------------------------------\n",
            "| approxkl           | 0.012370686 |\n",
            "| clipfrac           | 0.15534668  |\n",
            "| ep_len_mean        | 321         |\n",
            "| ep_reward_mean     | 0.0115      |\n",
            "| explained_variance | -0.00256    |\n",
            "| fps                | 497         |\n",
            "| n_updates          | 8           |\n",
            "| policy_entropy     | 1.8317493   |\n",
            "| policy_loss        | -0.00997186 |\n",
            "| serial_timesteps   | 4096        |\n",
            "| time_elapsed       | 41.5        |\n",
            "| total_timesteps    | 32768       |\n",
            "| value_loss         | 0.098414235 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008355652   |\n",
            "| clipfrac           | 0.112133786   |\n",
            "| ep_len_mean        | 322           |\n",
            "| ep_reward_mean     | 0.00794       |\n",
            "| explained_variance | 0.0548        |\n",
            "| fps                | 888           |\n",
            "| n_updates          | 9             |\n",
            "| policy_entropy     | 1.8201727     |\n",
            "| policy_loss        | -0.0076162047 |\n",
            "| serial_timesteps   | 4608          |\n",
            "| time_elapsed       | 49.7          |\n",
            "| total_timesteps    | 36864         |\n",
            "| value_loss         | 0.09693841    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0064058127 |\n",
            "| clipfrac           | 0.076123044  |\n",
            "| ep_len_mean        | 321          |\n",
            "| ep_reward_mean     | 0.014        |\n",
            "| explained_variance | -0.0402      |\n",
            "| fps                | 508          |\n",
            "| n_updates          | 10           |\n",
            "| policy_entropy     | 1.818197     |\n",
            "| policy_loss        | -0.007024575 |\n",
            "| serial_timesteps   | 5120         |\n",
            "| time_elapsed       | 54.3         |\n",
            "| total_timesteps    | 40960        |\n",
            "| value_loss         | 0.20604806   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.009058768   |\n",
            "| clipfrac           | 0.13300781    |\n",
            "| ep_len_mean        | 301           |\n",
            "| ep_reward_mean     | 0.0809        |\n",
            "| explained_variance | 0.0141        |\n",
            "| fps                | 913           |\n",
            "| n_updates          | 11            |\n",
            "| policy_entropy     | 1.7769308     |\n",
            "| policy_loss        | -0.0119617665 |\n",
            "| serial_timesteps   | 5632          |\n",
            "| time_elapsed       | 62.4          |\n",
            "| total_timesteps    | 45056         |\n",
            "| value_loss         | 1.0209548     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010830263  |\n",
            "| clipfrac           | 0.15690918   |\n",
            "| ep_len_mean        | 253          |\n",
            "| ep_reward_mean     | 0.234        |\n",
            "| explained_variance | -0.0161      |\n",
            "| fps                | 918          |\n",
            "| n_updates          | 12           |\n",
            "| policy_entropy     | 1.7389972    |\n",
            "| policy_loss        | -0.012336926 |\n",
            "| serial_timesteps   | 6144         |\n",
            "| time_elapsed       | 66.9         |\n",
            "| total_timesteps    | 49152        |\n",
            "| value_loss         | 2.0446987    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=0.92 +/- 0.05\n",
            "Episode length: 29.80 +/- 18.10\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0099159945 |\n",
            "| clipfrac           | 0.14892578   |\n",
            "| ep_len_mean        | 163          |\n",
            "| ep_reward_mean     | 0.523        |\n",
            "| explained_variance | 0.0127       |\n",
            "| fps                | 832          |\n",
            "| n_updates          | 13           |\n",
            "| policy_entropy     | 1.683506     |\n",
            "| policy_loss        | -0.01618229  |\n",
            "| serial_timesteps   | 6656         |\n",
            "| time_elapsed       | 71.3         |\n",
            "| total_timesteps    | 53248        |\n",
            "| value_loss         | 3.4311004    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.012544438  |\n",
            "| clipfrac           | 0.1897705    |\n",
            "| ep_len_mean        | 78           |\n",
            "| ep_reward_mean     | 0.782        |\n",
            "| explained_variance | 0.0155       |\n",
            "| fps                | 905          |\n",
            "| n_updates          | 14           |\n",
            "| policy_entropy     | 1.5758089    |\n",
            "| policy_loss        | -0.017059349 |\n",
            "| serial_timesteps   | 7168         |\n",
            "| time_elapsed       | 76.3         |\n",
            "| total_timesteps    | 57344        |\n",
            "| value_loss         | 5.208345     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011864503  |\n",
            "| clipfrac           | 0.16604003   |\n",
            "| ep_len_mean        | 57.3         |\n",
            "| ep_reward_mean     | 0.841        |\n",
            "| explained_variance | -0.0217      |\n",
            "| fps                | 862          |\n",
            "| n_updates          | 15           |\n",
            "| policy_entropy     | 1.5253243    |\n",
            "| policy_loss        | -0.015501684 |\n",
            "| serial_timesteps   | 7680         |\n",
            "| time_elapsed       | 80.8         |\n",
            "| total_timesteps    | 61440        |\n",
            "| value_loss         | 6.6660705    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016483333  |\n",
            "| clipfrac           | 0.26711425   |\n",
            "| ep_len_mean        | 45.5         |\n",
            "| ep_reward_mean     | 0.874        |\n",
            "| explained_variance | -0.0208      |\n",
            "| fps                | 907          |\n",
            "| n_updates          | 16           |\n",
            "| policy_entropy     | 1.395708     |\n",
            "| policy_loss        | -0.022111954 |\n",
            "| serial_timesteps   | 8192         |\n",
            "| time_elapsed       | 85.5         |\n",
            "| total_timesteps    | 65536        |\n",
            "| value_loss         | 7.8965673    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013222839  |\n",
            "| clipfrac           | 0.20012207   |\n",
            "| ep_len_mean        | 38.4         |\n",
            "| ep_reward_mean     | 0.893        |\n",
            "| explained_variance | -0.0406      |\n",
            "| fps                | 906          |\n",
            "| n_updates          | 17           |\n",
            "| policy_entropy     | 1.3867588    |\n",
            "| policy_loss        | -0.016154235 |\n",
            "| serial_timesteps   | 8704         |\n",
            "| time_elapsed       | 90           |\n",
            "| total_timesteps    | 69632        |\n",
            "| value_loss         | 8.133405     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "------------------------------------\n",
            "| approxkl           | 0.012609671 |\n",
            "| clipfrac           | 0.1871582   |\n",
            "| ep_len_mean        | 57.3        |\n",
            "| ep_reward_mean     | 0.84        |\n",
            "| explained_variance | 0.0158      |\n",
            "| fps                | 485         |\n",
            "| n_updates          | 18          |\n",
            "| policy_entropy     | 1.5472995   |\n",
            "| policy_loss        | -0.01835866 |\n",
            "| serial_timesteps   | 9216        |\n",
            "| time_elapsed       | 94.6        |\n",
            "| total_timesteps    | 73728       |\n",
            "| value_loss         | 4.6140566   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014217123  |\n",
            "| clipfrac           | 0.21169433   |\n",
            "| ep_len_mean        | 46.8         |\n",
            "| ep_reward_mean     | 0.87         |\n",
            "| explained_variance | 0.0164       |\n",
            "| fps                | 900          |\n",
            "| n_updates          | 19           |\n",
            "| policy_entropy     | 1.4066998    |\n",
            "| policy_loss        | -0.016554292 |\n",
            "| serial_timesteps   | 9728         |\n",
            "| time_elapsed       | 103          |\n",
            "| total_timesteps    | 77824        |\n",
            "| value_loss         | 6.0495296    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018947026  |\n",
            "| clipfrac           | 0.25063476   |\n",
            "| ep_len_mean        | 49           |\n",
            "| ep_reward_mean     | 0.864        |\n",
            "| explained_variance | 0.123        |\n",
            "| fps                | 482          |\n",
            "| n_updates          | 20           |\n",
            "| policy_entropy     | 1.461919     |\n",
            "| policy_loss        | -0.021208033 |\n",
            "| serial_timesteps   | 10240        |\n",
            "| time_elapsed       | 108          |\n",
            "| total_timesteps    | 81920        |\n",
            "| value_loss         | 5.050758     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014806238  |\n",
            "| clipfrac           | 0.18735352   |\n",
            "| ep_len_mean        | 65.5         |\n",
            "| ep_reward_mean     | 0.818        |\n",
            "| explained_variance | 0.117        |\n",
            "| fps                | 874          |\n",
            "| n_updates          | 21           |\n",
            "| policy_entropy     | 1.5067219    |\n",
            "| policy_loss        | -0.018963782 |\n",
            "| serial_timesteps   | 10752        |\n",
            "| time_elapsed       | 116          |\n",
            "| total_timesteps    | 86016        |\n",
            "| value_loss         | 4.634322     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015062523  |\n",
            "| clipfrac           | 0.21958008   |\n",
            "| ep_len_mean        | 48.1         |\n",
            "| ep_reward_mean     | 0.866        |\n",
            "| explained_variance | 0.183        |\n",
            "| fps                | 474          |\n",
            "| n_updates          | 22           |\n",
            "| policy_entropy     | 1.4061298    |\n",
            "| policy_loss        | -0.020684747 |\n",
            "| serial_timesteps   | 11264        |\n",
            "| time_elapsed       | 121          |\n",
            "| total_timesteps    | 90112        |\n",
            "| value_loss         | 5.5671635    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015517183  |\n",
            "| clipfrac           | 0.24951172   |\n",
            "| ep_len_mean        | 38.1         |\n",
            "| ep_reward_mean     | 0.894        |\n",
            "| explained_variance | 0.195        |\n",
            "| fps                | 878          |\n",
            "| n_updates          | 23           |\n",
            "| policy_entropy     | 1.2719735    |\n",
            "| policy_loss        | -0.022028249 |\n",
            "| serial_timesteps   | 11776        |\n",
            "| time_elapsed       | 129          |\n",
            "| total_timesteps    | 94208        |\n",
            "| value_loss         | 5.9621153    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02247234   |\n",
            "| clipfrac           | 0.3104004    |\n",
            "| ep_len_mean        | 35.9         |\n",
            "| ep_reward_mean     | 0.899        |\n",
            "| explained_variance | 0.258        |\n",
            "| fps                | 883          |\n",
            "| n_updates          | 24           |\n",
            "| policy_entropy     | 1.1722472    |\n",
            "| policy_loss        | -0.033978827 |\n",
            "| serial_timesteps   | 12288        |\n",
            "| time_elapsed       | 134          |\n",
            "| total_timesteps    | 98304        |\n",
            "| value_loss         | 5.750941     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015725568  |\n",
            "| clipfrac           | 0.17963867   |\n",
            "| ep_len_mean        | 29.4         |\n",
            "| ep_reward_mean     | 0.918        |\n",
            "| explained_variance | 0.257        |\n",
            "| fps                | 835          |\n",
            "| n_updates          | 25           |\n",
            "| policy_entropy     | 1.0676992    |\n",
            "| policy_loss        | -0.021491911 |\n",
            "| serial_timesteps   | 12800        |\n",
            "| time_elapsed       | 139          |\n",
            "| total_timesteps    | 102400       |\n",
            "| value_loss         | 6.6338754    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018815102  |\n",
            "| clipfrac           | 0.22131348   |\n",
            "| ep_len_mean        | 26.1         |\n",
            "| ep_reward_mean     | 0.927        |\n",
            "| explained_variance | 0.277        |\n",
            "| fps                | 874          |\n",
            "| n_updates          | 26           |\n",
            "| policy_entropy     | 0.8998575    |\n",
            "| policy_loss        | -0.020219631 |\n",
            "| serial_timesteps   | 13312        |\n",
            "| time_elapsed       | 144          |\n",
            "| total_timesteps    | 106496       |\n",
            "| value_loss         | 6.7374725    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "------------------------------------\n",
            "| approxkl           | 0.01357246  |\n",
            "| clipfrac           | 0.16569825  |\n",
            "| ep_len_mean        | 24.4        |\n",
            "| ep_reward_mean     | 0.932       |\n",
            "| explained_variance | 0.302       |\n",
            "| fps                | 851         |\n",
            "| n_updates          | 27          |\n",
            "| policy_entropy     | 0.8301182   |\n",
            "| policy_loss        | -0.01970561 |\n",
            "| serial_timesteps   | 13824       |\n",
            "| time_elapsed       | 148         |\n",
            "| total_timesteps    | 110592      |\n",
            "| value_loss         | 5.6808324   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.00891514  |\n",
            "| clipfrac           | 0.13330078  |\n",
            "| ep_len_mean        | 24.3        |\n",
            "| ep_reward_mean     | 0.932       |\n",
            "| explained_variance | 0.287       |\n",
            "| fps                | 871         |\n",
            "| n_updates          | 28          |\n",
            "| policy_entropy     | 0.74698645  |\n",
            "| policy_loss        | -0.01586464 |\n",
            "| serial_timesteps   | 14336       |\n",
            "| time_elapsed       | 153         |\n",
            "| total_timesteps    | 114688      |\n",
            "| value_loss         | 4.3462806   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008737027  |\n",
            "| clipfrac           | 0.12702636   |\n",
            "| ep_len_mean        | 21.5         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.413        |\n",
            "| fps                | 877          |\n",
            "| n_updates          | 29           |\n",
            "| policy_entropy     | 0.64610755   |\n",
            "| policy_loss        | -0.014564688 |\n",
            "| serial_timesteps   | 14848        |\n",
            "| time_elapsed       | 158          |\n",
            "| total_timesteps    | 118784       |\n",
            "| value_loss         | 3.614579     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.006341424  |\n",
            "| clipfrac           | 0.099658206  |\n",
            "| ep_len_mean        | 20.8         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.423        |\n",
            "| fps                | 819          |\n",
            "| n_updates          | 30           |\n",
            "| policy_entropy     | 0.56959844   |\n",
            "| policy_loss        | -0.012681281 |\n",
            "| serial_timesteps   | 15360        |\n",
            "| time_elapsed       | 162          |\n",
            "| total_timesteps    | 122880       |\n",
            "| value_loss         | 2.709853     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.00516321   |\n",
            "| clipfrac           | 0.08115234   |\n",
            "| ep_len_mean        | 19.3         |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.488        |\n",
            "| fps                | 858          |\n",
            "| n_updates          | 31           |\n",
            "| policy_entropy     | 0.46196547   |\n",
            "| policy_loss        | -0.012122857 |\n",
            "| serial_timesteps   | 15872        |\n",
            "| time_elapsed       | 167          |\n",
            "| total_timesteps    | 126976       |\n",
            "| value_loss         | 2.097788     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0055942973 |\n",
            "| clipfrac           | 0.057250977  |\n",
            "| ep_len_mean        | 18.3         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.556        |\n",
            "| fps                | 825          |\n",
            "| n_updates          | 32           |\n",
            "| policy_entropy     | 0.367521     |\n",
            "| policy_loss        | -0.010432501 |\n",
            "| serial_timesteps   | 16384        |\n",
            "| time_elapsed       | 172          |\n",
            "| total_timesteps    | 131072       |\n",
            "| value_loss         | 1.4448258    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0028809528  |\n",
            "| clipfrac           | 0.036010742   |\n",
            "| ep_len_mean        | 17.4          |\n",
            "| ep_reward_mean     | 0.952         |\n",
            "| explained_variance | 0.599         |\n",
            "| fps                | 841           |\n",
            "| n_updates          | 33            |\n",
            "| policy_entropy     | 0.30397215    |\n",
            "| policy_loss        | -0.0066019422 |\n",
            "| serial_timesteps   | 16896         |\n",
            "| time_elapsed       | 177           |\n",
            "| total_timesteps    | 135168        |\n",
            "| value_loss         | 0.9608933     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0025710098  |\n",
            "| clipfrac           | 0.034692384   |\n",
            "| ep_len_mean        | 17.4          |\n",
            "| ep_reward_mean     | 0.952         |\n",
            "| explained_variance | 0.558         |\n",
            "| fps                | 858           |\n",
            "| n_updates          | 34            |\n",
            "| policy_entropy     | 0.255448      |\n",
            "| policy_loss        | -0.0051285927 |\n",
            "| serial_timesteps   | 17408         |\n",
            "| time_elapsed       | 182           |\n",
            "| total_timesteps    | 139264        |\n",
            "| value_loss         | 0.58008623    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019253459  |\n",
            "| clipfrac           | 0.08139648   |\n",
            "| ep_len_mean        | 18.1         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.198        |\n",
            "| fps                | 822          |\n",
            "| n_updates          | 35           |\n",
            "| policy_entropy     | 0.23516746   |\n",
            "| policy_loss        | -0.012132247 |\n",
            "| serial_timesteps   | 17920        |\n",
            "| time_elapsed       | 187          |\n",
            "| total_timesteps    | 143360       |\n",
            "| value_loss         | 0.33174485   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.007082174  |\n",
            "| clipfrac           | 0.029272461  |\n",
            "| ep_len_mean        | 17.8         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.0832       |\n",
            "| fps                | 856          |\n",
            "| n_updates          | 36           |\n",
            "| policy_entropy     | 0.24384046   |\n",
            "| policy_loss        | -0.006713363 |\n",
            "| serial_timesteps   | 18432        |\n",
            "| time_elapsed       | 192          |\n",
            "| total_timesteps    | 147456       |\n",
            "| value_loss         | 0.17218727   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013464193  |\n",
            "| clipfrac           | 0.01965332    |\n",
            "| ep_len_mean        | 17.4          |\n",
            "| ep_reward_mean     | 0.952         |\n",
            "| explained_variance | 0.11          |\n",
            "| fps                | 832           |\n",
            "| n_updates          | 37            |\n",
            "| policy_entropy     | 0.18668793    |\n",
            "| policy_loss        | -0.0045823045 |\n",
            "| serial_timesteps   | 18944         |\n",
            "| time_elapsed       | 197           |\n",
            "| total_timesteps    | 151552        |\n",
            "| value_loss         | 0.08387156    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0012487303 |\n",
            "| clipfrac           | 0.020996094  |\n",
            "| ep_len_mean        | 17           |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.386        |\n",
            "| fps                | 879          |\n",
            "| n_updates          | 38           |\n",
            "| policy_entropy     | 0.15846173   |\n",
            "| policy_loss        | -0.004566534 |\n",
            "| serial_timesteps   | 19456        |\n",
            "| time_elapsed       | 201          |\n",
            "| total_timesteps    | 155648       |\n",
            "| value_loss         | 0.02685151   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015771346  |\n",
            "| clipfrac           | 0.029638672   |\n",
            "| ep_len_mean        | 17.1          |\n",
            "| ep_reward_mean     | 0.952         |\n",
            "| explained_variance | 0.616         |\n",
            "| fps                | 872           |\n",
            "| n_updates          | 39            |\n",
            "| policy_entropy     | 0.13471815    |\n",
            "| policy_loss        | -0.0075625055 |\n",
            "| serial_timesteps   | 19968         |\n",
            "| time_elapsed       | 206           |\n",
            "| total_timesteps    | 159744        |\n",
            "| value_loss         | 0.0125805605  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010864049  |\n",
            "| clipfrac           | 0.019165039   |\n",
            "| ep_len_mean        | 16.8          |\n",
            "| ep_reward_mean     | 0.953         |\n",
            "| explained_variance | 0.739         |\n",
            "| fps                | 855           |\n",
            "| n_updates          | 40            |\n",
            "| policy_entropy     | 0.10256685    |\n",
            "| policy_loss        | -0.0057706246 |\n",
            "| serial_timesteps   | 20480         |\n",
            "| time_elapsed       | 211           |\n",
            "| total_timesteps    | 163840        |\n",
            "| value_loss         | 0.008681228   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.018434267 |\n",
            "| clipfrac           | 0.15683594  |\n",
            "| ep_len_mean        | 21.9        |\n",
            "| ep_reward_mean     | 0.939       |\n",
            "| explained_variance | 0.0381      |\n",
            "| fps                | 896         |\n",
            "| n_updates          | 41          |\n",
            "| policy_entropy     | 0.1423694   |\n",
            "| policy_loss        | 0.008342674 |\n",
            "| serial_timesteps   | 20992       |\n",
            "| time_elapsed       | 216         |\n",
            "| total_timesteps    | 167936      |\n",
            "| value_loss         | 0.035533417 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=0.94 +/- 0.00\n",
            "Episode length: 22.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015312968  |\n",
            "| clipfrac           | 0.14121094   |\n",
            "| ep_len_mean        | 21.8         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.36         |\n",
            "| fps                | 849          |\n",
            "| n_updates          | 42           |\n",
            "| policy_entropy     | 0.24521048   |\n",
            "| policy_loss        | -0.015987912 |\n",
            "| serial_timesteps   | 21504        |\n",
            "| time_elapsed       | 220          |\n",
            "| total_timesteps    | 172032       |\n",
            "| value_loss         | 0.02997464   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009180237  |\n",
            "| clipfrac           | 0.14152832   |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.533        |\n",
            "| fps                | 910          |\n",
            "| n_updates          | 43           |\n",
            "| policy_entropy     | 0.23604274   |\n",
            "| policy_loss        | -0.016002564 |\n",
            "| serial_timesteps   | 22016        |\n",
            "| time_elapsed       | 225          |\n",
            "| total_timesteps    | 176128       |\n",
            "| value_loss         | 0.01758524   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=0.29 +/- 0.38\n",
            "Episode length: 233.80 +/- 120.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.004494191  |\n",
            "| clipfrac           | 0.09448242   |\n",
            "| ep_len_mean        | 18.6         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.733        |\n",
            "| fps                | 567          |\n",
            "| n_updates          | 44           |\n",
            "| policy_entropy     | 0.18144265   |\n",
            "| policy_loss        | -0.015996296 |\n",
            "| serial_timesteps   | 22528        |\n",
            "| time_elapsed       | 230          |\n",
            "| total_timesteps    | 180224       |\n",
            "| value_loss         | 0.012019559  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.004164969  |\n",
            "| clipfrac           | 0.076220706  |\n",
            "| ep_len_mean        | 17.7         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.764        |\n",
            "| fps                | 896          |\n",
            "| n_updates          | 45           |\n",
            "| policy_entropy     | 0.1545813    |\n",
            "| policy_loss        | -0.013643007 |\n",
            "| serial_timesteps   | 23040        |\n",
            "| time_elapsed       | 237          |\n",
            "| total_timesteps    | 184320       |\n",
            "| value_loss         | 0.009787595  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0039215973 |\n",
            "| clipfrac           | 0.058520507  |\n",
            "| ep_len_mean        | 17.1         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.782        |\n",
            "| fps                | 881          |\n",
            "| n_updates          | 46           |\n",
            "| policy_entropy     | 0.12626372   |\n",
            "| policy_loss        | -0.012810071 |\n",
            "| serial_timesteps   | 23552        |\n",
            "| time_elapsed       | 241          |\n",
            "| total_timesteps    | 188416       |\n",
            "| value_loss         | 0.008166415  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018950863  |\n",
            "| clipfrac           | 0.028930664   |\n",
            "| ep_len_mean        | 16.6          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.805         |\n",
            "| fps                | 829           |\n",
            "| n_updates          | 47            |\n",
            "| policy_entropy     | 0.09725921    |\n",
            "| policy_loss        | -0.0051438035 |\n",
            "| serial_timesteps   | 24064         |\n",
            "| time_elapsed       | 246           |\n",
            "| total_timesteps    | 192512        |\n",
            "| value_loss         | 0.0066637406  |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0010678305 |\n",
            "| clipfrac           | 0.016357422  |\n",
            "| ep_len_mean        | 16.4         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.824        |\n",
            "| fps                | 874          |\n",
            "| n_updates          | 48           |\n",
            "| policy_entropy     | 0.078609064  |\n",
            "| policy_loss        | -0.00282784  |\n",
            "| serial_timesteps   | 24576        |\n",
            "| time_elapsed       | 251          |\n",
            "| total_timesteps    | 196608       |\n",
            "| value_loss         | 0.005701362  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0009860373 |\n",
            "| clipfrac           | 0.01508789   |\n",
            "| ep_len_mean        | 16.4         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.824        |\n",
            "| fps                | 847          |\n",
            "| n_updates          | 49           |\n",
            "| policy_entropy     | 0.070021406  |\n",
            "| policy_loss        | -0.002323953 |\n",
            "| serial_timesteps   | 25088        |\n",
            "| time_elapsed       | 256          |\n",
            "| total_timesteps    | 200704       |\n",
            "| value_loss         | 0.005456348  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00047980435 |\n",
            "| clipfrac           | 0.008398438   |\n",
            "| ep_len_mean        | 16.4          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.834         |\n",
            "| fps                | 868           |\n",
            "| n_updates          | 50            |\n",
            "| policy_entropy     | 0.05958129    |\n",
            "| policy_loss        | -0.001562645  |\n",
            "| serial_timesteps   | 25600         |\n",
            "| time_elapsed       | 260           |\n",
            "| total_timesteps    | 204800        |\n",
            "| value_loss         | 0.0050172666  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0004893254  |\n",
            "| clipfrac           | 0.0087890625  |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.836         |\n",
            "| fps                | 895           |\n",
            "| n_updates          | 51            |\n",
            "| policy_entropy     | 0.053251654   |\n",
            "| policy_loss        | -0.0017619213 |\n",
            "| serial_timesteps   | 26112         |\n",
            "| time_elapsed       | 265           |\n",
            "| total_timesteps    | 208896        |\n",
            "| value_loss         | 0.0048110005  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00043575506 |\n",
            "| clipfrac           | 0.0078125     |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.838         |\n",
            "| fps                | 845           |\n",
            "| n_updates          | 52            |\n",
            "| policy_entropy     | 0.044315245   |\n",
            "| policy_loss        | -0.0018426694 |\n",
            "| serial_timesteps   | 26624         |\n",
            "| time_elapsed       | 270           |\n",
            "| total_timesteps    | 212992        |\n",
            "| value_loss         | 0.004717754   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00013016319  |\n",
            "| clipfrac           | 0.0019775392   |\n",
            "| ep_len_mean        | 16.2           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.844          |\n",
            "| fps                | 872            |\n",
            "| n_updates          | 53             |\n",
            "| policy_entropy     | 0.039963692    |\n",
            "| policy_loss        | -0.00019903206 |\n",
            "| serial_timesteps   | 27136          |\n",
            "| time_elapsed       | 275            |\n",
            "| total_timesteps    | 217088         |\n",
            "| value_loss         | 0.0044719107   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00013380227 |\n",
            "| clipfrac           | 0.0025878907  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.845         |\n",
            "| fps                | 843           |\n",
            "| n_updates          | 54            |\n",
            "| policy_entropy     | 0.03367804    |\n",
            "| policy_loss        | -0.0004746973 |\n",
            "| serial_timesteps   | 27648         |\n",
            "| time_elapsed       | 279           |\n",
            "| total_timesteps    | 221184        |\n",
            "| value_loss         | 0.004298583   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000352292   |\n",
            "| clipfrac           | 0.0064453124  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.843         |\n",
            "| fps                | 878           |\n",
            "| n_updates          | 55            |\n",
            "| policy_entropy     | 0.028530264   |\n",
            "| policy_loss        | -0.0021571422 |\n",
            "| serial_timesteps   | 28160         |\n",
            "| time_elapsed       | 284           |\n",
            "| total_timesteps    | 225280        |\n",
            "| value_loss         | 0.0042874697  |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.004027094 |\n",
            "| clipfrac           | 0.06694336  |\n",
            "| ep_len_mean        | 20          |\n",
            "| ep_reward_mean     | 0.944       |\n",
            "| explained_variance | 0.616       |\n",
            "| fps                | 881         |\n",
            "| n_updates          | 56          |\n",
            "| policy_entropy     | 0.21153347  |\n",
            "| policy_loss        | -0.01145215 |\n",
            "| serial_timesteps   | 28672       |\n",
            "| time_elapsed       | 289         |\n",
            "| total_timesteps    | 229376      |\n",
            "| value_loss         | 0.013033301 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 19.00 +/- 1.55\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0039166855 |\n",
            "| clipfrac           | 0.06254883   |\n",
            "| ep_len_mean        | 20.1         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.642        |\n",
            "| fps                | 854          |\n",
            "| n_updates          | 57           |\n",
            "| policy_entropy     | 0.25510764   |\n",
            "| policy_loss        | -0.012671536 |\n",
            "| serial_timesteps   | 29184        |\n",
            "| time_elapsed       | 293          |\n",
            "| total_timesteps    | 233472       |\n",
            "| value_loss         | 0.013962476  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0045028725 |\n",
            "| clipfrac           | 0.076733395  |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.669        |\n",
            "| fps                | 892          |\n",
            "| n_updates          | 58           |\n",
            "| policy_entropy     | 0.25447634   |\n",
            "| policy_loss        | -0.015161129 |\n",
            "| serial_timesteps   | 29696        |\n",
            "| time_elapsed       | 298          |\n",
            "| total_timesteps    | 237568       |\n",
            "| value_loss         | 0.014894036  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=0.90 +/- 0.08\n",
            "Episode length: 34.80 +/- 29.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0063552074 |\n",
            "| clipfrac           | 0.10007324   |\n",
            "| ep_len_mean        | 19.9         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.673        |\n",
            "| fps                | 815          |\n",
            "| n_updates          | 59           |\n",
            "| policy_entropy     | 0.25925368   |\n",
            "| policy_loss        | -0.017578555 |\n",
            "| serial_timesteps   | 30208        |\n",
            "| time_elapsed       | 303          |\n",
            "| total_timesteps    | 241664       |\n",
            "| value_loss         | 0.015214737  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.05568894   |\n",
            "| clipfrac           | 0.11018066   |\n",
            "| ep_len_mean        | 18.4         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.721        |\n",
            "| fps                | 902          |\n",
            "| n_updates          | 60           |\n",
            "| policy_entropy     | 0.188184     |\n",
            "| policy_loss        | -0.022196861 |\n",
            "| serial_timesteps   | 30720        |\n",
            "| time_elapsed       | 308          |\n",
            "| total_timesteps    | 245760       |\n",
            "| value_loss         | 0.011307417  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0045522507 |\n",
            "| clipfrac           | 0.076855466  |\n",
            "| ep_len_mean        | 17.3         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.765        |\n",
            "| fps                | 884          |\n",
            "| n_updates          | 61           |\n",
            "| policy_entropy     | 0.14556353   |\n",
            "| policy_loss        | -0.014221442 |\n",
            "| serial_timesteps   | 31232        |\n",
            "| time_elapsed       | 312          |\n",
            "| total_timesteps    | 249856       |\n",
            "| value_loss         | 0.008075642  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026440644  |\n",
            "| clipfrac           | 0.039575197   |\n",
            "| ep_len_mean        | 16.9          |\n",
            "| ep_reward_mean     | 0.953         |\n",
            "| explained_variance | 0.802         |\n",
            "| fps                | 833           |\n",
            "| n_updates          | 62            |\n",
            "| policy_entropy     | 0.11290395    |\n",
            "| policy_loss        | -0.0073232427 |\n",
            "| serial_timesteps   | 31744         |\n",
            "| time_elapsed       | 317           |\n",
            "| total_timesteps    | 253952        |\n",
            "| value_loss         | 0.0061972183  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015429885  |\n",
            "| clipfrac           | 0.024804687   |\n",
            "| ep_len_mean        | 16.7          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.808         |\n",
            "| fps                | 840           |\n",
            "| n_updates          | 63            |\n",
            "| policy_entropy     | 0.093611486   |\n",
            "| policy_loss        | -0.0041919416 |\n",
            "| serial_timesteps   | 32256         |\n",
            "| time_elapsed       | 322           |\n",
            "| total_timesteps    | 258048        |\n",
            "| value_loss         | 0.005682931   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007039617  |\n",
            "| clipfrac           | 0.011328125   |\n",
            "| ep_len_mean        | 16.4          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.827         |\n",
            "| fps                | 829           |\n",
            "| n_updates          | 64            |\n",
            "| policy_entropy     | 0.073328495   |\n",
            "| policy_loss        | -0.0013571393 |\n",
            "| serial_timesteps   | 32768         |\n",
            "| time_elapsed       | 327           |\n",
            "| total_timesteps    | 262144        |\n",
            "| value_loss         | 0.0047344966  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00078500266 |\n",
            "| clipfrac           | 0.013012695   |\n",
            "| ep_len_mean        | 16.5          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.827         |\n",
            "| fps                | 845           |\n",
            "| n_updates          | 65            |\n",
            "| policy_entropy     | 0.062548466   |\n",
            "| policy_loss        | -0.002580035  |\n",
            "| serial_timesteps   | 33280         |\n",
            "| time_elapsed       | 332           |\n",
            "| total_timesteps    | 266240        |\n",
            "| value_loss         | 0.0046086526  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00068220176 |\n",
            "| clipfrac           | 0.011865234   |\n",
            "| ep_len_mean        | 16.4          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.835         |\n",
            "| fps                | 813           |\n",
            "| n_updates          | 66            |\n",
            "| policy_entropy     | 0.05300942    |\n",
            "| policy_loss        | -0.0024335764 |\n",
            "| serial_timesteps   | 33792         |\n",
            "| time_elapsed       | 337           |\n",
            "| total_timesteps    | 270336        |\n",
            "| value_loss         | 0.004425709   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00034717863 |\n",
            "| clipfrac           | 0.0063964846  |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.839         |\n",
            "| fps                | 866           |\n",
            "| n_updates          | 67            |\n",
            "| policy_entropy     | 0.04386171    |\n",
            "| policy_loss        | -0.0014212797 |\n",
            "| serial_timesteps   | 34304         |\n",
            "| time_elapsed       | 342           |\n",
            "| total_timesteps    | 274432        |\n",
            "| value_loss         | 0.0041398257  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00043372638 |\n",
            "| clipfrac           | 0.0071289064  |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.842         |\n",
            "| fps                | 848           |\n",
            "| n_updates          | 68            |\n",
            "| policy_entropy     | 0.037840284   |\n",
            "| policy_loss        | -0.0014535344 |\n",
            "| serial_timesteps   | 34816         |\n",
            "| time_elapsed       | 346           |\n",
            "| total_timesteps    | 278528        |\n",
            "| value_loss         | 0.0040375944  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00033456733 |\n",
            "| clipfrac           | 0.0058105467  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.842         |\n",
            "| fps                | 816           |\n",
            "| n_updates          | 69            |\n",
            "| policy_entropy     | 0.032384817   |\n",
            "| policy_loss        | -0.001514437  |\n",
            "| serial_timesteps   | 35328         |\n",
            "| time_elapsed       | 351           |\n",
            "| total_timesteps    | 282624        |\n",
            "| value_loss         | 0.004039091   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00031012652 |\n",
            "| clipfrac           | 0.005102539   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.842         |\n",
            "| fps                | 842           |\n",
            "| n_updates          | 70            |\n",
            "| policy_entropy     | 0.02849788    |\n",
            "| policy_loss        | -0.0006872284 |\n",
            "| serial_timesteps   | 35840         |\n",
            "| time_elapsed       | 356           |\n",
            "| total_timesteps    | 286720        |\n",
            "| value_loss         | 0.0039403704  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00044695218 |\n",
            "| clipfrac           | 0.006616211   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.84          |\n",
            "| fps                | 815           |\n",
            "| n_updates          | 71            |\n",
            "| policy_entropy     | 0.025495246   |\n",
            "| policy_loss        | -0.0024051063 |\n",
            "| serial_timesteps   | 36352         |\n",
            "| time_elapsed       | 361           |\n",
            "| total_timesteps    | 290816        |\n",
            "| value_loss         | 0.0040022507  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00016186743 |\n",
            "| clipfrac           | 0.003100586   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.844         |\n",
            "| fps                | 851           |\n",
            "| n_updates          | 72            |\n",
            "| policy_entropy     | 0.021042991   |\n",
            "| policy_loss        | -0.0008197681 |\n",
            "| serial_timesteps   | 36864         |\n",
            "| time_elapsed       | 366           |\n",
            "| total_timesteps    | 294912        |\n",
            "| value_loss         | 0.0037939672  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.3004664e-05 |\n",
            "| clipfrac           | 0.0010742188  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.847         |\n",
            "| fps                | 873           |\n",
            "| n_updates          | 73            |\n",
            "| policy_entropy     | 0.020164635   |\n",
            "| policy_loss        | 8.9414025e-05 |\n",
            "| serial_timesteps   | 37376         |\n",
            "| time_elapsed       | 371           |\n",
            "| total_timesteps    | 299008        |\n",
            "| value_loss         | 0.0037688916  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 8.748115e-05   |\n",
            "| clipfrac           | 0.0019775392   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 832            |\n",
            "| n_updates          | 74             |\n",
            "| policy_entropy     | 0.01838451     |\n",
            "| policy_loss        | -0.00038587704 |\n",
            "| serial_timesteps   | 37888          |\n",
            "| time_elapsed       | 376            |\n",
            "| total_timesteps    | 303104         |\n",
            "| value_loss         | 0.0036857035   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00010330853 |\n",
            "| clipfrac           | 0.0012451172  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.85          |\n",
            "| fps                | 867           |\n",
            "| n_updates          | 75            |\n",
            "| policy_entropy     | 0.015249861   |\n",
            "| policy_loss        | -0.0003781658 |\n",
            "| serial_timesteps   | 38400         |\n",
            "| time_elapsed       | 380           |\n",
            "| total_timesteps    | 307200        |\n",
            "| value_loss         | 0.0036304859  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00011651726  |\n",
            "| clipfrac           | 0.001538086    |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.848          |\n",
            "| fps                | 833            |\n",
            "| n_updates          | 76             |\n",
            "| policy_entropy     | 0.0140511      |\n",
            "| policy_loss        | -0.00017550774 |\n",
            "| serial_timesteps   | 38912          |\n",
            "| time_elapsed       | 385            |\n",
            "| total_timesteps    | 311296         |\n",
            "| value_loss         | 0.0035975925   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 6.780594e-05   |\n",
            "| clipfrac           | 0.0010742188   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.846          |\n",
            "| fps                | 877            |\n",
            "| n_updates          | 77             |\n",
            "| policy_entropy     | 0.011611393    |\n",
            "| policy_loss        | -0.00022494672 |\n",
            "| serial_timesteps   | 39424          |\n",
            "| time_elapsed       | 390            |\n",
            "| total_timesteps    | 315392         |\n",
            "| value_loss         | 0.0036416594   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 4.5053e-05     |\n",
            "| clipfrac           | 0.00083007815  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 860            |\n",
            "| n_updates          | 78             |\n",
            "| policy_entropy     | 0.009402862    |\n",
            "| policy_loss        | -0.00011146942 |\n",
            "| serial_timesteps   | 39936          |\n",
            "| time_elapsed       | 395            |\n",
            "| total_timesteps    | 319488         |\n",
            "| value_loss         | 0.0034498808   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00014857097 |\n",
            "| clipfrac           | 0.0010498047  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.846         |\n",
            "| fps                | 831           |\n",
            "| n_updates          | 79            |\n",
            "| policy_entropy     | 0.008989548   |\n",
            "| policy_loss        | -0.0003759617 |\n",
            "| serial_timesteps   | 40448         |\n",
            "| time_elapsed       | 400           |\n",
            "| total_timesteps    | 323584        |\n",
            "| value_loss         | 0.0035209446  |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0021159914 |\n",
            "| clipfrac           | 0.02998047   |\n",
            "| ep_len_mean        | 18.3         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.589        |\n",
            "| fps                | 860          |\n",
            "| n_updates          | 80           |\n",
            "| policy_entropy     | 0.055637695  |\n",
            "| policy_loss        | -0.006198163 |\n",
            "| serial_timesteps   | 40960        |\n",
            "| time_elapsed       | 404          |\n",
            "| total_timesteps    | 327680       |\n",
            "| value_loss         | 0.007465788  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=0.94 +/- 0.00\n",
            "Episode length: 20.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014167133  |\n",
            "| clipfrac           | 0.020019531   |\n",
            "| ep_len_mean        | 20.2          |\n",
            "| ep_reward_mean     | 0.944         |\n",
            "| explained_variance | 0.633         |\n",
            "| fps                | 850           |\n",
            "| n_updates          | 81            |\n",
            "| policy_entropy     | 0.19297412    |\n",
            "| policy_loss        | -0.0017115818 |\n",
            "| serial_timesteps   | 41472         |\n",
            "| time_elapsed       | 409           |\n",
            "| total_timesteps    | 331776        |\n",
            "| value_loss         | 0.01002349    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014408673  |\n",
            "| clipfrac           | 0.024023438   |\n",
            "| ep_len_mean        | 19.9          |\n",
            "| ep_reward_mean     | 0.945         |\n",
            "| explained_variance | 0.646         |\n",
            "| fps                | 862           |\n",
            "| n_updates          | 82            |\n",
            "| policy_entropy     | 0.19557345    |\n",
            "| policy_loss        | -0.0028110605 |\n",
            "| serial_timesteps   | 41984         |\n",
            "| time_elapsed       | 414           |\n",
            "| total_timesteps    | 335872        |\n",
            "| value_loss         | 0.01059779    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.02323347    |\n",
            "| clipfrac           | 0.050610352   |\n",
            "| ep_len_mean        | 20.2          |\n",
            "| ep_reward_mean     | 0.944         |\n",
            "| explained_variance | 0.634         |\n",
            "| fps                | 902           |\n",
            "| n_updates          | 83            |\n",
            "| policy_entropy     | 0.23003355    |\n",
            "| policy_loss        | -0.0076856776 |\n",
            "| serial_timesteps   | 42496         |\n",
            "| time_elapsed       | 419           |\n",
            "| total_timesteps    | 339968        |\n",
            "| value_loss         | 0.014262617   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 19.60 +/- 0.80\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0029602088 |\n",
            "| clipfrac           | 0.042041015  |\n",
            "| ep_len_mean        | 20.1         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.68         |\n",
            "| fps                | 827          |\n",
            "| n_updates          | 84           |\n",
            "| policy_entropy     | 0.2815444    |\n",
            "| policy_loss        | -0.006295259 |\n",
            "| serial_timesteps   | 43008        |\n",
            "| time_elapsed       | 423          |\n",
            "| total_timesteps    | 344064       |\n",
            "| value_loss         | 0.012425327  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.018118914   |\n",
            "| clipfrac           | 0.06022949    |\n",
            "| ep_len_mean        | 20.5          |\n",
            "| ep_reward_mean     | 0.943         |\n",
            "| explained_variance | 0.686         |\n",
            "| fps                | 894           |\n",
            "| n_updates          | 85            |\n",
            "| policy_entropy     | 0.30209467    |\n",
            "| policy_loss        | -0.0063600442 |\n",
            "| serial_timesteps   | 43520         |\n",
            "| time_elapsed       | 428           |\n",
            "| total_timesteps    | 348160        |\n",
            "| value_loss         | 0.0120561635  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=0.93 +/- 0.02\n",
            "Episode length: 24.40 +/- 8.80\n",
            "-------------------------------------\n",
            "| approxkl           | 0.002724565  |\n",
            "| clipfrac           | 0.050463866  |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.67         |\n",
            "| fps                | 826          |\n",
            "| n_updates          | 86           |\n",
            "| policy_entropy     | 0.31593207   |\n",
            "| policy_loss        | -0.008444939 |\n",
            "| serial_timesteps   | 44032        |\n",
            "| time_elapsed       | 433          |\n",
            "| total_timesteps    | 352256       |\n",
            "| value_loss         | 0.013064672  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0031629957 |\n",
            "| clipfrac           | 0.057128906  |\n",
            "| ep_len_mean        | 20.1         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.675        |\n",
            "| fps                | 890          |\n",
            "| n_updates          | 87           |\n",
            "| policy_entropy     | 0.33769497   |\n",
            "| policy_loss        | -0.01046744  |\n",
            "| serial_timesteps   | 44544        |\n",
            "| time_elapsed       | 438          |\n",
            "| total_timesteps    | 356352       |\n",
            "| value_loss         | 0.012927206  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=0.94 +/- 0.01\n",
            "Episode length: 21.20 +/- 2.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0046524946 |\n",
            "| clipfrac           | 0.07502441   |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.662        |\n",
            "| fps                | 846          |\n",
            "| n_updates          | 88           |\n",
            "| policy_entropy     | 0.3802026    |\n",
            "| policy_loss        | -0.014759565 |\n",
            "| serial_timesteps   | 45056        |\n",
            "| time_elapsed       | 442          |\n",
            "| total_timesteps    | 360448       |\n",
            "| value_loss         | 0.013188219  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.004126302  |\n",
            "| clipfrac           | 0.087524414  |\n",
            "| ep_len_mean        | 20.2         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.664        |\n",
            "| fps                | 886          |\n",
            "| n_updates          | 89           |\n",
            "| policy_entropy     | 0.391326     |\n",
            "| policy_loss        | -0.016866136 |\n",
            "| serial_timesteps   | 45568        |\n",
            "| time_elapsed       | 447          |\n",
            "| total_timesteps    | 364544       |\n",
            "| value_loss         | 0.01319257   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.21781898   |\n",
            "| clipfrac           | 0.23273925   |\n",
            "| ep_len_mean        | 19.6         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.245        |\n",
            "| fps                | 882          |\n",
            "| n_updates          | 90           |\n",
            "| policy_entropy     | 0.50291747   |\n",
            "| policy_loss        | -0.020034479 |\n",
            "| serial_timesteps   | 46080        |\n",
            "| time_elapsed       | 452          |\n",
            "| total_timesteps    | 368640       |\n",
            "| value_loss         | 0.023043495  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008585982  |\n",
            "| clipfrac           | 0.10288086   |\n",
            "| ep_len_mean        | 49.3         |\n",
            "| ep_reward_mean     | 0.862        |\n",
            "| explained_variance | -0.0192      |\n",
            "| fps                | 537          |\n",
            "| n_updates          | 91           |\n",
            "| policy_entropy     | 1.0145704    |\n",
            "| policy_loss        | -0.004713853 |\n",
            "| serial_timesteps   | 46592        |\n",
            "| time_elapsed       | 457          |\n",
            "| total_timesteps    | 372736       |\n",
            "| value_loss         | 0.17281553   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0058192397 |\n",
            "| clipfrac           | 0.06547852   |\n",
            "| ep_len_mean        | 81.5         |\n",
            "| ep_reward_mean     | 0.772        |\n",
            "| explained_variance | -0.0964      |\n",
            "| fps                | 999          |\n",
            "| n_updates          | 92           |\n",
            "| policy_entropy     | 1.0332844    |\n",
            "| policy_loss        | -0.004358906 |\n",
            "| serial_timesteps   | 47104        |\n",
            "| time_elapsed       | 464          |\n",
            "| total_timesteps    | 376832       |\n",
            "| value_loss         | 0.15509452   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.004715916  |\n",
            "| clipfrac           | 0.055517577  |\n",
            "| ep_len_mean        | 106          |\n",
            "| ep_reward_mean     | 0.702        |\n",
            "| explained_variance | 0.186        |\n",
            "| fps                | 528          |\n",
            "| n_updates          | 93           |\n",
            "| policy_entropy     | 1.0810697    |\n",
            "| policy_loss        | -0.004207507 |\n",
            "| serial_timesteps   | 47616        |\n",
            "| time_elapsed       | 468          |\n",
            "| total_timesteps    | 380928       |\n",
            "| value_loss         | 0.12378265   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008585415  |\n",
            "| clipfrac           | 0.088696286  |\n",
            "| ep_len_mean        | 77.7         |\n",
            "| ep_reward_mean     | 0.782        |\n",
            "| explained_variance | 0.34         |\n",
            "| fps                | 975          |\n",
            "| n_updates          | 94           |\n",
            "| policy_entropy     | 1.0807304    |\n",
            "| policy_loss        | -0.009466766 |\n",
            "| serial_timesteps   | 48128        |\n",
            "| time_elapsed       | 476          |\n",
            "| total_timesteps    | 385024       |\n",
            "| value_loss         | 0.089308575  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026065994  |\n",
            "| clipfrac           | 0.16374512   |\n",
            "| ep_len_mean        | 54.6         |\n",
            "| ep_reward_mean     | 0.848        |\n",
            "| explained_variance | 0.497        |\n",
            "| fps                | 963          |\n",
            "| n_updates          | 95           |\n",
            "| policy_entropy     | 1.055746     |\n",
            "| policy_loss        | -0.017171318 |\n",
            "| serial_timesteps   | 48640        |\n",
            "| time_elapsed       | 480          |\n",
            "| total_timesteps    | 389120       |\n",
            "| value_loss         | 0.08015914   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018964436  |\n",
            "| clipfrac           | 0.19389649   |\n",
            "| ep_len_mean        | 29.4         |\n",
            "| ep_reward_mean     | 0.918        |\n",
            "| explained_variance | 0.542        |\n",
            "| fps                | 510          |\n",
            "| n_updates          | 96           |\n",
            "| policy_entropy     | 0.8080322    |\n",
            "| policy_loss        | -0.026485214 |\n",
            "| serial_timesteps   | 49152        |\n",
            "| time_elapsed       | 484          |\n",
            "| total_timesteps    | 393216       |\n",
            "| value_loss         | 0.086290635  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.017711587 |\n",
            "| clipfrac           | 0.23208007  |\n",
            "| ep_len_mean        | 27.7        |\n",
            "| ep_reward_mean     | 0.923       |\n",
            "| explained_variance | 0.717       |\n",
            "| fps                | 916         |\n",
            "| n_updates          | 97          |\n",
            "| policy_entropy     | 0.7156056   |\n",
            "| policy_loss        | -0.02902845 |\n",
            "| serial_timesteps   | 49664       |\n",
            "| time_elapsed       | 492         |\n",
            "| total_timesteps    | 397312      |\n",
            "| value_loss         | 0.0509157   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009667987  |\n",
            "| clipfrac           | 0.14375      |\n",
            "| ep_len_mean        | 24.6         |\n",
            "| ep_reward_mean     | 0.932        |\n",
            "| explained_variance | 0.748        |\n",
            "| fps                | 505          |\n",
            "| n_updates          | 98           |\n",
            "| policy_entropy     | 0.65787673   |\n",
            "| policy_loss        | -0.026511952 |\n",
            "| serial_timesteps   | 50176        |\n",
            "| time_elapsed       | 497          |\n",
            "| total_timesteps    | 401408       |\n",
            "| value_loss         | 0.03511734   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0066755763 |\n",
            "| clipfrac           | 0.11340332   |\n",
            "| ep_len_mean        | 25           |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.675        |\n",
            "| fps                | 904          |\n",
            "| n_updates          | 99           |\n",
            "| policy_entropy     | 0.6093563    |\n",
            "| policy_loss        | -0.020145537 |\n",
            "| serial_timesteps   | 50688        |\n",
            "| time_elapsed       | 505          |\n",
            "| total_timesteps    | 405504       |\n",
            "| value_loss         | 0.030470341  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0075268755 |\n",
            "| clipfrac           | 0.10554199   |\n",
            "| ep_len_mean        | 20.7         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.706        |\n",
            "| fps                | 908          |\n",
            "| n_updates          | 100          |\n",
            "| policy_entropy     | 0.49560338   |\n",
            "| policy_loss        | -0.019970847 |\n",
            "| serial_timesteps   | 51200        |\n",
            "| time_elapsed       | 510          |\n",
            "| total_timesteps    | 409600       |\n",
            "| value_loss         | 0.014999164  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.005147328  |\n",
            "| clipfrac           | 0.07319336   |\n",
            "| ep_len_mean        | 19.7         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.75         |\n",
            "| fps                | 849          |\n",
            "| n_updates          | 101          |\n",
            "| policy_entropy     | 0.40337253   |\n",
            "| policy_loss        | -0.020437974 |\n",
            "| serial_timesteps   | 51712        |\n",
            "| time_elapsed       | 514          |\n",
            "| total_timesteps    | 413696       |\n",
            "| value_loss         | 0.0096979905 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0066655404 |\n",
            "| clipfrac           | 0.057714842  |\n",
            "| ep_len_mean        | 18.6         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.83         |\n",
            "| fps                | 879          |\n",
            "| n_updates          | 102          |\n",
            "| policy_entropy     | 0.32054824   |\n",
            "| policy_loss        | -0.01893218  |\n",
            "| serial_timesteps   | 52224        |\n",
            "| time_elapsed       | 519          |\n",
            "| total_timesteps    | 417792       |\n",
            "| value_loss         | 0.0055104503 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0034853138 |\n",
            "| clipfrac           | 0.08549805   |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.85         |\n",
            "| fps                | 837          |\n",
            "| n_updates          | 103          |\n",
            "| policy_entropy     | 0.23862132   |\n",
            "| policy_loss        | -0.015610479 |\n",
            "| serial_timesteps   | 52736        |\n",
            "| time_elapsed       | 524          |\n",
            "| total_timesteps    | 421888       |\n",
            "| value_loss         | 0.0043931017 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.006070126  |\n",
            "| clipfrac           | 0.08532715   |\n",
            "| ep_len_mean        | 16.9         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.845        |\n",
            "| fps                | 874          |\n",
            "| n_updates          | 104          |\n",
            "| policy_entropy     | 0.14536221   |\n",
            "| policy_loss        | -0.010485368 |\n",
            "| serial_timesteps   | 53248        |\n",
            "| time_elapsed       | 528          |\n",
            "| total_timesteps    | 425984       |\n",
            "| value_loss         | 0.0040582865 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.016757017   |\n",
            "| clipfrac           | 0.06794433    |\n",
            "| ep_len_mean        | 16.4          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.846         |\n",
            "| fps                | 823           |\n",
            "| n_updates          | 105           |\n",
            "| policy_entropy     | 0.066529855   |\n",
            "| policy_loss        | -0.0061719073 |\n",
            "| serial_timesteps   | 53760         |\n",
            "| time_elapsed       | 533           |\n",
            "| total_timesteps    | 430080        |\n",
            "| value_loss         | 0.0039153406  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0014688532   |\n",
            "| clipfrac           | 0.006665039    |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.847          |\n",
            "| fps                | 846            |\n",
            "| n_updates          | 106            |\n",
            "| policy_entropy     | 0.049298346    |\n",
            "| policy_loss        | -0.00095796026 |\n",
            "| serial_timesteps   | 54272          |\n",
            "| time_elapsed       | 538            |\n",
            "| total_timesteps    | 434176         |\n",
            "| value_loss         | 0.00373927     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0003831166 |\n",
            "| clipfrac           | 0.0068359375 |\n",
            "| ep_len_mean        | 16.2         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.845        |\n",
            "| fps                | 846          |\n",
            "| n_updates          | 107          |\n",
            "| policy_entropy     | 0.03747843   |\n",
            "| policy_loss        | -0.001661879 |\n",
            "| serial_timesteps   | 54784        |\n",
            "| time_elapsed       | 543          |\n",
            "| total_timesteps    | 438272       |\n",
            "| value_loss         | 0.0037296969 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00026844846 |\n",
            "| clipfrac           | 0.0032958984  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.845         |\n",
            "| fps                | 814           |\n",
            "| n_updates          | 108           |\n",
            "| policy_entropy     | 0.029267848   |\n",
            "| policy_loss        | -0.0008663073 |\n",
            "| serial_timesteps   | 55296         |\n",
            "| time_elapsed       | 548           |\n",
            "| total_timesteps    | 442368        |\n",
            "| value_loss         | 0.0036047944  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0002764223  |\n",
            "| clipfrac           | 0.0045410157  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.848         |\n",
            "| fps                | 848           |\n",
            "| n_updates          | 109           |\n",
            "| policy_entropy     | 0.022080265   |\n",
            "| policy_loss        | -0.0012519669 |\n",
            "| serial_timesteps   | 55808         |\n",
            "| time_elapsed       | 553           |\n",
            "| total_timesteps    | 446464        |\n",
            "| value_loss         | 0.0035480137  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00018486875  |\n",
            "| clipfrac           | 0.0028564453   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 823            |\n",
            "| n_updates          | 110            |\n",
            "| policy_entropy     | 0.017234907    |\n",
            "| policy_loss        | -0.00066514115 |\n",
            "| serial_timesteps   | 56320          |\n",
            "| time_elapsed       | 558            |\n",
            "| total_timesteps    | 450560         |\n",
            "| value_loss         | 0.0035550478   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00012975986 |\n",
            "| clipfrac           | 0.0020263672  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.847         |\n",
            "| fps                | 837           |\n",
            "| n_updates          | 111           |\n",
            "| policy_entropy     | 0.013705099   |\n",
            "| policy_loss        | -0.0005186564 |\n",
            "| serial_timesteps   | 56832         |\n",
            "| time_elapsed       | 563           |\n",
            "| total_timesteps    | 454656        |\n",
            "| value_loss         | 0.0035334541  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.9620587e-05  |\n",
            "| clipfrac           | 0.00061035156  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 834            |\n",
            "| n_updates          | 112            |\n",
            "| policy_entropy     | 0.011372144    |\n",
            "| policy_loss        | -0.00025912627 |\n",
            "| serial_timesteps   | 57344          |\n",
            "| time_elapsed       | 568            |\n",
            "| total_timesteps    | 458752         |\n",
            "| value_loss         | 0.0034861206   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 6.5315064e-05  |\n",
            "| clipfrac           | 0.001171875    |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.848          |\n",
            "| fps                | 817            |\n",
            "| n_updates          | 113            |\n",
            "| policy_entropy     | 0.009644085    |\n",
            "| policy_loss        | -0.00032928545 |\n",
            "| serial_timesteps   | 57856          |\n",
            "| time_elapsed       | 572            |\n",
            "| total_timesteps    | 462848         |\n",
            "| value_loss         | 0.0034498845   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.000108573455 |\n",
            "| clipfrac           | 0.0017089844   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 851            |\n",
            "| n_updates          | 114            |\n",
            "| policy_entropy     | 0.010858931    |\n",
            "| policy_loss        | -0.0005135162  |\n",
            "| serial_timesteps   | 58368          |\n",
            "| time_elapsed       | 577            |\n",
            "| total_timesteps    | 466944         |\n",
            "| value_loss         | 0.0034289074   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 7.6440774e-05 |\n",
            "| clipfrac           | 0.0011962891  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.847         |\n",
            "| fps                | 818           |\n",
            "| n_updates          | 115           |\n",
            "| policy_entropy     | 0.010458342   |\n",
            "| policy_loss        | -0.000575492  |\n",
            "| serial_timesteps   | 58880         |\n",
            "| time_elapsed       | 582           |\n",
            "| total_timesteps    | 471040        |\n",
            "| value_loss         | 0.0034274987  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 5.1941322e-05  |\n",
            "| clipfrac           | 0.0007568359   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 864            |\n",
            "| n_updates          | 116            |\n",
            "| policy_entropy     | 0.009979272    |\n",
            "| policy_loss        | -2.0368984e-06 |\n",
            "| serial_timesteps   | 59392          |\n",
            "| time_elapsed       | 587            |\n",
            "| total_timesteps    | 475136         |\n",
            "| value_loss         | 0.0034200964   |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| approxkl           | 8.054946e-05    |\n",
            "| clipfrac           | 0.0010498047    |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.956           |\n",
            "| explained_variance | 0.85            |\n",
            "| fps                | 866             |\n",
            "| n_updates          | 117             |\n",
            "| policy_entropy     | 0.008836601     |\n",
            "| policy_loss        | -0.000103843515 |\n",
            "| serial_timesteps   | 59904           |\n",
            "| time_elapsed       | 592             |\n",
            "| total_timesteps    | 479232          |\n",
            "| value_loss         | 0.0033613455    |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 7.4978176e-05  |\n",
            "| clipfrac           | 0.0010498047   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 838            |\n",
            "| n_updates          | 118            |\n",
            "| policy_entropy     | 0.010606813    |\n",
            "| policy_loss        | -0.00022900058 |\n",
            "| serial_timesteps   | 60416          |\n",
            "| time_elapsed       | 597            |\n",
            "| total_timesteps    | 483328         |\n",
            "| value_loss         | 0.003326369    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00015331584 |\n",
            "| clipfrac           | 0.0013916015  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.851         |\n",
            "| fps                | 862           |\n",
            "| n_updates          | 119           |\n",
            "| policy_entropy     | 0.008634147   |\n",
            "| policy_loss        | -0.0002511907 |\n",
            "| serial_timesteps   | 60928         |\n",
            "| time_elapsed       | 602           |\n",
            "| total_timesteps    | 487424        |\n",
            "| value_loss         | 0.003317846   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000349162   |\n",
            "| clipfrac           | 0.0020996095  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.847         |\n",
            "| fps                | 840           |\n",
            "| n_updates          | 120           |\n",
            "| policy_entropy     | 0.010069667   |\n",
            "| policy_loss        | -0.0006591465 |\n",
            "| serial_timesteps   | 61440         |\n",
            "| time_elapsed       | 606           |\n",
            "| total_timesteps    | 491520        |\n",
            "| value_loss         | 0.0034954704  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 9.475164e-05   |\n",
            "| clipfrac           | 0.0010009765   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 862            |\n",
            "| n_updates          | 121            |\n",
            "| policy_entropy     | 0.007165647    |\n",
            "| policy_loss        | -0.00033253778 |\n",
            "| serial_timesteps   | 61952          |\n",
            "| time_elapsed       | 611            |\n",
            "| total_timesteps    | 495616         |\n",
            "| value_loss         | 0.0033461873   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 8.139561e-06  |\n",
            "| clipfrac           | 0.0001953125  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.85          |\n",
            "| fps                | 853           |\n",
            "| n_updates          | 122           |\n",
            "| policy_entropy     | 0.006256698   |\n",
            "| policy_loss        | -3.214507e-05 |\n",
            "| serial_timesteps   | 62464         |\n",
            "| time_elapsed       | 616           |\n",
            "| total_timesteps    | 499712        |\n",
            "| value_loss         | 0.0033002351  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 4.3761516e-05  |\n",
            "| clipfrac           | 0.00041503907  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.839          |\n",
            "| fps                | 828            |\n",
            "| n_updates          | 123            |\n",
            "| policy_entropy     | 0.008510524    |\n",
            "| policy_loss        | -0.00010934107 |\n",
            "| serial_timesteps   | 62976          |\n",
            "| time_elapsed       | 621            |\n",
            "| total_timesteps    | 503808         |\n",
            "| value_loss         | 0.0033403034   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00018637022 |\n",
            "| clipfrac           | 0.0013671875  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.85          |\n",
            "| fps                | 848           |\n",
            "| n_updates          | 124           |\n",
            "| policy_entropy     | 0.008590483   |\n",
            "| policy_loss        | -0.0006264037 |\n",
            "| serial_timesteps   | 63488         |\n",
            "| time_elapsed       | 626           |\n",
            "| total_timesteps    | 507904        |\n",
            "| value_loss         | 0.003316875   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00018448249  |\n",
            "| clipfrac           | 0.0010253906   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 817            |\n",
            "| n_updates          | 125            |\n",
            "| policy_entropy     | 0.005906941    |\n",
            "| policy_loss        | -0.00033817123 |\n",
            "| serial_timesteps   | 64000          |\n",
            "| time_elapsed       | 631            |\n",
            "| total_timesteps    | 512000         |\n",
            "| value_loss         | 0.0033404299   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00061650004 |\n",
            "| clipfrac           | 0.0021240234  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.832         |\n",
            "| fps                | 851           |\n",
            "| n_updates          | 126           |\n",
            "| policy_entropy     | 0.009848321   |\n",
            "| policy_loss        | -0.0009137622 |\n",
            "| serial_timesteps   | 64512         |\n",
            "| time_elapsed       | 636           |\n",
            "| total_timesteps    | 516096        |\n",
            "| value_loss         | 0.003632497   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00012018661  |\n",
            "| clipfrac           | 0.00061035156  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 851            |\n",
            "| n_updates          | 127            |\n",
            "| policy_entropy     | 0.004477099    |\n",
            "| policy_loss        | -0.00023043474 |\n",
            "| serial_timesteps   | 65024          |\n",
            "| time_elapsed       | 640            |\n",
            "| total_timesteps    | 520192         |\n",
            "| value_loss         | 0.0032493789   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 9.012146e-06   |\n",
            "| clipfrac           | 0.00017089843  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 872            |\n",
            "| n_updates          | 128            |\n",
            "| policy_entropy     | 0.004789102    |\n",
            "| policy_loss        | -3.8969185e-05 |\n",
            "| serial_timesteps   | 65536          |\n",
            "| time_elapsed       | 645            |\n",
            "| total_timesteps    | 524288         |\n",
            "| value_loss         | 0.0032290393   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00016432835  |\n",
            "| clipfrac           | 0.0016357421   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 872            |\n",
            "| n_updates          | 129            |\n",
            "| policy_entropy     | 0.006582967    |\n",
            "| policy_loss        | -0.00016507498 |\n",
            "| serial_timesteps   | 66048          |\n",
            "| time_elapsed       | 650            |\n",
            "| total_timesteps    | 528384         |\n",
            "| value_loss         | 0.0032930833   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 9.362539e-05  |\n",
            "| clipfrac           | 0.0008544922  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.852         |\n",
            "| fps                | 839           |\n",
            "| n_updates          | 130           |\n",
            "| policy_entropy     | 0.0073745092  |\n",
            "| policy_loss        | -0.0001453564 |\n",
            "| serial_timesteps   | 66560         |\n",
            "| time_elapsed       | 655           |\n",
            "| total_timesteps    | 532480        |\n",
            "| value_loss         | 0.0032124924  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00019268862  |\n",
            "| clipfrac           | 0.00080566405  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 868            |\n",
            "| n_updates          | 131            |\n",
            "| policy_entropy     | 0.00448291     |\n",
            "| policy_loss        | -0.00025677012 |\n",
            "| serial_timesteps   | 67072          |\n",
            "| time_elapsed       | 659            |\n",
            "| total_timesteps    | 536576         |\n",
            "| value_loss         | 0.003204116    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0038003635  |\n",
            "| clipfrac           | 0.027636718   |\n",
            "| ep_len_mean        | 16.9          |\n",
            "| ep_reward_mean     | 0.953         |\n",
            "| explained_variance | 0.475         |\n",
            "| fps                | 848           |\n",
            "| n_updates          | 132           |\n",
            "| policy_entropy     | 0.044428818   |\n",
            "| policy_loss        | -0.0019800228 |\n",
            "| serial_timesteps   | 67584         |\n",
            "| time_elapsed       | 664           |\n",
            "| total_timesteps    | 540672        |\n",
            "| value_loss         | 0.006455294   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0050706407  |\n",
            "| clipfrac           | 0.030273438   |\n",
            "| ep_len_mean        | 16.6          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.566         |\n",
            "| fps                | 886           |\n",
            "| n_updates          | 133           |\n",
            "| policy_entropy     | 0.066367075   |\n",
            "| policy_loss        | -0.0035632972 |\n",
            "| serial_timesteps   | 68096         |\n",
            "| time_elapsed       | 669           |\n",
            "| total_timesteps    | 544768        |\n",
            "| value_loss         | 0.005717191   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0009422777  |\n",
            "| clipfrac           | 0.0109375     |\n",
            "| ep_len_mean        | 16.3          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.764         |\n",
            "| fps                | 851           |\n",
            "| n_updates          | 134           |\n",
            "| policy_entropy     | 0.043360896   |\n",
            "| policy_loss        | -0.0016057221 |\n",
            "| serial_timesteps   | 68608         |\n",
            "| time_elapsed       | 674           |\n",
            "| total_timesteps    | 548864        |\n",
            "| value_loss         | 0.004978285   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00037215056  |\n",
            "| clipfrac           | 0.005053711    |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.779          |\n",
            "| fps                | 826            |\n",
            "| n_updates          | 135            |\n",
            "| policy_entropy     | 0.034647085    |\n",
            "| policy_loss        | -0.00057374575 |\n",
            "| serial_timesteps   | 69120          |\n",
            "| time_elapsed       | 678            |\n",
            "| total_timesteps    | 552960         |\n",
            "| value_loss         | 0.004947778    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00013662234 |\n",
            "| clipfrac           | 0.0019042969  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.781         |\n",
            "| fps                | 858           |\n",
            "| n_updates          | 136           |\n",
            "| policy_entropy     | 0.03525693    |\n",
            "| policy_loss        | 0.0001586586  |\n",
            "| serial_timesteps   | 69632         |\n",
            "| time_elapsed       | 683           |\n",
            "| total_timesteps    | 557056        |\n",
            "| value_loss         | 0.0048710424  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00086621783 |\n",
            "| clipfrac           | 0.009033203   |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.772         |\n",
            "| fps                | 838           |\n",
            "| n_updates          | 137           |\n",
            "| policy_entropy     | 0.041999914   |\n",
            "| policy_loss        | -0.0020110386 |\n",
            "| serial_timesteps   | 70144         |\n",
            "| time_elapsed       | 688           |\n",
            "| total_timesteps    | 561152        |\n",
            "| value_loss         | 0.0051138033  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00022998848 |\n",
            "| clipfrac           | 0.0041015623  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.779         |\n",
            "| fps                | 873           |\n",
            "| n_updates          | 138           |\n",
            "| policy_entropy     | 0.029309308   |\n",
            "| policy_loss        | -0.0005273737 |\n",
            "| serial_timesteps   | 70656         |\n",
            "| time_elapsed       | 693           |\n",
            "| total_timesteps    | 565248        |\n",
            "| value_loss         | 0.0049549346  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00024720104  |\n",
            "| clipfrac           | 0.0036376952   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.779          |\n",
            "| fps                | 875            |\n",
            "| n_updates          | 139            |\n",
            "| policy_entropy     | 0.025278423    |\n",
            "| policy_loss        | -0.00057222304 |\n",
            "| serial_timesteps   | 71168          |\n",
            "| time_elapsed       | 698            |\n",
            "| total_timesteps    | 569344         |\n",
            "| value_loss         | 0.0048838113   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0002578787   |\n",
            "| clipfrac           | 0.0041503906   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 834            |\n",
            "| n_updates          | 140            |\n",
            "| policy_entropy     | 0.021288358    |\n",
            "| policy_loss        | -0.00086612545 |\n",
            "| serial_timesteps   | 71680          |\n",
            "| time_elapsed       | 702            |\n",
            "| total_timesteps    | 573440         |\n",
            "| value_loss         | 0.004766332    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00019001121  |\n",
            "| clipfrac           | 0.003540039    |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 881            |\n",
            "| n_updates          | 141            |\n",
            "| policy_entropy     | 0.018278759    |\n",
            "| policy_loss        | -0.00071002916 |\n",
            "| serial_timesteps   | 72192          |\n",
            "| time_elapsed       | 707            |\n",
            "| total_timesteps    | 577536         |\n",
            "| value_loss         | 0.004807549    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0001502931  |\n",
            "| clipfrac           | 0.0025146485  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.781         |\n",
            "| fps                | 843           |\n",
            "| n_updates          | 142           |\n",
            "| policy_entropy     | 0.015366408   |\n",
            "| policy_loss        | -0.0004477432 |\n",
            "| serial_timesteps   | 72704         |\n",
            "| time_elapsed       | 712           |\n",
            "| total_timesteps    | 581632        |\n",
            "| value_loss         | 0.0046949075  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0001142877   |\n",
            "| clipfrac           | 0.0018066406   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 860            |\n",
            "| n_updates          | 143            |\n",
            "| policy_entropy     | 0.012325372    |\n",
            "| policy_loss        | -0.00040824077 |\n",
            "| serial_timesteps   | 73216          |\n",
            "| time_elapsed       | 717            |\n",
            "| total_timesteps    | 585728         |\n",
            "| value_loss         | 0.004776976    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 6.0664635e-05  |\n",
            "| clipfrac           | 0.0010253906   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 861            |\n",
            "| n_updates          | 144            |\n",
            "| policy_entropy     | 0.010585937    |\n",
            "| policy_loss        | -0.00032485952 |\n",
            "| serial_timesteps   | 73728          |\n",
            "| time_elapsed       | 722            |\n",
            "| total_timesteps    | 589824         |\n",
            "| value_loss         | 0.004613049    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 6.714617e-05   |\n",
            "| clipfrac           | 0.00095214846  |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 824            |\n",
            "| n_updates          | 145            |\n",
            "| policy_entropy     | 0.009333508    |\n",
            "| policy_loss        | -4.4696055e-05 |\n",
            "| serial_timesteps   | 74240          |\n",
            "| time_elapsed       | 726            |\n",
            "| total_timesteps    | 593920         |\n",
            "| value_loss         | 0.0046640416   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.163467e-05   |\n",
            "| clipfrac           | 0.00041503907  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 860            |\n",
            "| n_updates          | 146            |\n",
            "| policy_entropy     | 0.008061717    |\n",
            "| policy_loss        | -3.6688492e-05 |\n",
            "| serial_timesteps   | 74752          |\n",
            "| time_elapsed       | 731            |\n",
            "| total_timesteps    | 598016         |\n",
            "| value_loss         | 0.004694362    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014933617  |\n",
            "| clipfrac           | 0.0017333984  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 829           |\n",
            "| n_updates          | 147           |\n",
            "| policy_entropy     | 0.0077831573  |\n",
            "| policy_loss        | -0.0004776197 |\n",
            "| serial_timesteps   | 75264         |\n",
            "| time_elapsed       | 736           |\n",
            "| total_timesteps    | 602112        |\n",
            "| value_loss         | 0.0046426663  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.1324606e-08  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 869            |\n",
            "| n_updates          | 148            |\n",
            "| policy_entropy     | 0.005843126    |\n",
            "| policy_loss        | -1.9212662e-06 |\n",
            "| serial_timesteps   | 75776          |\n",
            "| time_elapsed       | 741            |\n",
            "| total_timesteps    | 606208         |\n",
            "| value_loss         | 0.0046468885   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 3.2535503e-05  |\n",
            "| clipfrac           | 0.000390625    |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 848            |\n",
            "| n_updates          | 149            |\n",
            "| policy_entropy     | 0.004974315    |\n",
            "| policy_loss        | -3.6218382e-05 |\n",
            "| serial_timesteps   | 76288          |\n",
            "| time_elapsed       | 746            |\n",
            "| total_timesteps    | 610304         |\n",
            "| value_loss         | 0.0046004793   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0007128435   |\n",
            "| clipfrac           | 0.0013671875   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 875            |\n",
            "| n_updates          | 150            |\n",
            "| policy_entropy     | 0.0042960094   |\n",
            "| policy_loss        | -0.00020876131 |\n",
            "| serial_timesteps   | 76800          |\n",
            "| time_elapsed       | 751            |\n",
            "| total_timesteps    | 614400         |\n",
            "| value_loss         | 0.0046322267   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.8621435e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 853            |\n",
            "| n_updates          | 151            |\n",
            "| policy_entropy     | 0.0032138457   |\n",
            "| policy_loss        | -0.00011910191 |\n",
            "| serial_timesteps   | 77312          |\n",
            "| time_elapsed       | 755            |\n",
            "| total_timesteps    | 618496         |\n",
            "| value_loss         | 0.0046426826   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 1.4389918e-05  |\n",
            "| clipfrac           | 0.0001953125   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 844            |\n",
            "| n_updates          | 152            |\n",
            "| policy_entropy     | 0.003992427    |\n",
            "| policy_loss        | -4.5436966e-05 |\n",
            "| serial_timesteps   | 77824          |\n",
            "| time_elapsed       | 760            |\n",
            "| total_timesteps    | 622592         |\n",
            "| value_loss         | 0.0045960248   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.3685416e-05 |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 876           |\n",
            "| n_updates          | 153           |\n",
            "| policy_entropy     | 0.003437215   |\n",
            "| policy_loss        | -8.952459e-05 |\n",
            "| serial_timesteps   | 78336         |\n",
            "| time_elapsed       | 765           |\n",
            "| total_timesteps    | 626688        |\n",
            "| value_loss         | 0.0045336685  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 3.600213e-05   |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 847            |\n",
            "| n_updates          | 154            |\n",
            "| policy_entropy     | 0.002747849    |\n",
            "| policy_loss        | -0.00012544771 |\n",
            "| serial_timesteps   | 78848          |\n",
            "| time_elapsed       | 770            |\n",
            "| total_timesteps    | 630784         |\n",
            "| value_loss         | 0.004567104    |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 1.598462e-11 |\n",
            "| clipfrac           | 0.0          |\n",
            "| ep_len_mean        | 16           |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.785        |\n",
            "| fps                | 880          |\n",
            "| n_updates          | 155          |\n",
            "| policy_entropy     | 0.002810621  |\n",
            "| policy_loss        | 8.526549e-08 |\n",
            "| serial_timesteps   | 79360        |\n",
            "| time_elapsed       | 774          |\n",
            "| total_timesteps    | 634880       |\n",
            "| value_loss         | 0.0044609546 |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.9291303e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 875            |\n",
            "| n_updates          | 156            |\n",
            "| policy_entropy     | 0.002579324    |\n",
            "| policy_loss        | -3.1826705e-05 |\n",
            "| serial_timesteps   | 79872          |\n",
            "| time_elapsed       | 779            |\n",
            "| total_timesteps    | 638976         |\n",
            "| value_loss         | 0.004431036    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 7.952268e-05  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 828           |\n",
            "| n_updates          | 157           |\n",
            "| policy_entropy     | 0.0017325729  |\n",
            "| policy_loss        | -9.388327e-05 |\n",
            "| serial_timesteps   | 80384         |\n",
            "| time_elapsed       | 784           |\n",
            "| total_timesteps    | 643072        |\n",
            "| value_loss         | 0.004492061   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 5.0316896e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 866            |\n",
            "| n_updates          | 158            |\n",
            "| policy_entropy     | 0.0016995402   |\n",
            "| policy_loss        | -1.4382822e-07 |\n",
            "| serial_timesteps   | 80896          |\n",
            "| time_elapsed       | 789            |\n",
            "| total_timesteps    | 647168         |\n",
            "| value_loss         | 0.004482817    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 6.95507e-05   |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 834           |\n",
            "| n_updates          | 159           |\n",
            "| policy_entropy     | 0.0014680589  |\n",
            "| policy_loss        | -9.434707e-05 |\n",
            "| serial_timesteps   | 81408         |\n",
            "| time_elapsed       | 793           |\n",
            "| total_timesteps    | 651264        |\n",
            "| value_loss         | 0.004515099   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 8.236471e-05  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 877           |\n",
            "| n_updates          | 160           |\n",
            "| policy_entropy     | 0.0012793539  |\n",
            "| policy_loss        | -9.382054e-05 |\n",
            "| serial_timesteps   | 81920         |\n",
            "| time_elapsed       | 798           |\n",
            "| total_timesteps    | 655360        |\n",
            "| value_loss         | 0.0044219317  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.3853597e-12 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 858           |\n",
            "| n_updates          | 161           |\n",
            "| policy_entropy     | 0.0012635865  |\n",
            "| policy_loss        | 3.1417584e-09 |\n",
            "| serial_timesteps   | 82432         |\n",
            "| time_elapsed       | 803           |\n",
            "| total_timesteps    | 659456        |\n",
            "| value_loss         | 0.0044023506  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 2.0593635e-12 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.786         |\n",
            "| fps                | 834           |\n",
            "| n_updates          | 162           |\n",
            "| policy_entropy     | 0.0012480692  |\n",
            "| policy_loss        | 1.7879938e-08 |\n",
            "| serial_timesteps   | 82944         |\n",
            "| time_elapsed       | 808           |\n",
            "| total_timesteps    | 663552        |\n",
            "| value_loss         | 0.004391271   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0001085289  |\n",
            "| clipfrac           | 0.00061035156 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 847           |\n",
            "| n_updates          | 163           |\n",
            "| policy_entropy     | 0.0019856472  |\n",
            "| policy_loss        | -9.155147e-05 |\n",
            "| serial_timesteps   | 83456         |\n",
            "| time_elapsed       | 813           |\n",
            "| total_timesteps    | 667648        |\n",
            "| value_loss         | 0.004475407   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| approxkl           | 0.0005446735    |\n",
            "| clipfrac           | 0.00029296876   |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.956           |\n",
            "| explained_variance | 0.785           |\n",
            "| fps                | 825             |\n",
            "| n_updates          | 164             |\n",
            "| policy_entropy     | 0.0017596167    |\n",
            "| policy_loss        | -0.000111932204 |\n",
            "| serial_timesteps   | 83968           |\n",
            "| time_elapsed       | 817             |\n",
            "| total_timesteps    | 671744          |\n",
            "| value_loss         | 0.004460321     |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.4399666e-11 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 854           |\n",
            "| n_updates          | 165           |\n",
            "| policy_entropy     | 0.0013950428  |\n",
            "| policy_loss        | 4.528265e-08  |\n",
            "| serial_timesteps   | 84480         |\n",
            "| time_elapsed       | 822           |\n",
            "| total_timesteps    | 675840        |\n",
            "| value_loss         | 0.004387788   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 8.0673396e-10 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 859           |\n",
            "| n_updates          | 166           |\n",
            "| policy_entropy     | 0.001314054   |\n",
            "| policy_loss        | -5.690752e-07 |\n",
            "| serial_timesteps   | 84992         |\n",
            "| time_elapsed       | 827           |\n",
            "| total_timesteps    | 679936        |\n",
            "| value_loss         | 0.0044369446  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 2.4794899e-11 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 835           |\n",
            "| n_updates          | 167           |\n",
            "| policy_entropy     | 0.0012461245  |\n",
            "| policy_loss        | 4.543399e-08  |\n",
            "| serial_timesteps   | 85504         |\n",
            "| time_elapsed       | 832           |\n",
            "| total_timesteps    | 684032        |\n",
            "| value_loss         | 0.004435416   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0028801374 |\n",
            "| clipfrac           | 0.0011962891 |\n",
            "| ep_len_mean        | 16           |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.783        |\n",
            "| fps                | 879          |\n",
            "| n_updates          | 168          |\n",
            "| policy_entropy     | 0.0029915203 |\n",
            "| policy_loss        | 0.0005100408 |\n",
            "| serial_timesteps   | 86016        |\n",
            "| time_elapsed       | 837          |\n",
            "| total_timesteps    | 688128       |\n",
            "| value_loss         | 0.0044312114 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 1.6927843e-05  |\n",
            "| clipfrac           | 0.00017089843  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.804          |\n",
            "| fps                | 842            |\n",
            "| n_updates          | 169            |\n",
            "| policy_entropy     | 0.005741422    |\n",
            "| policy_loss        | -2.1459298e-05 |\n",
            "| serial_timesteps   | 86528          |\n",
            "| time_elapsed       | 842            |\n",
            "| total_timesteps    | 692224         |\n",
            "| value_loss         | 0.0034621395   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0006249731   |\n",
            "| clipfrac           | 0.0040527345   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.848          |\n",
            "| fps                | 878            |\n",
            "| n_updates          | 170            |\n",
            "| policy_entropy     | 0.027016679    |\n",
            "| policy_loss        | -0.00082706567 |\n",
            "| serial_timesteps   | 87040          |\n",
            "| time_elapsed       | 846            |\n",
            "| total_timesteps    | 696320         |\n",
            "| value_loss         | 0.0030401608   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000920021   |\n",
            "| clipfrac           | 0.010205078   |\n",
            "| ep_len_mean        | 16.3          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.836         |\n",
            "| fps                | 845           |\n",
            "| n_updates          | 171           |\n",
            "| policy_entropy     | 0.051416326   |\n",
            "| policy_loss        | -0.0011631561 |\n",
            "| serial_timesteps   | 87552         |\n",
            "| time_elapsed       | 851           |\n",
            "| total_timesteps    | 700416        |\n",
            "| value_loss         | 0.0033307243  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0006512264   |\n",
            "| clipfrac           | 0.007470703    |\n",
            "| ep_len_mean        | 16.2           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.836          |\n",
            "| fps                | 881            |\n",
            "| n_updates          | 172            |\n",
            "| policy_entropy     | 0.046293914    |\n",
            "| policy_loss        | -0.00072232244 |\n",
            "| serial_timesteps   | 88064          |\n",
            "| time_elapsed       | 856            |\n",
            "| total_timesteps    | 704512         |\n",
            "| value_loss         | 0.003367       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00039824616 |\n",
            "| clipfrac           | 0.007055664   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.791         |\n",
            "| fps                | 874           |\n",
            "| n_updates          | 173           |\n",
            "| policy_entropy     | 0.03013653    |\n",
            "| policy_loss        | -0.0021274504 |\n",
            "| serial_timesteps   | 88576         |\n",
            "| time_elapsed       | 861           |\n",
            "| total_timesteps    | 708608        |\n",
            "| value_loss         | 0.0036205605  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0002177183   |\n",
            "| clipfrac           | 0.004296875    |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.78           |\n",
            "| fps                | 829            |\n",
            "| n_updates          | 174            |\n",
            "| policy_entropy     | 0.026355037    |\n",
            "| policy_loss        | -0.00067665614 |\n",
            "| serial_timesteps   | 89088          |\n",
            "| time_elapsed       | 865            |\n",
            "| total_timesteps    | 712704         |\n",
            "| value_loss         | 0.0044381623   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00016939385  |\n",
            "| clipfrac           | 0.003076172    |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 873            |\n",
            "| n_updates          | 175            |\n",
            "| policy_entropy     | 0.02463198     |\n",
            "| policy_loss        | -0.00037902195 |\n",
            "| serial_timesteps   | 89600          |\n",
            "| time_elapsed       | 870            |\n",
            "| total_timesteps    | 716800         |\n",
            "| value_loss         | 0.0044742543   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 9.715488e-05  |\n",
            "| clipfrac           | 0.0010986328  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.781         |\n",
            "| fps                | 839           |\n",
            "| n_updates          | 176           |\n",
            "| policy_entropy     | 0.025036827   |\n",
            "| policy_loss        | 0.00015855143 |\n",
            "| serial_timesteps   | 90112         |\n",
            "| time_elapsed       | 875           |\n",
            "| total_timesteps    | 720896        |\n",
            "| value_loss         | 0.004522183   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00013272793  |\n",
            "| clipfrac           | 0.0025390624   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.78           |\n",
            "| fps                | 872            |\n",
            "| n_updates          | 177            |\n",
            "| policy_entropy     | 0.018666323    |\n",
            "| policy_loss        | -0.00047315034 |\n",
            "| serial_timesteps   | 90624          |\n",
            "| time_elapsed       | 880            |\n",
            "| total_timesteps    | 724992         |\n",
            "| value_loss         | 0.0044822088   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00010237907 |\n",
            "| clipfrac           | 0.001953125   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 841           |\n",
            "| n_updates          | 178           |\n",
            "| policy_entropy     | 0.016754335   |\n",
            "| policy_loss        | -0.0002184459 |\n",
            "| serial_timesteps   | 91136         |\n",
            "| time_elapsed       | 884           |\n",
            "| total_timesteps    | 729088        |\n",
            "| value_loss         | 0.0045365826  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00020722812  |\n",
            "| clipfrac           | 0.0029052733   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.78           |\n",
            "| fps                | 829            |\n",
            "| n_updates          | 179            |\n",
            "| policy_entropy     | 0.013158223    |\n",
            "| policy_loss        | -0.00089086156 |\n",
            "| serial_timesteps   | 91648          |\n",
            "| time_elapsed       | 889            |\n",
            "| total_timesteps    | 733184         |\n",
            "| value_loss         | 0.004483459    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 6.478434e-05  |\n",
            "| clipfrac           | 0.0012695312  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 883           |\n",
            "| n_updates          | 180           |\n",
            "| policy_entropy     | 0.01102045    |\n",
            "| policy_loss        | -0.0001203449 |\n",
            "| serial_timesteps   | 92160         |\n",
            "| time_elapsed       | 894           |\n",
            "| total_timesteps    | 737280        |\n",
            "| value_loss         | 0.004355991   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0015558631   |\n",
            "| clipfrac           | 0.021069337    |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 844            |\n",
            "| n_updates          | 181            |\n",
            "| policy_entropy     | 0.029380864    |\n",
            "| policy_loss        | -0.00033086594 |\n",
            "| serial_timesteps   | 92672          |\n",
            "| time_elapsed       | 899            |\n",
            "| total_timesteps    | 741376         |\n",
            "| value_loss         | 0.004084476    |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.124533094  |\n",
            "| clipfrac           | 0.13637695   |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.711        |\n",
            "| fps                | 871          |\n",
            "| n_updates          | 182          |\n",
            "| policy_entropy     | 0.0584198    |\n",
            "| policy_loss        | -0.012645793 |\n",
            "| serial_timesteps   | 93184        |\n",
            "| time_elapsed       | 904          |\n",
            "| total_timesteps    | 745472       |\n",
            "| value_loss         | 0.0059629595 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008620433   |\n",
            "| clipfrac           | 0.003100586   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.842         |\n",
            "| fps                | 866           |\n",
            "| n_updates          | 183           |\n",
            "| policy_entropy     | 0.011564044   |\n",
            "| policy_loss        | -0.0009954406 |\n",
            "| serial_timesteps   | 93696         |\n",
            "| time_elapsed       | 908           |\n",
            "| total_timesteps    | 749568        |\n",
            "| value_loss         | 0.0032501773  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00011273056  |\n",
            "| clipfrac           | 0.0007324219   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 835            |\n",
            "| n_updates          | 184            |\n",
            "| policy_entropy     | 0.0067526074   |\n",
            "| policy_loss        | -0.00016033492 |\n",
            "| serial_timesteps   | 94208          |\n",
            "| time_elapsed       | 913            |\n",
            "| total_timesteps    | 753664         |\n",
            "| value_loss         | 0.0030260985   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 3.616212e-09   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 865            |\n",
            "| n_updates          | 185            |\n",
            "| policy_entropy     | 0.0069192387   |\n",
            "| policy_loss        | -1.5287515e-07 |\n",
            "| serial_timesteps   | 94720          |\n",
            "| time_elapsed       | 918            |\n",
            "| total_timesteps    | 757760         |\n",
            "| value_loss         | 0.0029562102   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003537161  |\n",
            "| clipfrac           | 0.0013427734  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.85          |\n",
            "| fps                | 829           |\n",
            "| n_updates          | 186           |\n",
            "| policy_entropy     | 0.005503011   |\n",
            "| policy_loss        | -0.0003266398 |\n",
            "| serial_timesteps   | 95232         |\n",
            "| time_elapsed       | 923           |\n",
            "| total_timesteps    | 761856        |\n",
            "| value_loss         | 0.0029437658  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003403216  |\n",
            "| clipfrac           | 0.0013183594  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.851         |\n",
            "| fps                | 874           |\n",
            "| n_updates          | 187           |\n",
            "| policy_entropy     | 0.0035217833  |\n",
            "| policy_loss        | -0.0006024268 |\n",
            "| serial_timesteps   | 95744         |\n",
            "| time_elapsed       | 928           |\n",
            "| total_timesteps    | 765952        |\n",
            "| value_loss         | 0.0029238309  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 1.0382098e-05  |\n",
            "| clipfrac           | 0.0001953125   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 835            |\n",
            "| n_updates          | 188            |\n",
            "| policy_entropy     | 0.0029556681   |\n",
            "| policy_loss        | -0.00010936274 |\n",
            "| serial_timesteps   | 96256          |\n",
            "| time_elapsed       | 932            |\n",
            "| total_timesteps    | 770048         |\n",
            "| value_loss         | 0.0029208285   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.2904564e-10  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 875            |\n",
            "| n_updates          | 189            |\n",
            "| policy_entropy     | 0.0026591993   |\n",
            "| policy_loss        | -4.5897286e-07 |\n",
            "| serial_timesteps   | 96768          |\n",
            "| time_elapsed       | 937            |\n",
            "| total_timesteps    | 774144         |\n",
            "| value_loss         | 0.002927533    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00023051907 |\n",
            "| clipfrac           | 0.0010742188  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.847         |\n",
            "| fps                | 873           |\n",
            "| n_updates          | 190           |\n",
            "| policy_entropy     | 0.003559575   |\n",
            "| policy_loss        | -0.0007414165 |\n",
            "| serial_timesteps   | 97280         |\n",
            "| time_elapsed       | 942           |\n",
            "| total_timesteps    | 778240        |\n",
            "| value_loss         | 0.002955738   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| approxkl           | 3.12005e-05     |\n",
            "| clipfrac           | 0.00021972656   |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.956           |\n",
            "| explained_variance | 0.851           |\n",
            "| fps                | 828             |\n",
            "| n_updates          | 191             |\n",
            "| policy_entropy     | 0.0021062475    |\n",
            "| policy_loss        | -0.000109590124 |\n",
            "| serial_timesteps   | 97792           |\n",
            "| time_elapsed       | 947             |\n",
            "| total_timesteps    | 782336          |\n",
            "| value_loss         | 0.0028909657    |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 4.5862293e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.848          |\n",
            "| fps                | 885            |\n",
            "| n_updates          | 192            |\n",
            "| policy_entropy     | 0.0021242083   |\n",
            "| policy_loss        | -3.9270745e-07 |\n",
            "| serial_timesteps   | 98304          |\n",
            "| time_elapsed       | 952            |\n",
            "| total_timesteps    | 786432         |\n",
            "| value_loss         | 0.0028977073   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 5.2317155e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 833            |\n",
            "| n_updates          | 193            |\n",
            "| policy_entropy     | 0.0018265632   |\n",
            "| policy_loss        | -0.00010816308 |\n",
            "| serial_timesteps   | 98816          |\n",
            "| time_elapsed       | 956            |\n",
            "| total_timesteps    | 790528         |\n",
            "| value_loss         | 0.0029244365   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00055751915  |\n",
            "| clipfrac           | 0.00068359374  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 865            |\n",
            "| n_updates          | 194            |\n",
            "| policy_entropy     | 0.0015215422   |\n",
            "| policy_loss        | -0.00011976519 |\n",
            "| serial_timesteps   | 99328          |\n",
            "| time_elapsed       | 961            |\n",
            "| total_timesteps    | 794624         |\n",
            "| value_loss         | 0.0029262896   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.5404898e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 860            |\n",
            "| n_updates          | 195            |\n",
            "| policy_entropy     | 0.001472823    |\n",
            "| policy_loss        | -1.4628094e-07 |\n",
            "| serial_timesteps   | 99840          |\n",
            "| time_elapsed       | 966            |\n",
            "| total_timesteps    | 798720         |\n",
            "| value_loss         | 0.0029165172   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 7.250271e-11   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 820            |\n",
            "| n_updates          | 196            |\n",
            "| policy_entropy     | 0.0014944494   |\n",
            "| policy_loss        | -1.8935825e-07 |\n",
            "| serial_timesteps   | 100352         |\n",
            "| time_elapsed       | 971            |\n",
            "| total_timesteps    | 802816         |\n",
            "| value_loss         | 0.0029127393   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 5.6640518e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 853            |\n",
            "| n_updates          | 197            |\n",
            "| policy_entropy     | 0.0018371048   |\n",
            "| policy_loss        | -5.5960765e-05 |\n",
            "| serial_timesteps   | 100864         |\n",
            "| time_elapsed       | 976            |\n",
            "| total_timesteps    | 806912         |\n",
            "| value_loss         | 0.0028796652   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 5.9578575e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 816            |\n",
            "| n_updates          | 198            |\n",
            "| policy_entropy     | 0.001974518    |\n",
            "| policy_loss        | -3.4699966e-05 |\n",
            "| serial_timesteps   | 101376         |\n",
            "| time_elapsed       | 980            |\n",
            "| total_timesteps    | 811008         |\n",
            "| value_loss         | 0.0028755863   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.3432537e-11 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.852         |\n",
            "| fps                | 853           |\n",
            "| n_updates          | 199           |\n",
            "| policy_entropy     | 0.0019918082  |\n",
            "| policy_loss        | 1.1541997e-07 |\n",
            "| serial_timesteps   | 101888        |\n",
            "| time_elapsed       | 985           |\n",
            "| total_timesteps    | 815104        |\n",
            "| value_loss         | 0.0028434542  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00017701524  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 872            |\n",
            "| n_updates          | 200            |\n",
            "| policy_entropy     | 0.001196316    |\n",
            "| policy_loss        | -0.00010256409 |\n",
            "| serial_timesteps   | 102400         |\n",
            "| time_elapsed       | 990            |\n",
            "| total_timesteps    | 819200         |\n",
            "| value_loss         | 0.0028772198   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 1.0684793e-12 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.852         |\n",
            "| fps                | 834           |\n",
            "| n_updates          | 201           |\n",
            "| policy_entropy     | 0.0011782754  |\n",
            "| policy_loss        | 6.3402696e-09 |\n",
            "| serial_timesteps   | 102912        |\n",
            "| time_elapsed       | 995           |\n",
            "| total_timesteps    | 823296        |\n",
            "| value_loss         | 0.0028694293  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00019542823  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 870            |\n",
            "| n_updates          | 202            |\n",
            "| policy_entropy     | 0.0009493431   |\n",
            "| policy_loss        | -0.00010995955 |\n",
            "| serial_timesteps   | 103424         |\n",
            "| time_elapsed       | 1e+03          |\n",
            "| total_timesteps    | 827392         |\n",
            "| value_loss         | 0.0028666635   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 7.820816e-14  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.852         |\n",
            "| fps                | 829           |\n",
            "| n_updates          | 203           |\n",
            "| policy_entropy     | 0.0009309075  |\n",
            "| policy_loss        | 3.1956007e-09 |\n",
            "| serial_timesteps   | 103936        |\n",
            "| time_elapsed       | 1e+03         |\n",
            "| total_timesteps    | 831488        |\n",
            "| value_loss         | 0.002858588   |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| approxkl           | 1.029141e-11    |\n",
            "| clipfrac           | 0.0             |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.956           |\n",
            "| explained_variance | 0.851           |\n",
            "| fps                | 874             |\n",
            "| n_updates          | 204             |\n",
            "| policy_entropy     | 0.00093254074   |\n",
            "| policy_loss        | -1.17107994e-07 |\n",
            "| serial_timesteps   | 104448          |\n",
            "| time_elapsed       | 1.01e+03        |\n",
            "| total_timesteps    | 835584          |\n",
            "| value_loss         | 0.0028614956    |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.8828545e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 875            |\n",
            "| n_updates          | 205            |\n",
            "| policy_entropy     | 0.00091683865  |\n",
            "| policy_loss        | -1.6170307e-07 |\n",
            "| serial_timesteps   | 104960         |\n",
            "| time_elapsed       | 1.01e+03       |\n",
            "| total_timesteps    | 839680         |\n",
            "| value_loss         | 0.002864474    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 1.9769431e-12 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.851         |\n",
            "| fps                | 839           |\n",
            "| n_updates          | 206           |\n",
            "| policy_entropy     | 0.0008749557  |\n",
            "| policy_loss        | -5.157053e-08 |\n",
            "| serial_timesteps   | 105472        |\n",
            "| time_elapsed       | 1.02e+03      |\n",
            "| total_timesteps    | 843776        |\n",
            "| value_loss         | 0.002855176   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.5544482e-12  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 856            |\n",
            "| n_updates          | 207            |\n",
            "| policy_entropy     | 0.0008535531   |\n",
            "| policy_loss        | -6.7609655e-08 |\n",
            "| serial_timesteps   | 105984         |\n",
            "| time_elapsed       | 1.02e+03       |\n",
            "| total_timesteps    | 847872         |\n",
            "| value_loss         | 0.0028575258   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 2.6066731e-12  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 838            |\n",
            "| n_updates          | 208            |\n",
            "| policy_entropy     | 0.00082918676  |\n",
            "| policy_loss        | -6.4371854e-08 |\n",
            "| serial_timesteps   | 106496         |\n",
            "| time_elapsed       | 1.03e+03       |\n",
            "| total_timesteps    | 851968         |\n",
            "| value_loss         | 0.0028423104   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 7.003064e-13  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.852         |\n",
            "| fps                | 882           |\n",
            "| n_updates          | 209           |\n",
            "| policy_entropy     | 0.00081946154 |\n",
            "| policy_loss        | -9.757787e-09 |\n",
            "| serial_timesteps   | 107008        |\n",
            "| time_elapsed       | 1.03e+03      |\n",
            "| total_timesteps    | 856064        |\n",
            "| value_loss         | 0.0028836767  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 5.7097413e-12 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.852         |\n",
            "| fps                | 821           |\n",
            "| n_updates          | 210           |\n",
            "| policy_entropy     | 0.000817625   |\n",
            "| policy_loss        | -7.755443e-08 |\n",
            "| serial_timesteps   | 107520        |\n",
            "| time_elapsed       | 1.04e+03      |\n",
            "| total_timesteps    | 860160        |\n",
            "| value_loss         | 0.0028142326  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.6988235e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 836            |\n",
            "| n_updates          | 211            |\n",
            "| policy_entropy     | 0.00083450053  |\n",
            "| policy_loss        | -1.1150405e-07 |\n",
            "| serial_timesteps   | 108032         |\n",
            "| time_elapsed       | 1.04e+03       |\n",
            "| total_timesteps    | 864256         |\n",
            "| value_loss         | 0.0028282756   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 9.206342e-12   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 838            |\n",
            "| n_updates          | 212            |\n",
            "| policy_entropy     | 0.0008900935   |\n",
            "| policy_loss        | -1.4932739e-07 |\n",
            "| serial_timesteps   | 108544         |\n",
            "| time_elapsed       | 1.05e+03       |\n",
            "| total_timesteps    | 868352         |\n",
            "| value_loss         | 0.0028704063   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 8.208303e-13  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.852         |\n",
            "| fps                | 827           |\n",
            "| n_updates          | 213           |\n",
            "| policy_entropy     | 0.0008883757  |\n",
            "| policy_loss        | 1.2463716e-09 |\n",
            "| serial_timesteps   | 109056        |\n",
            "| time_elapsed       | 1.05e+03      |\n",
            "| total_timesteps    | 872448        |\n",
            "| value_loss         | 0.0028420459  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00030043587  |\n",
            "| clipfrac           | 0.00029296876  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 876            |\n",
            "| n_updates          | 214            |\n",
            "| policy_entropy     | 0.0009795611   |\n",
            "| policy_loss        | -5.7187997e-05 |\n",
            "| serial_timesteps   | 109568         |\n",
            "| time_elapsed       | 1.06e+03       |\n",
            "| total_timesteps    | 876544         |\n",
            "| value_loss         | 0.002857842    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 4.6540032e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 842            |\n",
            "| n_updates          | 215            |\n",
            "| policy_entropy     | 0.00066724385  |\n",
            "| policy_loss        | -3.3482792e-07 |\n",
            "| serial_timesteps   | 110080         |\n",
            "| time_elapsed       | 1.06e+03       |\n",
            "| total_timesteps    | 880640         |\n",
            "| value_loss         | 0.002882431    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.9787137e-10  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 868            |\n",
            "| n_updates          | 216            |\n",
            "| policy_entropy     | 0.00072549406  |\n",
            "| policy_loss        | -1.0892138e-06 |\n",
            "| serial_timesteps   | 110592         |\n",
            "| time_elapsed       | 1.07e+03       |\n",
            "| total_timesteps    | 884736         |\n",
            "| value_loss         | 0.0028567938   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.0684778e-11 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.852         |\n",
            "| fps                | 885           |\n",
            "| n_updates          | 217           |\n",
            "| policy_entropy     | 0.00080975797 |\n",
            "| policy_loss        | 3.1326635e-09 |\n",
            "| serial_timesteps   | 111104        |\n",
            "| time_elapsed       | 1.07e+03      |\n",
            "| total_timesteps    | 888832        |\n",
            "| value_loss         | 0.0028266136  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 1.6323348e-10  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 837            |\n",
            "| n_updates          | 218            |\n",
            "| policy_entropy     | 0.0008573211   |\n",
            "| policy_loss        | -1.4340768e-07 |\n",
            "| serial_timesteps   | 111616         |\n",
            "| time_elapsed       | 1.08e+03       |\n",
            "| total_timesteps    | 892928         |\n",
            "| value_loss         | 0.002824264    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00013980841  |\n",
            "| clipfrac           | 0.0006591797   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 862            |\n",
            "| n_updates          | 219            |\n",
            "| policy_entropy     | 0.0007866366   |\n",
            "| policy_loss        | -1.6672866e-05 |\n",
            "| serial_timesteps   | 112128         |\n",
            "| time_elapsed       | 1.08e+03       |\n",
            "| total_timesteps    | 897024         |\n",
            "| value_loss         | 0.0028229353   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 2.9042367e-12  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 833            |\n",
            "| n_updates          | 220            |\n",
            "| policy_entropy     | 0.00076867035  |\n",
            "| policy_loss        | -3.1547096e-08 |\n",
            "| serial_timesteps   | 112640         |\n",
            "| time_elapsed       | 1.09e+03       |\n",
            "| total_timesteps    | 901120         |\n",
            "| value_loss         | 0.0028230494   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 4.421032e-12  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.851         |\n",
            "| fps                | 842           |\n",
            "| n_updates          | 221           |\n",
            "| policy_entropy     | 0.00076768367 |\n",
            "| policy_loss        | -6.802875e-08 |\n",
            "| serial_timesteps   | 113152        |\n",
            "| time_elapsed       | 1.09e+03      |\n",
            "| total_timesteps    | 905216        |\n",
            "| value_loss         | 0.0027784184  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.637126e-13   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 857            |\n",
            "| n_updates          | 222            |\n",
            "| policy_entropy     | 0.0007689243   |\n",
            "| policy_loss        | -1.5475962e-08 |\n",
            "| serial_timesteps   | 113664         |\n",
            "| time_elapsed       | 1.1e+03        |\n",
            "| total_timesteps    | 909312         |\n",
            "| value_loss         | 0.0028150124   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 1.1761232e-12  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 829            |\n",
            "| n_updates          | 223            |\n",
            "| policy_entropy     | 0.0007829152   |\n",
            "| policy_loss        | -2.2944004e-08 |\n",
            "| serial_timesteps   | 114176         |\n",
            "| time_elapsed       | 1.1e+03        |\n",
            "| total_timesteps    | 913408         |\n",
            "| value_loss         | 0.002833995    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.1307095e-05 |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.851         |\n",
            "| fps                | 866           |\n",
            "| n_updates          | 224           |\n",
            "| policy_entropy     | 0.00096040685 |\n",
            "| policy_loss        | -4.992994e-05 |\n",
            "| serial_timesteps   | 114688        |\n",
            "| time_elapsed       | 1.11e+03      |\n",
            "| total_timesteps    | 917504        |\n",
            "| value_loss         | 0.002843007   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 2.3217992e-12  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 847            |\n",
            "| n_updates          | 225            |\n",
            "| policy_entropy     | 0.0009841468   |\n",
            "| policy_loss        | -6.3133484e-08 |\n",
            "| serial_timesteps   | 115200         |\n",
            "| time_elapsed       | 1.11e+03       |\n",
            "| total_timesteps    | 921600         |\n",
            "| value_loss         | 0.002801331    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.7267194e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 867            |\n",
            "| n_updates          | 226            |\n",
            "| policy_entropy     | 0.0010012889   |\n",
            "| policy_loss        | -1.2997698e-07 |\n",
            "| serial_timesteps   | 115712         |\n",
            "| time_elapsed       | 1.12e+03       |\n",
            "| total_timesteps    | 925696         |\n",
            "| value_loss         | 0.002827006    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 6.4572854e-11 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.851         |\n",
            "| fps                | 872           |\n",
            "| n_updates          | 227           |\n",
            "| policy_entropy     | 0.0011397211  |\n",
            "| policy_loss        | -3.144378e-08 |\n",
            "| serial_timesteps   | 116224        |\n",
            "| time_elapsed       | 1.12e+03      |\n",
            "| total_timesteps    | 929792        |\n",
            "| value_loss         | 0.0027975778  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 9.408321e-05   |\n",
            "| clipfrac           | 0.00024414062  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.847          |\n",
            "| fps                | 825            |\n",
            "| n_updates          | 228            |\n",
            "| policy_entropy     | 0.0013487978   |\n",
            "| policy_loss        | -5.1983632e-05 |\n",
            "| serial_timesteps   | 116736         |\n",
            "| time_elapsed       | 1.12e+03       |\n",
            "| total_timesteps    | 933888         |\n",
            "| value_loss         | 0.0028448168   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0005527777  |\n",
            "| clipfrac           | 0.00041503907 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.85          |\n",
            "| fps                | 864           |\n",
            "| n_updates          | 229           |\n",
            "| policy_entropy     | 0.0020153983  |\n",
            "| policy_loss        | -0.0002511915 |\n",
            "| serial_timesteps   | 117248        |\n",
            "| time_elapsed       | 1.13e+03      |\n",
            "| total_timesteps    | 937984        |\n",
            "| value_loss         | 0.0028416968  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0011896287   |\n",
            "| clipfrac           | 0.00070800784  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 841            |\n",
            "| n_updates          | 230            |\n",
            "| policy_entropy     | 0.0009135517   |\n",
            "| policy_loss        | -0.00035751984 |\n",
            "| serial_timesteps   | 117760         |\n",
            "| time_elapsed       | 1.13e+03       |\n",
            "| total_timesteps    | 942080         |\n",
            "| value_loss         | 0.002825031    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 4.4646396e-12  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 860            |\n",
            "| n_updates          | 231            |\n",
            "| policy_entropy     | 0.00083993963  |\n",
            "| policy_loss        | -2.7824717e-08 |\n",
            "| serial_timesteps   | 118272         |\n",
            "| time_elapsed       | 1.14e+03       |\n",
            "| total_timesteps    | 946176         |\n",
            "| value_loss         | 0.0027875665   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 3.4804167e-12 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.851         |\n",
            "| fps                | 835           |\n",
            "| n_updates          | 232           |\n",
            "| policy_entropy     | 0.0008478486  |\n",
            "| policy_loss        | -1.606968e-08 |\n",
            "| serial_timesteps   | 118784        |\n",
            "| time_elapsed       | 1.14e+03      |\n",
            "| total_timesteps    | 950272        |\n",
            "| value_loss         | 0.002827252   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.23130586e-11 |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 848            |\n",
            "| n_updates          | 233            |\n",
            "| policy_entropy     | 0.0008612236   |\n",
            "| policy_loss        | -7.956114e-08  |\n",
            "| serial_timesteps   | 119296         |\n",
            "| time_elapsed       | 1.15e+03       |\n",
            "| total_timesteps    | 954368         |\n",
            "| value_loss         | 0.00284542     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 6.03009e-11    |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 843            |\n",
            "| n_updates          | 234            |\n",
            "| policy_entropy     | 0.0008531121   |\n",
            "| policy_loss        | -2.9086513e-07 |\n",
            "| serial_timesteps   | 119808         |\n",
            "| time_elapsed       | 1.15e+03       |\n",
            "| total_timesteps    | 958464         |\n",
            "| value_loss         | 0.0028065357   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 3.2752447e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 808            |\n",
            "| n_updates          | 235            |\n",
            "| policy_entropy     | 0.0008121377   |\n",
            "| policy_loss        | -1.4478283e-07 |\n",
            "| serial_timesteps   | 120320         |\n",
            "| time_elapsed       | 1.16e+03       |\n",
            "| total_timesteps    | 962560         |\n",
            "| value_loss         | 0.0027837914   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00016425138 |\n",
            "| clipfrac           | 0.0004394531  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.851         |\n",
            "| fps                | 862           |\n",
            "| n_updates          | 236           |\n",
            "| policy_entropy     | 0.00097342674 |\n",
            "| policy_loss        | -0.000124588  |\n",
            "| serial_timesteps   | 120832        |\n",
            "| time_elapsed       | 1.16e+03      |\n",
            "| total_timesteps    | 966656        |\n",
            "| value_loss         | 0.002775021   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 1.4582048e-09  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 811            |\n",
            "| n_updates          | 237            |\n",
            "| policy_entropy     | 0.0012285283   |\n",
            "| policy_loss        | -1.2602693e-06 |\n",
            "| serial_timesteps   | 121344         |\n",
            "| time_elapsed       | 1.17e+03       |\n",
            "| total_timesteps    | 970752         |\n",
            "| value_loss         | 0.0027742148   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 9.4850315e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 857            |\n",
            "| n_updates          | 238            |\n",
            "| policy_entropy     | 0.0024817712   |\n",
            "| policy_loss        | -4.6838864e-05 |\n",
            "| serial_timesteps   | 121856         |\n",
            "| time_elapsed       | 1.17e+03       |\n",
            "| total_timesteps    | 974848         |\n",
            "| value_loss         | 0.0027896613   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0002909209   |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.849          |\n",
            "| fps                | 857            |\n",
            "| n_updates          | 239            |\n",
            "| policy_entropy     | 0.00083714334  |\n",
            "| policy_loss        | -0.00012430953 |\n",
            "| serial_timesteps   | 122368         |\n",
            "| time_elapsed       | 1.18e+03       |\n",
            "| total_timesteps    | 978944         |\n",
            "| value_loss         | 0.0027608906   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0001709635   |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.851          |\n",
            "| fps                | 814            |\n",
            "| n_updates          | 240            |\n",
            "| policy_entropy     | 0.00057158596  |\n",
            "| policy_loss        | -0.00010737618 |\n",
            "| serial_timesteps   | 122880         |\n",
            "| time_elapsed       | 1.18e+03       |\n",
            "| total_timesteps    | 983040         |\n",
            "| value_loss         | 0.0028131972   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 7.0797934e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.85           |\n",
            "| fps                | 858            |\n",
            "| n_updates          | 241            |\n",
            "| policy_entropy     | 0.0006530739   |\n",
            "| policy_loss        | -5.4493234e-05 |\n",
            "| serial_timesteps   | 123392         |\n",
            "| time_elapsed       | 1.19e+03       |\n",
            "| total_timesteps    | 987136         |\n",
            "| value_loss         | 0.0027939663   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00012824289  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 818            |\n",
            "| n_updates          | 242            |\n",
            "| policy_entropy     | 0.0006460981   |\n",
            "| policy_loss        | -0.00010984478 |\n",
            "| serial_timesteps   | 123904         |\n",
            "| time_elapsed       | 1.19e+03       |\n",
            "| total_timesteps    | 991232         |\n",
            "| value_loss         | 0.0027823912   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.7023652e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.852          |\n",
            "| fps                | 843            |\n",
            "| n_updates          | 243            |\n",
            "| policy_entropy     | 0.00068135426  |\n",
            "| policy_loss        | -1.6813865e-07 |\n",
            "| serial_timesteps   | 124416         |\n",
            "| time_elapsed       | 1.2e+03        |\n",
            "| total_timesteps    | 995328         |\n",
            "| value_loss         | 0.0027614937   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.7115565e-10 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.849         |\n",
            "| fps                | 836           |\n",
            "| n_updates          | 244           |\n",
            "| policy_entropy     | 0.00057890883 |\n",
            "| policy_loss        | -9.158757e-07 |\n",
            "| serial_timesteps   | 124928        |\n",
            "| time_elapsed       | 1.2e+03       |\n",
            "| total_timesteps    | 999424        |\n",
            "| value_loss         | 0.0027869262  |\n",
            "--------------------------------------\n",
            "WARNING:tensorflow:From train.py:433: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "Saving to logs/ppo2/MiniGrid-SimpleCrossingEnvUmaze-v0_2\n",
            "\u001b[0m[6399a4a3bda8:01391] *** Process received signal ***\n",
            "[6399a4a3bda8:01391] Signal: Segmentation fault (11)\n",
            "[6399a4a3bda8:01391] Signal code: Address not mapped (1)\n",
            "[6399a4a3bda8:01391] Failing at address: 0x7f7694e1820d\n",
            "[6399a4a3bda8:01391] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f76978bf980]\n",
            "[6399a4a3bda8:01391] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f76974fe8a5]\n",
            "[6399a4a3bda8:01391] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f7697d69e44]\n",
            "[6399a4a3bda8:01391] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f76974ff735]\n",
            "[6399a4a3bda8:01391] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f7697d67cb3]\n",
            "[6399a4a3bda8:01391] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fHBq73665yD"
      },
      "source": [
        "#### Evaluate trained agent\n",
        "\n",
        "\n",
        "You can remove the `--folder logs/` to evaluate pretrained agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8YuEgU6bT3",
        "outputId": "1216c13d-837b-42ac-c15a-85ec3126ad10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python enjoy.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --no-render --n-timesteps 5000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"enjoy.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"enjoy.py\", line 77, in main\n",
            "    model_path = find_saved_model(algo, log_path, env_id, load_best=args.load_best)\n",
            "  File \"/content/rl-baselines-zoo/utils/utils.py\", line 369, in find_saved_model\n",
            "    raise ValueError(\"No model found for {} on {}, path: {}\".format(algo, env_id, model_path))\n",
            "ValueError: No model found for ppo2 on MiniGrid-SimpleCrossingS9N1-v0, path: logs/ppo2/MiniGrid-SimpleCrossingS9N1-v0.zip\n",
            "[821dccaef3a6:03091] *** Process received signal ***\n",
            "[821dccaef3a6:03091] Signal: Segmentation fault (11)\n",
            "[821dccaef3a6:03091] Signal code: Address not mapped (1)\n",
            "[821dccaef3a6:03091] Failing at address: 0x7fec1431620d\n",
            "[821dccaef3a6:03091] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fec16dbb980]\n",
            "[821dccaef3a6:03091] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fec169fa8a5]\n",
            "[821dccaef3a6:03091] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fec17265e44]\n",
            "[821dccaef3a6:03091] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fec169fb735]\n",
            "[821dccaef3a6:03091] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fec17263cb3]\n",
            "[821dccaef3a6:03091] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Il2J0VHPLC"
      },
      "source": [
        "#### Tune Hyperparameters\n",
        "\n",
        "We use [Optuna](https://optuna.org/) for optimizing the hyperparameters.\n",
        "\n",
        "Tune the hyperparameters for PPO2, using a tpe sampler and median pruner, 2 parallels jobs,\n",
        "with a budget of 1000 trials and a maximum of 50000 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sC22eGHTH-"
      },
      "source": [
        "!python -m train.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --gym-packages gym_minigrid -n 5000 -optimize --n-trials 100 --n-jobs 8 --sampler tpe --pruner median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Record  a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS1VBBaQ_emT"
      },
      "source": [
        "!pip install pyglet==1.3.1  # pyglet v1.4.1 throws an error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip3AauLzwNGP"
      },
      "source": [
        "!python -m utils.record_video --algo ppo2 --env MiniGrid-SimpleCrossingEnvUmaze-v0 --gym-packages gym_minigrid --exp-id 0 -f logs/ -n 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBuUfnzI8DN6"
      },
      "source": [
        "### Display the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3OTfpf8CXu"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOjFuwK9HI0"
      },
      "source": [
        "show_videos(prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjdpP0HE8D2p"
      },
      "source": [
        "### Continue Training\n",
        "\n",
        "Here, we will continue training of the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgMZQJJF6u1C"
      },
      "source": [
        "!python train.py --algo a2c --env CartPole-v1 --n-timesteps 50000 -i logs/a2c/CartPole-v1.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSaoyiAE8cVj"
      },
      "source": [
        "!python enjoy.py --algo a2c --env CartPole-v1 --no-render --n-timesteps 1000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9u4I1H-48O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}