{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codigo zoo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpg/RL-Agent-for-Unreal-Engine-4/blob/main/Codigo_zoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy9QoDC7XA7"
      },
      "source": [
        "# RL Baselines Zoo: Training in Colab\n",
        "\n",
        "\n",
        "\n",
        "Github Repo: [https://github.com/araffin/rl-baselines-zoo](https://github.com/araffin/rl-baselines-zoo)\n",
        "\n",
        "Stable-Baselines Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
        "\n",
        "# Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXVDDlTn02M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104e2481-94fe-4ac0-bf73-214678e1769d"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt-get update\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg freeglut3-dev xvfb\n",
        "!pip install stable-baselines[mpi] --upgrade\n",
        "!pip install pybullet\n",
        "!pip install box2d box2d-kengz pyyaml pytablewriter optuna scikit-optimize\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,696 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.8 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [253 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,784 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,132 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,368 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [223 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [54.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,211 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [868 kB]\n",
            "Get:24 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Fetched 10.9 MB in 4s (2,798 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0 xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 1,884 kB of archives.\n",
            "After this operation, 8,093 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 1,884 kB in 1s (1,359 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines[mpi]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl (240kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->stable-baselines[mpi]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.1\n",
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/d9/756b8fe29c574b34e3a60fd777688f8aaacb7eae37fcd1b5983ec415646d/pybullet-3.0.7-cp36-cp36m-manylinux1_x86_64.whl (87.5MB)\n",
            "\u001b[K     |████████████████████████████████| 87.5MB 34kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.0.7\n",
            "Collecting box2d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 9.1MB/s \n",
            "\u001b[?25hCollecting box2d-kengz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Collecting pytablewriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.4MB/s \n",
            "\u001b[?25hCollecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 30.5MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.5MB/s \n",
            "\u001b[?25hCollecting msgfy<1,>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl\n",
            "Collecting tcolorpy<1,>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/02/51/bbb0cc7f30771c285c354634bf83653a2871d58c6923bd29bfddeb9c9cb1/tcolorpy-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.6/dist-packages (from pytablewriter) (50.3.2)\n",
            "Collecting mbstrdecoder<2,>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/57/3bb55beafe0a5e9883621f01a560d16bcef6d4f844dc2dd40caa0a8d9182/mbstrdecoder-1.0.0-py3-none-any.whl\n",
            "Collecting DataProperty<2,>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/2d/e5413965af992f4e489b6f5eebf52db9c17953c772962d1223d434b05cef/DataProperty-0.50.0-py3-none-any.whl\n",
            "Collecting pathvalidate<3,>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/fa/1a951084aa93940399800e37ed6f096ad5c0de3c26604be62f9464a39fc1/pathvalidate-2.3.0-py3-none-any.whl\n",
            "Collecting tabledata<2,>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/df/b2/264d9707502f0259a3eb82ec48064df98b1735d5a5f315b6a1d7105263f4/tabledata-1.1.3-py3-none-any.whl\n",
            "Collecting typepy[datetime]<2,>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/55/a1111b2eb1f4096c28b14645ca62aec560b1768338af21620e470b60872f/typepy-1.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/3c/06c76ec8b54b9b1fad7f35e903fd25010fe3e0d41bd94cea5e6f12e0d651/cmaes-0.7.0-py3-none-any.whl\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 27.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.20)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/61/5b64d73b01c1218f55c894b5ec0fb89b32c6960b7f7b3ad9f5ac0c373b9d/cliff-3.5.0-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c8/c16d30bbed11a1722060014c246d124582d1f781b26f5859d8dacc3e08e1/colorlog-4.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.7)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.4 in /usr/local/lib/python3.6/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2018.9; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2018.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2.8.1)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/54/af6e2703f064485d717cb311d3f9440cd302a823ba6d80a020b59eae166d/cmd2-1.4.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 31.3MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Collecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.1.1)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.4.0)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp36-none-any.whl size=359761 sha256=c7be942692b096c35485a769f50e3ce95db897981b370afe606e7cbe09533d73\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: box2d-kengz, PrettyTable, pyperclip\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp36-cp36m-linux_x86_64.whl size=1984595 sha256=891a3254b2c027750cbbdd951431baadd28a8ff5e4f312fd59c9ef3c8086189c\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-cp36-none-any.whl size=13700 sha256=d52035ba9a678f97e6c6db5d54cc09992b723330330f1d6154116766d01caed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11119 sha256=c8f0aa3996b7ed03b5cc6f4321c19b4cedbf9a14d61a4f1a79fd5c66881d8d69\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built box2d-kengz PrettyTable pyperclip\n",
            "Installing collected packages: box2d, box2d-kengz, msgfy, tcolorpy, mbstrdecoder, typepy, DataProperty, pathvalidate, tabledata, pytablewriter, cmaes, Mako, python-editor, alembic, colorama, pyperclip, cmd2, pbr, PrettyTable, stevedore, cliff, colorlog, optuna, pyaml, scikit-optimize\n",
            "  Found existing installation: prettytable 2.0.0\n",
            "    Uninstalling prettytable-2.0.0:\n",
            "      Successfully uninstalled prettytable-2.0.0\n",
            "Successfully installed DataProperty-0.50.0 Mako-1.1.3 PrettyTable-0.7.2 alembic-1.4.3 box2d-2.3.10 box2d-kengz-2.3.3 cliff-3.5.0 cmaes-0.7.0 cmd2-1.4.0 colorama-0.4.4 colorlog-4.6.2 mbstrdecoder-1.0.0 msgfy-0.1.0 optuna-2.3.0 pathvalidate-2.3.0 pbr-5.5.1 pyaml-20.4.0 pyperclip-1.8.1 pytablewriter-0.58.0 python-editor-1.0.4 scikit-optimize-0.8.1 stevedore-3.3.0 tabledata-1.1.3 tcolorpy-0.0.8 typepy-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjF3qRg7oGH"
      },
      "source": [
        "## Clone RL Baselines Zoo Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjGikdT1DFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c2f32a-7662-4858-d9b4-24379b280877"
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines-zoo'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 1840 (delta 20), reused 26 (delta 12), pack-reused 1796\u001b[K\n",
            "Receiving objects: 100% (1840/1840), 375.67 MiB | 36.88 MiB/s, done.\n",
            "Resolving deltas: 100% (1085/1085), done.\n",
            "Checking out files: 100% (333/333), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REMQlh-ezyVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c70569-290c-40e5-9152-52b68738327b"
      },
      "source": [
        "cd rl-baselines-zoo/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rl-baselines-zoo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSTHgpbhb1op",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "68eed625-b84d-4f42-a4e1-4394f20106ad"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/rl-baselines-zoo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ-pAbF7zRZ"
      },
      "source": [
        "## Train an RL Agent\n",
        "\n",
        "\n",
        "The train agent can be found in the `logs/` folder.\n",
        "\n",
        "Here we will train A2C on CartPole-v1 environment for 100 000 steps. \n",
        "\n",
        "\n",
        "To train it on Pong (Atari), you just have to pass `--env PongNoFrameskip-v4`\n",
        "\n",
        "Note: You need to update `hyperparams/algo.yml` to support new environments. You can access it in the side panel of Google Colab. (see https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34lXM8ZfMQfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df4f740-d978-497d-be49-b82aeb08b535"
      },
      "source": [
        "!pip install gym-minigrid\n",
        "# go to gym-minigrid in \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs\" and replace the modeled crossing env"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym-minigrid in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (1.18.5)\n",
            "Requirement already satisfied: gym>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.9.6->gym-minigrid) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmsGh9A8p4NO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c3120f-bb05-4ad8-afd6-5ff98e3ae450"
      },
      "source": [
        "# MONTANDO FOLDER COMPARTIDO DE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npzEtYM3Ovsm"
      },
      "source": [
        "# moving the mod crossing file (U MAZE MODELED) to the destination path\r\n",
        "%rm \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs/crossing.py\"\r\n",
        "%cp \"/content/drive/MyDrive/Colab Notebooks/modelo ts agentnet/ajustes de entorno y modelo/crossing.py\" \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs\"\r\n",
        "\r\n",
        "#SIMULTANEAMENTE SE DEBE REEMPLAZAR LA INFORMACIÓN DEL ENTORNO EN LOS HIPERPARAMETROS PPO2 DEL PAQUETE ZOO\r\n",
        "%rm \"/content/rl-baselines-zoo/hyperparams/ppo2.yml\"\r\n",
        "%cp \"/content/drive/MyDrive/Colab Notebooks/modelo ts agentnet/ajustes de entorno y modelo/ppo2.yml\" \"/content/rl-baselines-zoo/hyperparams\"\r\n",
        "\r\n",
        "#tambien, actualizar el archivo train.py con cambios en los env wrappers.\r\n",
        "%rm \"/content/rl-baselines-zoo/train.py\"\r\n",
        "%cp \"/content/drive/MyDrive/Colab Notebooks/modelo ts agentnet/ajustes de entorno y modelo/train.py\" \"/content/rl-baselines-zoo\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sC22eGHTH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a84fb51-ae36-479d-ad7c-e21f82ea3b3a"
      },
      "source": [
        "\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --gym-packages gym_minigrid\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MiniGrid-SimpleCrossingS9N1-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('cliprange', 0.2),\n",
            "             ('ent_coef', 0.0),\n",
            "             ('env_wrapper',\n",
            "              ['gym_minigrid.wrappers.RGBImgObsWrapper',\n",
            "               'gym_minigrid.wrappers.ImgObsWrapper']),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.95),\n",
            "             ('learning_rate', 0.00025),\n",
            "             ('n_envs', 8),\n",
            "             ('n_steps', 512),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 32),\n",
            "             ('noptepochs', 10),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'CnnPolicy')])\n",
            "Using 8 environments\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:103: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MiniGrid-SimpleCrossingS9N1-v0_1\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0022593315  |\n",
            "| clipfrac           | 0.012719726   |\n",
            "| ep_len_mean        | 324           |\n",
            "| ep_reward_mean     | 0             |\n",
            "| explained_variance | -0.241        |\n",
            "| fps                | 228           |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 1.9437125     |\n",
            "| policy_loss        | -0.0015956564 |\n",
            "| serial_timesteps   | 512           |\n",
            "| time_elapsed       | 1.91e-05      |\n",
            "| total_timesteps    | 4096          |\n",
            "| value_loss         | 7.5652047e-06 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0106105115  |\n",
            "| clipfrac           | 0.105126955   |\n",
            "| ep_len_mean        | 324           |\n",
            "| ep_reward_mean     | 0             |\n",
            "| explained_variance | 0.614         |\n",
            "| fps                | 378           |\n",
            "| n_updates          | 2             |\n",
            "| policy_entropy     | 1.9347632     |\n",
            "| policy_loss        | -0.00641857   |\n",
            "| serial_timesteps   | 1024          |\n",
            "| time_elapsed       | 18            |\n",
            "| total_timesteps    | 8192          |\n",
            "| value_loss         | 2.0578257e-06 |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "New best mean reward!\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008209451   |\n",
            "| clipfrac           | 0.053027343   |\n",
            "| ep_len_mean        | 324           |\n",
            "| ep_reward_mean     | 0             |\n",
            "| explained_variance | 0.755         |\n",
            "| fps                | 234           |\n",
            "| n_updates          | 3             |\n",
            "| policy_entropy     | 1.9372839     |\n",
            "| policy_loss        | -0.004706378  |\n",
            "| serial_timesteps   | 1536          |\n",
            "| time_elapsed       | 28.8          |\n",
            "| total_timesteps    | 12288         |\n",
            "| value_loss         | 3.1876712e-06 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.007019005   |\n",
            "| clipfrac           | 0.04345703    |\n",
            "| ep_len_mean        | 324           |\n",
            "| ep_reward_mean     | 0             |\n",
            "| explained_variance | 0.912         |\n",
            "| fps                | 376           |\n",
            "| n_updates          | 4             |\n",
            "| policy_entropy     | 1.9300888     |\n",
            "| policy_loss        | -0.0054343096 |\n",
            "| serial_timesteps   | 2048          |\n",
            "| time_elapsed       | 46.3          |\n",
            "| total_timesteps    | 16384         |\n",
            "| value_loss         | 6.5882923e-06 |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.01290486    |\n",
            "| clipfrac           | 0.1890625     |\n",
            "| ep_len_mean        | 324           |\n",
            "| ep_reward_mean     | 0             |\n",
            "| explained_variance | 0.809         |\n",
            "| fps                | 234           |\n",
            "| n_updates          | 5             |\n",
            "| policy_entropy     | 1.9124739     |\n",
            "| policy_loss        | -0.011707278  |\n",
            "| serial_timesteps   | 2560          |\n",
            "| time_elapsed       | 57.2          |\n",
            "| total_timesteps    | 20480         |\n",
            "| value_loss         | 4.3508994e-06 |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010660509  |\n",
            "| clipfrac           | 0.125        |\n",
            "| ep_len_mean        | 319          |\n",
            "| ep_reward_mean     | 0.0154       |\n",
            "| explained_variance | -0.00109     |\n",
            "| fps                | 370          |\n",
            "| n_updates          | 6            |\n",
            "| policy_entropy     | 1.9173092    |\n",
            "| policy_loss        | -0.008186715 |\n",
            "| serial_timesteps   | 3072         |\n",
            "| time_elapsed       | 74.6         |\n",
            "| total_timesteps    | 24576        |\n",
            "| value_loss         | 0.198019     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.00964944   |\n",
            "| clipfrac           | 0.12790528   |\n",
            "| ep_len_mean        | 320          |\n",
            "| ep_reward_mean     | 0.0128       |\n",
            "| explained_variance | 0.864        |\n",
            "| fps                | 380          |\n",
            "| n_updates          | 7            |\n",
            "| policy_entropy     | 1.8991512    |\n",
            "| policy_loss        | -0.009516945 |\n",
            "| serial_timesteps   | 3584         |\n",
            "| time_elapsed       | 85.7         |\n",
            "| total_timesteps    | 28672        |\n",
            "| value_loss         | 5.657518e-05 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0070185005  |\n",
            "| clipfrac           | 0.046484374   |\n",
            "| ep_len_mean        | 320           |\n",
            "| ep_reward_mean     | 0.0154        |\n",
            "| explained_variance | -0.00812      |\n",
            "| fps                | 237           |\n",
            "| n_updates          | 8             |\n",
            "| policy_entropy     | 1.9075062     |\n",
            "| policy_loss        | -0.0033006326 |\n",
            "| serial_timesteps   | 4096          |\n",
            "| time_elapsed       | 96.4          |\n",
            "| total_timesteps    | 32768         |\n",
            "| value_loss         | 0.099036284   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0051011145  |\n",
            "| clipfrac           | 0.048876952   |\n",
            "| ep_len_mean        | 319           |\n",
            "| ep_reward_mean     | 0.018         |\n",
            "| explained_variance | 0.0217        |\n",
            "| fps                | 379           |\n",
            "| n_updates          | 9             |\n",
            "| policy_entropy     | 1.8876314     |\n",
            "| policy_loss        | -0.0017338593 |\n",
            "| serial_timesteps   | 4608          |\n",
            "| time_elapsed       | 114           |\n",
            "| total_timesteps    | 36864         |\n",
            "| value_loss         | 0.09776502    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.007896235   |\n",
            "| clipfrac           | 0.11623535    |\n",
            "| ep_len_mean        | 318           |\n",
            "| ep_reward_mean     | 0.0204        |\n",
            "| explained_variance | -0.0103       |\n",
            "| fps                | 237           |\n",
            "| n_updates          | 10            |\n",
            "| policy_entropy     | 1.880574      |\n",
            "| policy_loss        | -0.0045793816 |\n",
            "| serial_timesteps   | 5120          |\n",
            "| time_elapsed       | 125           |\n",
            "| total_timesteps    | 40960         |\n",
            "| value_loss         | 0.10024585    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0070757978 |\n",
            "| clipfrac           | 0.10019531   |\n",
            "| ep_len_mean        | 318          |\n",
            "| ep_reward_mean     | 0.0232       |\n",
            "| explained_variance | 0.0299       |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 11           |\n",
            "| policy_entropy     | 1.8539727    |\n",
            "| policy_loss        | -0.006203535 |\n",
            "| serial_timesteps   | 5632         |\n",
            "| time_elapsed       | 142          |\n",
            "| total_timesteps    | 45056        |\n",
            "| value_loss         | 0.097890206  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.011687088 |\n",
            "| clipfrac           | 0.13908692  |\n",
            "| ep_len_mean        | 317         |\n",
            "| ep_reward_mean     | 0.0276      |\n",
            "| explained_variance | 0.0102      |\n",
            "| fps                | 369         |\n",
            "| n_updates          | 12          |\n",
            "| policy_entropy     | 1.8270609   |\n",
            "| policy_loss        | -0.00972906 |\n",
            "| serial_timesteps   | 6144        |\n",
            "| time_elapsed       | 153         |\n",
            "| total_timesteps    | 49152       |\n",
            "| value_loss         | 0.19515082  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008951405   |\n",
            "| clipfrac           | 0.10629883    |\n",
            "| ep_len_mean        | 317           |\n",
            "| ep_reward_mean     | 0.0276        |\n",
            "| explained_variance | 0.658         |\n",
            "| fps                | 239           |\n",
            "| n_updates          | 13            |\n",
            "| policy_entropy     | 1.8128732     |\n",
            "| policy_loss        | -0.011707314  |\n",
            "| serial_timesteps   | 6656          |\n",
            "| time_elapsed       | 164           |\n",
            "| total_timesteps    | 53248         |\n",
            "| value_loss         | 0.00022699898 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012332479   |\n",
            "| clipfrac           | 0.17194824    |\n",
            "| ep_len_mean        | 319           |\n",
            "| ep_reward_mean     | 0.0229        |\n",
            "| explained_variance | 0.0311        |\n",
            "| fps                | 381           |\n",
            "| n_updates          | 14            |\n",
            "| policy_entropy     | 1.7970587     |\n",
            "| policy_loss        | -0.0065321913 |\n",
            "| serial_timesteps   | 7168          |\n",
            "| time_elapsed       | 181           |\n",
            "| total_timesteps    | 57344         |\n",
            "| value_loss         | 0.19239952    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009324864  |\n",
            "| clipfrac           | 0.122558594  |\n",
            "| ep_len_mean        | 313          |\n",
            "| ep_reward_mean     | 0.0428       |\n",
            "| explained_variance | 0.103        |\n",
            "| fps                | 238          |\n",
            "| n_updates          | 15           |\n",
            "| policy_entropy     | 1.7534723    |\n",
            "| policy_loss        | -0.006321351 |\n",
            "| serial_timesteps   | 7680         |\n",
            "| time_elapsed       | 192          |\n",
            "| total_timesteps    | 61440        |\n",
            "| value_loss         | 0.4300402    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.010278086 |\n",
            "| clipfrac           | 0.13310547  |\n",
            "| ep_len_mean        | 304         |\n",
            "| ep_reward_mean     | 0.0723      |\n",
            "| explained_variance | 0.165       |\n",
            "| fps                | 380         |\n",
            "| n_updates          | 16          |\n",
            "| policy_entropy     | 1.72741     |\n",
            "| policy_loss        | -0.01235547 |\n",
            "| serial_timesteps   | 8192        |\n",
            "| time_elapsed       | 209         |\n",
            "| total_timesteps    | 65536       |\n",
            "| value_loss         | 0.37059444  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011409154  |\n",
            "| clipfrac           | 0.12531738   |\n",
            "| ep_len_mean        | 302          |\n",
            "| ep_reward_mean     | 0.0791       |\n",
            "| explained_variance | 0.209        |\n",
            "| fps                | 380          |\n",
            "| n_updates          | 17           |\n",
            "| policy_entropy     | 1.7158878    |\n",
            "| policy_loss        | -0.008507079 |\n",
            "| serial_timesteps   | 8704         |\n",
            "| time_elapsed       | 220          |\n",
            "| total_timesteps    | 69632        |\n",
            "| value_loss         | 0.17175303   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.007827805   |\n",
            "| clipfrac           | 0.10061035    |\n",
            "| ep_len_mean        | 300           |\n",
            "| ep_reward_mean     | 0.0839        |\n",
            "| explained_variance | 0.28          |\n",
            "| fps                | 237           |\n",
            "| n_updates          | 18            |\n",
            "| policy_entropy     | 1.7201294     |\n",
            "| policy_loss        | -0.0067839115 |\n",
            "| serial_timesteps   | 9216          |\n",
            "| time_elapsed       | 230           |\n",
            "| total_timesteps    | 73728         |\n",
            "| value_loss         | 0.16015553    |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.01146681  |\n",
            "| clipfrac           | 0.14865723  |\n",
            "| ep_len_mean        | 298         |\n",
            "| ep_reward_mean     | 0.0921      |\n",
            "| explained_variance | 0.207       |\n",
            "| fps                | 379         |\n",
            "| n_updates          | 19          |\n",
            "| policy_entropy     | 1.74206     |\n",
            "| policy_loss        | -0.01200537 |\n",
            "| serial_timesteps   | 9728        |\n",
            "| time_elapsed       | 248         |\n",
            "| total_timesteps    | 77824       |\n",
            "| value_loss         | 0.16974969  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008186003  |\n",
            "| clipfrac           | 0.09746094   |\n",
            "| ep_len_mean        | 293          |\n",
            "| ep_reward_mean     | 0.108        |\n",
            "| explained_variance | 0.18         |\n",
            "| fps                | 238          |\n",
            "| n_updates          | 20           |\n",
            "| policy_entropy     | 1.7681637    |\n",
            "| policy_loss        | -0.012174249 |\n",
            "| serial_timesteps   | 10240        |\n",
            "| time_elapsed       | 258          |\n",
            "| total_timesteps    | 81920        |\n",
            "| value_loss         | 0.31238148   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.013071107 |\n",
            "| clipfrac           | 0.14177246  |\n",
            "| ep_len_mean        | 293         |\n",
            "| ep_reward_mean     | 0.109       |\n",
            "| explained_variance | 0.249       |\n",
            "| fps                | 377         |\n",
            "| n_updates          | 21          |\n",
            "| policy_entropy     | 1.7583342   |\n",
            "| policy_loss        | -0.01440229 |\n",
            "| serial_timesteps   | 10752       |\n",
            "| time_elapsed       | 275         |\n",
            "| total_timesteps    | 86016       |\n",
            "| value_loss         | 0.16090715  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "------------------------------------\n",
            "| approxkl           | 0.01203858  |\n",
            "| clipfrac           | 0.17260742  |\n",
            "| ep_len_mean        | 291         |\n",
            "| ep_reward_mean     | 0.115       |\n",
            "| explained_variance | 0.129       |\n",
            "| fps                | 238         |\n",
            "| n_updates          | 22          |\n",
            "| policy_entropy     | 1.7009989   |\n",
            "| policy_loss        | -0.01625176 |\n",
            "| serial_timesteps   | 11264       |\n",
            "| time_elapsed       | 286         |\n",
            "| total_timesteps    | 90112       |\n",
            "| value_loss         | 0.43690977  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008941429  |\n",
            "| clipfrac           | 0.124731444  |\n",
            "| ep_len_mean        | 297          |\n",
            "| ep_reward_mean     | 0.0951       |\n",
            "| explained_variance | 0.253        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 23           |\n",
            "| policy_entropy     | 1.6805385    |\n",
            "| policy_loss        | -0.011672536 |\n",
            "| serial_timesteps   | 11776        |\n",
            "| time_elapsed       | 303          |\n",
            "| total_timesteps    | 94208        |\n",
            "| value_loss         | 0.08290072   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011253804  |\n",
            "| clipfrac           | 0.12194824   |\n",
            "| ep_len_mean        | 293          |\n",
            "| ep_reward_mean     | 0.109        |\n",
            "| explained_variance | 0.167        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 24           |\n",
            "| policy_entropy     | 1.6886575    |\n",
            "| policy_loss        | -0.022365589 |\n",
            "| serial_timesteps   | 12288        |\n",
            "| time_elapsed       | 314          |\n",
            "| total_timesteps    | 98304        |\n",
            "| value_loss         | 0.60085386   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0139227975 |\n",
            "| clipfrac           | 0.18364258   |\n",
            "| ep_len_mean        | 290          |\n",
            "| ep_reward_mean     | 0.122        |\n",
            "| explained_variance | 0.123        |\n",
            "| fps                | 236          |\n",
            "| n_updates          | 25           |\n",
            "| policy_entropy     | 1.731074     |\n",
            "| policy_loss        | -0.0170405   |\n",
            "| serial_timesteps   | 12800        |\n",
            "| time_elapsed       | 325          |\n",
            "| total_timesteps    | 102400       |\n",
            "| value_loss         | 0.44185406   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011400051  |\n",
            "| clipfrac           | 0.14355469   |\n",
            "| ep_len_mean        | 288          |\n",
            "| ep_reward_mean     | 0.13         |\n",
            "| explained_variance | 0.234        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 26           |\n",
            "| policy_entropy     | 1.6901238    |\n",
            "| policy_loss        | -0.016270448 |\n",
            "| serial_timesteps   | 13312        |\n",
            "| time_elapsed       | 343          |\n",
            "| total_timesteps    | 106496       |\n",
            "| value_loss         | 0.39291686   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01646742   |\n",
            "| clipfrac           | 0.20212403   |\n",
            "| ep_len_mean        | 289          |\n",
            "| ep_reward_mean     | 0.126        |\n",
            "| explained_variance | 0.366        |\n",
            "| fps                | 240          |\n",
            "| n_updates          | 27           |\n",
            "| policy_entropy     | 1.4780891    |\n",
            "| policy_loss        | -0.023243872 |\n",
            "| serial_timesteps   | 13824        |\n",
            "| time_elapsed       | 353          |\n",
            "| total_timesteps    | 110592       |\n",
            "| value_loss         | 0.42706162   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.012565853 |\n",
            "| clipfrac           | 0.15705566  |\n",
            "| ep_len_mean        | 278         |\n",
            "| ep_reward_mean     | 0.165       |\n",
            "| explained_variance | 0.31        |\n",
            "| fps                | 378         |\n",
            "| n_updates          | 28          |\n",
            "| policy_entropy     | 1.5848672   |\n",
            "| policy_loss        | -0.01805051 |\n",
            "| serial_timesteps   | 14336       |\n",
            "| time_elapsed       | 370         |\n",
            "| total_timesteps    | 114688      |\n",
            "| value_loss         | 0.72188425  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0139423385 |\n",
            "| clipfrac           | 0.16228028   |\n",
            "| ep_len_mean        | 271          |\n",
            "| ep_reward_mean     | 0.188        |\n",
            "| explained_variance | 0.372        |\n",
            "| fps                | 383          |\n",
            "| n_updates          | 29           |\n",
            "| policy_entropy     | 1.5646297    |\n",
            "| policy_loss        | -0.017804287 |\n",
            "| serial_timesteps   | 14848        |\n",
            "| time_elapsed       | 381          |\n",
            "| total_timesteps    | 118784       |\n",
            "| value_loss         | 0.58553636   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=0.12 +/- 0.24\n",
            "Episode length: 288.00 +/- 72.00\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019741613  |\n",
            "| clipfrac           | 0.20961913   |\n",
            "| ep_len_mean        | 271          |\n",
            "| ep_reward_mean     | 0.189        |\n",
            "| explained_variance | 0.308        |\n",
            "| fps                | 247          |\n",
            "| n_updates          | 30           |\n",
            "| policy_entropy     | 1.5088985    |\n",
            "| policy_loss        | -0.017306158 |\n",
            "| serial_timesteps   | 15360        |\n",
            "| time_elapsed       | 392          |\n",
            "| total_timesteps    | 122880       |\n",
            "| value_loss         | 0.42434233   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.014919482 |\n",
            "| clipfrac           | 0.17236328  |\n",
            "| ep_len_mean        | 269         |\n",
            "| ep_reward_mean     | 0.194       |\n",
            "| explained_variance | 0.476       |\n",
            "| fps                | 383         |\n",
            "| n_updates          | 31          |\n",
            "| policy_entropy     | 1.6359546   |\n",
            "| policy_loss        | -0.01693074 |\n",
            "| serial_timesteps   | 15872       |\n",
            "| time_elapsed       | 409         |\n",
            "| total_timesteps    | 126976      |\n",
            "| value_loss         | 0.33031172  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016706519  |\n",
            "| clipfrac           | 0.18652344   |\n",
            "| ep_len_mean        | 269          |\n",
            "| ep_reward_mean     | 0.192        |\n",
            "| explained_variance | 0.364        |\n",
            "| fps                | 240          |\n",
            "| n_updates          | 32           |\n",
            "| policy_entropy     | 1.5087359    |\n",
            "| policy_loss        | -0.014097971 |\n",
            "| serial_timesteps   | 16384        |\n",
            "| time_elapsed       | 419          |\n",
            "| total_timesteps    | 131072       |\n",
            "| value_loss         | 0.42244545   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013619961  |\n",
            "| clipfrac           | 0.18017578   |\n",
            "| ep_len_mean        | 265          |\n",
            "| ep_reward_mean     | 0.206        |\n",
            "| explained_variance | 0.41         |\n",
            "| fps                | 383          |\n",
            "| n_updates          | 33           |\n",
            "| policy_entropy     | 1.4777238    |\n",
            "| policy_loss        | -0.017908214 |\n",
            "| serial_timesteps   | 16896        |\n",
            "| time_elapsed       | 436          |\n",
            "| total_timesteps    | 135168       |\n",
            "| value_loss         | 0.5479425    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.01238232  |\n",
            "| clipfrac           | 0.16826172  |\n",
            "| ep_len_mean        | 247         |\n",
            "| ep_reward_mean     | 0.266       |\n",
            "| explained_variance | 0.563       |\n",
            "| fps                | 385         |\n",
            "| n_updates          | 34          |\n",
            "| policy_entropy     | 1.439028    |\n",
            "| policy_loss        | -0.02188215 |\n",
            "| serial_timesteps   | 17408       |\n",
            "| time_elapsed       | 447         |\n",
            "| total_timesteps    | 139264      |\n",
            "| value_loss         | 0.97236794  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "------------------------------------\n",
            "| approxkl           | 0.011600378 |\n",
            "| clipfrac           | 0.16086426  |\n",
            "| ep_len_mean        | 245         |\n",
            "| ep_reward_mean     | 0.271       |\n",
            "| explained_variance | 0.53        |\n",
            "| fps                | 239         |\n",
            "| n_updates          | 35          |\n",
            "| policy_entropy     | 1.4191351   |\n",
            "| policy_loss        | -0.02136812 |\n",
            "| serial_timesteps   | 17920       |\n",
            "| time_elapsed       | 457         |\n",
            "| total_timesteps    | 143360      |\n",
            "| value_loss         | 0.59811974  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01470422   |\n",
            "| clipfrac           | 0.2095459    |\n",
            "| ep_len_mean        | 229          |\n",
            "| ep_reward_mean     | 0.319        |\n",
            "| explained_variance | 0.511        |\n",
            "| fps                | 380          |\n",
            "| n_updates          | 36           |\n",
            "| policy_entropy     | 1.3701017    |\n",
            "| policy_loss        | -0.017466795 |\n",
            "| serial_timesteps   | 18432        |\n",
            "| time_elapsed       | 475          |\n",
            "| total_timesteps    | 147456       |\n",
            "| value_loss         | 0.58559513   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013554084  |\n",
            "| clipfrac           | 0.17824706   |\n",
            "| ep_len_mean        | 228          |\n",
            "| ep_reward_mean     | 0.321        |\n",
            "| explained_variance | 0.472        |\n",
            "| fps                | 243          |\n",
            "| n_updates          | 37           |\n",
            "| policy_entropy     | 1.4452927    |\n",
            "| policy_loss        | -0.022408212 |\n",
            "| serial_timesteps   | 18944        |\n",
            "| time_elapsed       | 485          |\n",
            "| total_timesteps    | 151552       |\n",
            "| value_loss         | 0.49798837   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0162752    |\n",
            "| clipfrac           | 0.20690918   |\n",
            "| ep_len_mean        | 211          |\n",
            "| ep_reward_mean     | 0.374        |\n",
            "| explained_variance | 0.48         |\n",
            "| fps                | 387          |\n",
            "| n_updates          | 38           |\n",
            "| policy_entropy     | 1.3607188    |\n",
            "| policy_loss        | -0.023488851 |\n",
            "| serial_timesteps   | 19456        |\n",
            "| time_elapsed       | 502          |\n",
            "| total_timesteps    | 155648       |\n",
            "| value_loss         | 0.7993101    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.015964044 |\n",
            "| clipfrac           | 0.19406739  |\n",
            "| ep_len_mean        | 201         |\n",
            "| ep_reward_mean     | 0.403       |\n",
            "| explained_variance | 0.555       |\n",
            "| fps                | 384         |\n",
            "| n_updates          | 39          |\n",
            "| policy_entropy     | 1.4273403   |\n",
            "| policy_loss        | -0.02322331 |\n",
            "| serial_timesteps   | 19968       |\n",
            "| time_elapsed       | 513         |\n",
            "| total_timesteps    | 159744      |\n",
            "| value_loss         | 0.96998596  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014441876  |\n",
            "| clipfrac           | 0.17421874   |\n",
            "| ep_len_mean        | 206          |\n",
            "| ep_reward_mean     | 0.386        |\n",
            "| explained_variance | 0.627        |\n",
            "| fps                | 242          |\n",
            "| n_updates          | 40           |\n",
            "| policy_entropy     | 1.3785032    |\n",
            "| policy_loss        | -0.016087107 |\n",
            "| serial_timesteps   | 20480        |\n",
            "| time_elapsed       | 523          |\n",
            "| total_timesteps    | 163840       |\n",
            "| value_loss         | 0.4173294    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015613231  |\n",
            "| clipfrac           | 0.21628419   |\n",
            "| ep_len_mean        | 193          |\n",
            "| ep_reward_mean     | 0.429        |\n",
            "| explained_variance | 0.549        |\n",
            "| fps                | 385          |\n",
            "| n_updates          | 41           |\n",
            "| policy_entropy     | 1.2705803    |\n",
            "| policy_loss        | -0.023226403 |\n",
            "| serial_timesteps   | 20992        |\n",
            "| time_elapsed       | 540          |\n",
            "| total_timesteps    | 167936       |\n",
            "| value_loss         | 0.99998635   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=0.19 +/- 0.39\n",
            "Episode length: 261.80 +/- 124.40\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013890338  |\n",
            "| clipfrac           | 0.18967286   |\n",
            "| ep_len_mean        | 176          |\n",
            "| ep_reward_mean     | 0.481        |\n",
            "| explained_variance | 0.607        |\n",
            "| fps                | 255          |\n",
            "| n_updates          | 42           |\n",
            "| policy_entropy     | 1.4221547    |\n",
            "| policy_loss        | -0.023706315 |\n",
            "| serial_timesteps   | 21504        |\n",
            "| time_elapsed       | 551          |\n",
            "| total_timesteps    | 172032       |\n",
            "| value_loss         | 0.9326533    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0151454825 |\n",
            "| clipfrac           | 0.17973633   |\n",
            "| ep_len_mean        | 180          |\n",
            "| ep_reward_mean     | 0.47         |\n",
            "| explained_variance | 0.663        |\n",
            "| fps                | 384          |\n",
            "| n_updates          | 43           |\n",
            "| policy_entropy     | 1.2666328    |\n",
            "| policy_loss        | -0.017701598 |\n",
            "| serial_timesteps   | 22016        |\n",
            "| time_elapsed       | 567          |\n",
            "| total_timesteps    | 176128       |\n",
            "| value_loss         | 0.63509244   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=0.39 +/- 0.47\n",
            "Episode length: 199.60 +/- 152.36\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01339863   |\n",
            "| clipfrac           | 0.18061523   |\n",
            "| ep_len_mean        | 184          |\n",
            "| ep_reward_mean     | 0.458        |\n",
            "| explained_variance | 0.605        |\n",
            "| fps                | 279          |\n",
            "| n_updates          | 44           |\n",
            "| policy_entropy     | 1.2893758    |\n",
            "| policy_loss        | -0.016062979 |\n",
            "| serial_timesteps   | 22528        |\n",
            "| time_elapsed       | 578          |\n",
            "| total_timesteps    | 180224       |\n",
            "| value_loss         | 0.3868283    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.017773407  |\n",
            "| clipfrac           | 0.18803711   |\n",
            "| ep_len_mean        | 178          |\n",
            "| ep_reward_mean     | 0.474        |\n",
            "| explained_variance | 0.693        |\n",
            "| fps                | 386          |\n",
            "| n_updates          | 45           |\n",
            "| policy_entropy     | 1.2125854    |\n",
            "| policy_loss        | -0.018247772 |\n",
            "| serial_timesteps   | 23040        |\n",
            "| time_elapsed       | 592          |\n",
            "| total_timesteps    | 184320       |\n",
            "| value_loss         | 0.49952206   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018639175  |\n",
            "| clipfrac           | 0.19936523   |\n",
            "| ep_len_mean        | 185          |\n",
            "| ep_reward_mean     | 0.453        |\n",
            "| explained_variance | 0.662        |\n",
            "| fps                | 383          |\n",
            "| n_updates          | 46           |\n",
            "| policy_entropy     | 1.1319578    |\n",
            "| policy_loss        | -0.021378722 |\n",
            "| serial_timesteps   | 23552        |\n",
            "| time_elapsed       | 603          |\n",
            "| total_timesteps    | 188416       |\n",
            "| value_loss         | 0.70660543   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019520525  |\n",
            "| clipfrac           | 0.23776856   |\n",
            "| ep_len_mean        | 185          |\n",
            "| ep_reward_mean     | 0.456        |\n",
            "| explained_variance | 0.674        |\n",
            "| fps                | 239          |\n",
            "| n_updates          | 47           |\n",
            "| policy_entropy     | 1.2136759    |\n",
            "| policy_loss        | -0.026569864 |\n",
            "| serial_timesteps   | 24064        |\n",
            "| time_elapsed       | 614          |\n",
            "| total_timesteps    | 192512       |\n",
            "| value_loss         | 0.8587945    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.01937354  |\n",
            "| clipfrac           | 0.21008301  |\n",
            "| ep_len_mean        | 167         |\n",
            "| ep_reward_mean     | 0.509       |\n",
            "| explained_variance | 0.747       |\n",
            "| fps                | 379         |\n",
            "| n_updates          | 48          |\n",
            "| policy_entropy     | 1.1937404   |\n",
            "| policy_loss        | -0.02184544 |\n",
            "| serial_timesteps   | 24576       |\n",
            "| time_elapsed       | 631         |\n",
            "| total_timesteps    | 196608      |\n",
            "| value_loss         | 0.615094    |\n",
            "------------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=0.19 +/- 0.39\n",
            "Episode length: 261.80 +/- 124.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016775046  |\n",
            "| clipfrac           | 0.20805664   |\n",
            "| ep_len_mean        | 159          |\n",
            "| ep_reward_mean     | 0.533        |\n",
            "| explained_variance | 0.721        |\n",
            "| fps                | 258          |\n",
            "| n_updates          | 49           |\n",
            "| policy_entropy     | 1.2204454    |\n",
            "| policy_loss        | -0.018793853 |\n",
            "| serial_timesteps   | 25088        |\n",
            "| time_elapsed       | 642          |\n",
            "| total_timesteps    | 200704       |\n",
            "| value_loss         | 0.77290046   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.021344991 |\n",
            "| clipfrac           | 0.2411377   |\n",
            "| ep_len_mean        | 137         |\n",
            "| ep_reward_mean     | 0.596       |\n",
            "| explained_variance | 0.679       |\n",
            "| fps                | 385         |\n",
            "| n_updates          | 50          |\n",
            "| policy_entropy     | 1.151129    |\n",
            "| policy_loss        | -0.02721798 |\n",
            "| serial_timesteps   | 25600       |\n",
            "| time_elapsed       | 657         |\n",
            "| total_timesteps    | 204800      |\n",
            "| value_loss         | 1.0383565   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.029677222  |\n",
            "| clipfrac           | 0.2890381    |\n",
            "| ep_len_mean        | 122          |\n",
            "| ep_reward_mean     | 0.642        |\n",
            "| explained_variance | 0.722        |\n",
            "| fps                | 385          |\n",
            "| n_updates          | 51           |\n",
            "| policy_entropy     | 1.2386944    |\n",
            "| policy_loss        | -0.029509256 |\n",
            "| serial_timesteps   | 26112        |\n",
            "| time_elapsed       | 668          |\n",
            "| total_timesteps    | 208896       |\n",
            "| value_loss         | 0.79909784   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=0.58 +/- 0.47\n",
            "Episode length: 137.40 +/- 152.36\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024509527  |\n",
            "| clipfrac           | 0.27775878   |\n",
            "| ep_len_mean        | 111          |\n",
            "| ep_reward_mean     | 0.677        |\n",
            "| explained_variance | 0.748        |\n",
            "| fps                | 303          |\n",
            "| n_updates          | 52           |\n",
            "| policy_entropy     | 1.0849183    |\n",
            "| policy_loss        | -0.035158657 |\n",
            "| serial_timesteps   | 26624        |\n",
            "| time_elapsed       | 679          |\n",
            "| total_timesteps    | 212992       |\n",
            "| value_loss         | 0.9911424    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024787622  |\n",
            "| clipfrac           | 0.28168947   |\n",
            "| ep_len_mean        | 123          |\n",
            "| ep_reward_mean     | 0.64         |\n",
            "| explained_variance | 0.707        |\n",
            "| fps                | 380          |\n",
            "| n_updates          | 53           |\n",
            "| policy_entropy     | 1.124584     |\n",
            "| policy_loss        | -0.035029892 |\n",
            "| serial_timesteps   | 27136        |\n",
            "| time_elapsed       | 692          |\n",
            "| total_timesteps    | 217088       |\n",
            "| value_loss         | 0.8832229    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 262.00 +/- 124.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.027288595  |\n",
            "| clipfrac           | 0.30786133   |\n",
            "| ep_len_mean        | 105          |\n",
            "| ep_reward_mean     | 0.697        |\n",
            "| explained_variance | 0.635        |\n",
            "| fps                | 256          |\n",
            "| n_updates          | 54           |\n",
            "| policy_entropy     | 0.8133777    |\n",
            "| policy_loss        | -0.048570625 |\n",
            "| serial_timesteps   | 27648        |\n",
            "| time_elapsed       | 703          |\n",
            "| total_timesteps    | 221184       |\n",
            "| value_loss         | 1.3667407    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.029170806  |\n",
            "| clipfrac           | 0.2730957    |\n",
            "| ep_len_mean        | 89.7         |\n",
            "| ep_reward_mean     | 0.74         |\n",
            "| explained_variance | 0.806        |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 55           |\n",
            "| policy_entropy     | 1.117991     |\n",
            "| policy_loss        | -0.022516768 |\n",
            "| serial_timesteps   | 28160        |\n",
            "| time_elapsed       | 719          |\n",
            "| total_timesteps    | 225280       |\n",
            "| value_loss         | 0.7171017    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024413144  |\n",
            "| clipfrac           | 0.25371093   |\n",
            "| ep_len_mean        | 100          |\n",
            "| ep_reward_mean     | 0.706        |\n",
            "| explained_variance | 0.82         |\n",
            "| fps                | 378          |\n",
            "| n_updates          | 56           |\n",
            "| policy_entropy     | 1.0720093    |\n",
            "| policy_loss        | -0.020734359 |\n",
            "| serial_timesteps   | 28672        |\n",
            "| time_elapsed       | 730          |\n",
            "| total_timesteps    | 229376       |\n",
            "| value_loss         | 0.61110836   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=0.58 +/- 0.47\n",
            "Episode length: 137.40 +/- 152.36\n",
            "------------------------------------\n",
            "| approxkl           | 0.023093257 |\n",
            "| clipfrac           | 0.26398927  |\n",
            "| ep_len_mean        | 112         |\n",
            "| ep_reward_mean     | 0.673       |\n",
            "| explained_variance | 0.781       |\n",
            "| fps                | 304         |\n",
            "| n_updates          | 57          |\n",
            "| policy_entropy     | 1.1170598   |\n",
            "| policy_loss        | -0.02051486 |\n",
            "| serial_timesteps   | 29184       |\n",
            "| time_elapsed       | 740         |\n",
            "| total_timesteps    | 233472      |\n",
            "| value_loss         | 0.6316671   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022267021  |\n",
            "| clipfrac           | 0.24248047   |\n",
            "| ep_len_mean        | 97.4         |\n",
            "| ep_reward_mean     | 0.716        |\n",
            "| explained_variance | 0.839        |\n",
            "| fps                | 381          |\n",
            "| n_updates          | 58           |\n",
            "| policy_entropy     | 1.1100099    |\n",
            "| policy_loss        | -0.021506447 |\n",
            "| serial_timesteps   | 29696        |\n",
            "| time_elapsed       | 754          |\n",
            "| total_timesteps    | 237568       |\n",
            "| value_loss         | 0.6957799    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024286222  |\n",
            "| clipfrac           | 0.27324218   |\n",
            "| ep_len_mean        | 94.6         |\n",
            "| ep_reward_mean     | 0.723        |\n",
            "| explained_variance | 0.816        |\n",
            "| fps                | 234          |\n",
            "| n_updates          | 59           |\n",
            "| policy_entropy     | 1.1872636    |\n",
            "| policy_loss        | -0.021599457 |\n",
            "| serial_timesteps   | 30208        |\n",
            "| time_elapsed       | 765          |\n",
            "| total_timesteps    | 241664       |\n",
            "| value_loss         | 0.56327075   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.025759334  |\n",
            "| clipfrac           | 0.25075683   |\n",
            "| ep_len_mean        | 86.5         |\n",
            "| ep_reward_mean     | 0.748        |\n",
            "| explained_variance | 0.854        |\n",
            "| fps                | 372          |\n",
            "| n_updates          | 60           |\n",
            "| policy_entropy     | 1.1153535    |\n",
            "| policy_loss        | -0.019392172 |\n",
            "| serial_timesteps   | 30720        |\n",
            "| time_elapsed       | 782          |\n",
            "| total_timesteps    | 245760       |\n",
            "| value_loss         | 0.7320342    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020311398  |\n",
            "| clipfrac           | 0.22531739   |\n",
            "| ep_len_mean        | 72.6         |\n",
            "| ep_reward_mean     | 0.79         |\n",
            "| explained_variance | 0.836        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 61           |\n",
            "| policy_entropy     | 1.0581043    |\n",
            "| policy_loss        | -0.024003569 |\n",
            "| serial_timesteps   | 31232        |\n",
            "| time_elapsed       | 793          |\n",
            "| total_timesteps    | 249856       |\n",
            "| value_loss         | 1.0655242    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=0.38 +/- 0.47\n",
            "Episode length: 199.80 +/- 152.11\n",
            "------------------------------------\n",
            "| approxkl           | 0.037050012 |\n",
            "| clipfrac           | 0.24645996  |\n",
            "| ep_len_mean        | 55.5        |\n",
            "| ep_reward_mean     | 0.842       |\n",
            "| explained_variance | 0.833       |\n",
            "| fps                | 271         |\n",
            "| n_updates          | 62          |\n",
            "| policy_entropy     | 0.86906636  |\n",
            "| policy_loss        | -0.01859067 |\n",
            "| serial_timesteps   | 31744       |\n",
            "| time_elapsed       | 804         |\n",
            "| total_timesteps    | 253952      |\n",
            "| value_loss         | 0.8944518   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.07425196   |\n",
            "| clipfrac           | 0.23237304   |\n",
            "| ep_len_mean        | 63           |\n",
            "| ep_reward_mean     | 0.821        |\n",
            "| explained_variance | 0.866        |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 63           |\n",
            "| policy_entropy     | 0.83259785   |\n",
            "| policy_loss        | -0.026878184 |\n",
            "| serial_timesteps   | 32256        |\n",
            "| time_elapsed       | 819          |\n",
            "| total_timesteps    | 258048       |\n",
            "| value_loss         | 0.7976868    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.40 +/- 150.77\n",
            "-------------------------------------\n",
            "| approxkl           | 0.05965265   |\n",
            "| clipfrac           | 0.2759033    |\n",
            "| ep_len_mean        | 49.6         |\n",
            "| ep_reward_mean     | 0.858        |\n",
            "| explained_variance | 0.835        |\n",
            "| fps                | 295          |\n",
            "| n_updates          | 64           |\n",
            "| policy_entropy     | 0.8358256    |\n",
            "| policy_loss        | -0.013001929 |\n",
            "| serial_timesteps   | 32768        |\n",
            "| time_elapsed       | 830          |\n",
            "| total_timesteps    | 262144       |\n",
            "| value_loss         | 1.0040992    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.13267009   |\n",
            "| clipfrac           | 0.21442871   |\n",
            "| ep_len_mean        | 42.8         |\n",
            "| ep_reward_mean     | 0.878        |\n",
            "| explained_variance | 0.828        |\n",
            "| fps                | 381          |\n",
            "| n_updates          | 65           |\n",
            "| policy_entropy     | 0.59529066   |\n",
            "| policy_loss        | -0.024531484 |\n",
            "| serial_timesteps   | 33280        |\n",
            "| time_elapsed       | 844          |\n",
            "| total_timesteps    | 266240       |\n",
            "| value_loss         | 0.8037259    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=0.38 +/- 0.47\n",
            "Episode length: 199.80 +/- 152.11\n",
            "-------------------------------------\n",
            "| approxkl           | 0.043980192  |\n",
            "| clipfrac           | 0.2173584    |\n",
            "| ep_len_mean        | 47           |\n",
            "| ep_reward_mean     | 0.867        |\n",
            "| explained_variance | 0.769        |\n",
            "| fps                | 273          |\n",
            "| n_updates          | 66           |\n",
            "| policy_entropy     | 0.5302576    |\n",
            "| policy_loss        | -0.018460477 |\n",
            "| serial_timesteps   | 33792        |\n",
            "| time_elapsed       | 854          |\n",
            "| total_timesteps    | 270336       |\n",
            "| value_loss         | 0.8119275    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.035945892  |\n",
            "| clipfrac           | 0.2288086    |\n",
            "| ep_len_mean        | 41.2         |\n",
            "| ep_reward_mean     | 0.884        |\n",
            "| explained_variance | 0.776        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 67           |\n",
            "| policy_entropy     | 0.53809273   |\n",
            "| policy_loss        | -0.022274049 |\n",
            "| serial_timesteps   | 34304        |\n",
            "| time_elapsed       | 869          |\n",
            "| total_timesteps    | 274432       |\n",
            "| value_loss         | 0.78191745   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.07412573   |\n",
            "| clipfrac           | 0.22231445   |\n",
            "| ep_len_mean        | 37.1         |\n",
            "| ep_reward_mean     | 0.895        |\n",
            "| explained_variance | 0.8          |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 68           |\n",
            "| policy_entropy     | 0.54929996   |\n",
            "| policy_loss        | -0.021518487 |\n",
            "| serial_timesteps   | 34816        |\n",
            "| time_elapsed       | 880          |\n",
            "| total_timesteps    | 278528       |\n",
            "| value_loss         | 0.78574765   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=0.19 +/- 0.37\n",
            "Episode length: 264.20 +/- 119.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0599397    |\n",
            "| clipfrac           | 0.26662597   |\n",
            "| ep_len_mean        | 42           |\n",
            "| ep_reward_mean     | 0.879        |\n",
            "| explained_variance | 0.87         |\n",
            "| fps                | 249          |\n",
            "| n_updates          | 69           |\n",
            "| policy_entropy     | 0.64958805   |\n",
            "| policy_loss        | -0.021775274 |\n",
            "| serial_timesteps   | 35328        |\n",
            "| time_elapsed       | 891          |\n",
            "| total_timesteps    | 282624       |\n",
            "| value_loss         | 0.55254656   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0316403    |\n",
            "| clipfrac           | 0.22565918   |\n",
            "| ep_len_mean        | 37.8         |\n",
            "| ep_reward_mean     | 0.891        |\n",
            "| explained_variance | 0.842        |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 70           |\n",
            "| policy_entropy     | 0.67569387   |\n",
            "| policy_loss        | -0.010673648 |\n",
            "| serial_timesteps   | 35840        |\n",
            "| time_elapsed       | 908          |\n",
            "| total_timesteps    | 286720       |\n",
            "| value_loss         | 0.5712358    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 262.00 +/- 124.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.027659014  |\n",
            "| clipfrac           | 0.20646973   |\n",
            "| ep_len_mean        | 28.7         |\n",
            "| ep_reward_mean     | 0.919        |\n",
            "| explained_variance | 0.883        |\n",
            "| fps                | 253          |\n",
            "| n_updates          | 71           |\n",
            "| policy_entropy     | 0.5473081    |\n",
            "| policy_loss        | -0.012923581 |\n",
            "| serial_timesteps   | 36352        |\n",
            "| time_elapsed       | 918          |\n",
            "| total_timesteps    | 290816       |\n",
            "| value_loss         | 0.5595037    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.24928753   |\n",
            "| clipfrac           | 0.30407715   |\n",
            "| ep_len_mean        | 40.3         |\n",
            "| ep_reward_mean     | 0.885        |\n",
            "| explained_variance | 0.848        |\n",
            "| fps                | 381          |\n",
            "| n_updates          | 72           |\n",
            "| policy_entropy     | 0.6230219    |\n",
            "| policy_loss        | -0.027563253 |\n",
            "| serial_timesteps   | 36864        |\n",
            "| time_elapsed       | 935          |\n",
            "| total_timesteps    | 294912       |\n",
            "| value_loss         | 0.58365875   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03127237   |\n",
            "| clipfrac           | 0.21945801   |\n",
            "| ep_len_mean        | 28.6         |\n",
            "| ep_reward_mean     | 0.918        |\n",
            "| explained_variance | 0.876        |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 73           |\n",
            "| policy_entropy     | 0.5713854    |\n",
            "| policy_loss        | -0.019528307 |\n",
            "| serial_timesteps   | 37376        |\n",
            "| time_elapsed       | 945          |\n",
            "| total_timesteps    | 299008       |\n",
            "| value_loss         | 0.4664673    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 75.80 +/- 124.10\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.025587112  |\n",
            "| clipfrac           | 0.16928712   |\n",
            "| ep_len_mean        | 32.3         |\n",
            "| ep_reward_mean     | 0.907        |\n",
            "| explained_variance | 0.846        |\n",
            "| fps                | 328          |\n",
            "| n_updates          | 74           |\n",
            "| policy_entropy     | 0.5158381    |\n",
            "| policy_loss        | -0.010940227 |\n",
            "| serial_timesteps   | 37888        |\n",
            "| time_elapsed       | 956          |\n",
            "| total_timesteps    | 303104       |\n",
            "| value_loss         | 0.5757797    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03954165   |\n",
            "| clipfrac           | 0.18923339   |\n",
            "| ep_len_mean        | 24.6         |\n",
            "| ep_reward_mean     | 0.932        |\n",
            "| explained_variance | 0.663        |\n",
            "| fps                | 381          |\n",
            "| n_updates          | 75           |\n",
            "| policy_entropy     | 0.30858815   |\n",
            "| policy_loss        | 0.0014191493 |\n",
            "| serial_timesteps   | 38400        |\n",
            "| time_elapsed       | 969          |\n",
            "| total_timesteps    | 307200       |\n",
            "| value_loss         | 0.35861513   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=0.19 +/- 0.38\n",
            "Episode length: 263.00 +/- 122.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04609086   |\n",
            "| clipfrac           | 0.18554688   |\n",
            "| ep_len_mean        | 33.1         |\n",
            "| ep_reward_mean     | 0.905        |\n",
            "| explained_variance | 0.499        |\n",
            "| fps                | 257          |\n",
            "| n_updates          | 76           |\n",
            "| policy_entropy     | 0.30875728   |\n",
            "| policy_loss        | -0.017432595 |\n",
            "| serial_timesteps   | 38912        |\n",
            "| time_elapsed       | 979          |\n",
            "| total_timesteps    | 311296       |\n",
            "| value_loss         | 0.5775134    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.16260234   |\n",
            "| clipfrac           | 0.19160156   |\n",
            "| ep_len_mean        | 26.4         |\n",
            "| ep_reward_mean     | 0.927        |\n",
            "| explained_variance | 0.555        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 77           |\n",
            "| policy_entropy     | 0.25530505   |\n",
            "| policy_loss        | -0.024920236 |\n",
            "| serial_timesteps   | 39424        |\n",
            "| time_elapsed       | 995          |\n",
            "| total_timesteps    | 315392       |\n",
            "| value_loss         | 0.2931524    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03924985   |\n",
            "| clipfrac           | 0.13649902   |\n",
            "| ep_len_mean        | 23.9         |\n",
            "| ep_reward_mean     | 0.933        |\n",
            "| explained_variance | 0.508        |\n",
            "| fps                | 378          |\n",
            "| n_updates          | 78           |\n",
            "| policy_entropy     | 0.2412935    |\n",
            "| policy_loss        | -0.014132045 |\n",
            "| serial_timesteps   | 39936        |\n",
            "| time_elapsed       | 1.01e+03     |\n",
            "| total_timesteps    | 319488       |\n",
            "| value_loss         | 0.3420511    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.60 +/- 1.85\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.042064093  |\n",
            "| clipfrac           | 0.15227051   |\n",
            "| ep_len_mean        | 23.3         |\n",
            "| ep_reward_mean     | 0.935        |\n",
            "| explained_variance | 0.581        |\n",
            "| fps                | 362          |\n",
            "| n_updates          | 79           |\n",
            "| policy_entropy     | 0.288876     |\n",
            "| policy_loss        | -0.011572061 |\n",
            "| serial_timesteps   | 40448        |\n",
            "| time_elapsed       | 1.02e+03     |\n",
            "| total_timesteps    | 323584       |\n",
            "| value_loss         | 0.2945725    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.08402618   |\n",
            "| clipfrac           | 0.12807617   |\n",
            "| ep_len_mean        | 22.5         |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.459        |\n",
            "| fps                | 372          |\n",
            "| n_updates          | 80           |\n",
            "| policy_entropy     | 0.21447138   |\n",
            "| policy_loss        | -0.019819722 |\n",
            "| serial_timesteps   | 40960        |\n",
            "| time_elapsed       | 1.03e+03     |\n",
            "| total_timesteps    | 327680       |\n",
            "| value_loss         | 0.31420603   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.20 +/- 122.94\n",
            "-------------------------------------\n",
            "| approxkl           | 0.05683147   |\n",
            "| clipfrac           | 0.2064209    |\n",
            "| ep_len_mean        | 27.6         |\n",
            "| ep_reward_mean     | 0.921        |\n",
            "| explained_variance | 0.399        |\n",
            "| fps                | 326          |\n",
            "| n_updates          | 81           |\n",
            "| policy_entropy     | 0.25486577   |\n",
            "| policy_loss        | -0.030628975 |\n",
            "| serial_timesteps   | 41472        |\n",
            "| time_elapsed       | 1.04e+03     |\n",
            "| total_timesteps    | 331776       |\n",
            "| value_loss         | 0.45451817   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.029546466  |\n",
            "| clipfrac           | 0.09963379   |\n",
            "| ep_len_mean        | 16.7         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.646        |\n",
            "| fps                | 370          |\n",
            "| n_updates          | 82           |\n",
            "| policy_entropy     | 0.13840318   |\n",
            "| policy_loss        | -0.009327209 |\n",
            "| serial_timesteps   | 41984        |\n",
            "| time_elapsed       | 1.05e+03     |\n",
            "| total_timesteps    | 335872       |\n",
            "| value_loss         | 0.08625309   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03921523   |\n",
            "| clipfrac           | 0.12216797   |\n",
            "| ep_len_mean        | 19           |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.804        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 83           |\n",
            "| policy_entropy     | 0.19785513   |\n",
            "| policy_loss        | -0.019216668 |\n",
            "| serial_timesteps   | 42496        |\n",
            "| time_elapsed       | 1.06e+03     |\n",
            "| total_timesteps    | 339968       |\n",
            "| value_loss         | 0.08063899   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.00 +/- 1.55\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.056185137  |\n",
            "| clipfrac           | 0.14421387   |\n",
            "| ep_len_mean        | 19.4         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.798        |\n",
            "| fps                | 367          |\n",
            "| n_updates          | 84           |\n",
            "| policy_entropy     | 0.26754987   |\n",
            "| policy_loss        | -0.014127968 |\n",
            "| serial_timesteps   | 43008        |\n",
            "| time_elapsed       | 1.07e+03     |\n",
            "| total_timesteps    | 344064       |\n",
            "| value_loss         | 0.1422616    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0364184    |\n",
            "| clipfrac           | 0.13571778   |\n",
            "| ep_len_mean        | 19.6         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.756        |\n",
            "| fps                | 373          |\n",
            "| n_updates          | 85           |\n",
            "| policy_entropy     | 0.24071923   |\n",
            "| policy_loss        | -0.021363648 |\n",
            "| serial_timesteps   | 43520        |\n",
            "| time_elapsed       | 1.08e+03     |\n",
            "| total_timesteps    | 348160       |\n",
            "| value_loss         | 0.1745834    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.20 +/- 2.48\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014291677  |\n",
            "| clipfrac           | 0.051171876  |\n",
            "| ep_len_mean        | 17.1         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.746        |\n",
            "| fps                | 359          |\n",
            "| n_updates          | 86           |\n",
            "| policy_entropy     | 0.0957243    |\n",
            "| policy_loss        | -0.005453353 |\n",
            "| serial_timesteps   | 44032        |\n",
            "| time_elapsed       | 1.1e+03      |\n",
            "| total_timesteps    | 352256       |\n",
            "| value_loss         | 0.045235183  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.124772094   |\n",
            "| clipfrac           | 0.08706055    |\n",
            "| ep_len_mean        | 18.4          |\n",
            "| ep_reward_mean     | 0.949         |\n",
            "| explained_variance | 0.743         |\n",
            "| fps                | 374           |\n",
            "| n_updates          | 87            |\n",
            "| policy_entropy     | 0.12956646    |\n",
            "| policy_loss        | -0.0131751895 |\n",
            "| serial_timesteps   | 44544         |\n",
            "| time_elapsed       | 1.11e+03      |\n",
            "| total_timesteps    | 356352        |\n",
            "| value_loss         | 0.067619205   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.60 +/- 1.20\n",
            "--------------------------------------\n",
            "| approxkl           | 0.03209102    |\n",
            "| clipfrac           | 0.08771972    |\n",
            "| ep_len_mean        | 16.6          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.815         |\n",
            "| fps                | 361           |\n",
            "| n_updates          | 88            |\n",
            "| policy_entropy     | 0.18380353    |\n",
            "| policy_loss        | -7.655879e-07 |\n",
            "| serial_timesteps   | 45056         |\n",
            "| time_elapsed       | 1.12e+03      |\n",
            "| total_timesteps    | 360448        |\n",
            "| value_loss         | 0.081681855   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.039242204  |\n",
            "| clipfrac           | 0.21726075   |\n",
            "| ep_len_mean        | 27.4         |\n",
            "| ep_reward_mean     | 0.922        |\n",
            "| explained_variance | 0.717        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 89           |\n",
            "| policy_entropy     | 0.42506504   |\n",
            "| policy_loss        | -0.031213328 |\n",
            "| serial_timesteps   | 45568        |\n",
            "| time_elapsed       | 1.13e+03     |\n",
            "| total_timesteps    | 364544       |\n",
            "| value_loss         | 0.40710735   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.030124158 |\n",
            "| clipfrac           | 0.08918457  |\n",
            "| ep_len_mean        | 18.2        |\n",
            "| ep_reward_mean     | 0.949       |\n",
            "| explained_variance | 0.777       |\n",
            "| fps                | 373         |\n",
            "| n_updates          | 90          |\n",
            "| policy_entropy     | 0.18839064  |\n",
            "| policy_loss        | -0.01927866 |\n",
            "| serial_timesteps   | 46080       |\n",
            "| time_elapsed       | 1.14e+03    |\n",
            "| total_timesteps    | 368640      |\n",
            "| value_loss         | 0.062176775 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 3.31\n",
            "-------------------------------------\n",
            "| approxkl           | 0.047848873  |\n",
            "| clipfrac           | 0.20915528   |\n",
            "| ep_len_mean        | 24.9         |\n",
            "| ep_reward_mean     | 0.93         |\n",
            "| explained_variance | 0.842        |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 91           |\n",
            "| policy_entropy     | 0.4299221    |\n",
            "| policy_loss        | -0.028432056 |\n",
            "| serial_timesteps   | 46592        |\n",
            "| time_elapsed       | 1.15e+03     |\n",
            "| total_timesteps    | 372736       |\n",
            "| value_loss         | 0.19461659   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.06756126   |\n",
            "| clipfrac           | 0.21982422   |\n",
            "| ep_len_mean        | 29.5         |\n",
            "| ep_reward_mean     | 0.915        |\n",
            "| explained_variance | 0.872        |\n",
            "| fps                | 373          |\n",
            "| n_updates          | 92           |\n",
            "| policy_entropy     | 0.5087983    |\n",
            "| policy_loss        | -0.019976709 |\n",
            "| serial_timesteps   | 47104        |\n",
            "| time_elapsed       | 1.16e+03     |\n",
            "| total_timesteps    | 376832       |\n",
            "| value_loss         | 0.25772673   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.80 +/- 3.31\n",
            "------------------------------------\n",
            "| approxkl           | 0.025703881 |\n",
            "| clipfrac           | 0.15422364  |\n",
            "| ep_len_mean        | 23.3        |\n",
            "| ep_reward_mean     | 0.933       |\n",
            "| explained_variance | 0.884       |\n",
            "| fps                | 363         |\n",
            "| n_updates          | 93          |\n",
            "| policy_entropy     | 0.5088734   |\n",
            "| policy_loss        | -0.02160874 |\n",
            "| serial_timesteps   | 47616       |\n",
            "| time_elapsed       | 1.17e+03    |\n",
            "| total_timesteps    | 380928      |\n",
            "| value_loss         | 0.23495527  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.017009621 |\n",
            "| clipfrac           | 0.15512696  |\n",
            "| ep_len_mean        | 21.7        |\n",
            "| ep_reward_mean     | 0.939       |\n",
            "| explained_variance | 0.893       |\n",
            "| fps                | 377         |\n",
            "| n_updates          | 94          |\n",
            "| policy_entropy     | 0.44180393  |\n",
            "| policy_loss        | -0.02814171 |\n",
            "| serial_timesteps   | 48128       |\n",
            "| time_elapsed       | 1.18e+03    |\n",
            "| total_timesteps    | 385024      |\n",
            "| value_loss         | 0.13891548  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014267424  |\n",
            "| clipfrac           | 0.11428223   |\n",
            "| ep_len_mean        | 20.1         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.918        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 95           |\n",
            "| policy_entropy     | 0.43094105   |\n",
            "| policy_loss        | -0.016550627 |\n",
            "| serial_timesteps   | 48640        |\n",
            "| time_elapsed       | 1.2e+03      |\n",
            "| total_timesteps    | 389120       |\n",
            "| value_loss         | 0.11802195   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.20 +/- 1.94\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026178584  |\n",
            "| clipfrac           | 0.2078125    |\n",
            "| ep_len_mean        | 28.2         |\n",
            "| ep_reward_mean     | 0.918        |\n",
            "| explained_variance | 0.919        |\n",
            "| fps                | 367          |\n",
            "| n_updates          | 96           |\n",
            "| policy_entropy     | 0.7560546    |\n",
            "| policy_loss        | -0.020483572 |\n",
            "| serial_timesteps   | 49152        |\n",
            "| time_elapsed       | 1.21e+03     |\n",
            "| total_timesteps    | 393216       |\n",
            "| value_loss         | 0.22003019   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.030404678  |\n",
            "| clipfrac           | 0.07551269   |\n",
            "| ep_len_mean        | 18           |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.919        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 97           |\n",
            "| policy_entropy     | 0.23497756   |\n",
            "| policy_loss        | -0.004933331 |\n",
            "| serial_timesteps   | 49664        |\n",
            "| time_elapsed       | 1.22e+03     |\n",
            "| total_timesteps    | 397312       |\n",
            "| value_loss         | 0.065384254  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.20 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018937498  |\n",
            "| clipfrac           | 0.09636231   |\n",
            "| ep_len_mean        | 20.1         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.937        |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 98           |\n",
            "| policy_entropy     | 0.29562837   |\n",
            "| policy_loss        | -0.019678432 |\n",
            "| serial_timesteps   | 50176        |\n",
            "| time_elapsed       | 1.23e+03     |\n",
            "| total_timesteps    | 401408       |\n",
            "| value_loss         | 0.07568418   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.032396022 |\n",
            "| clipfrac           | 0.05246582  |\n",
            "| ep_len_mean        | 16.1        |\n",
            "| ep_reward_mean     | 0.955       |\n",
            "| explained_variance | 0.726       |\n",
            "| fps                | 374         |\n",
            "| n_updates          | 99          |\n",
            "| policy_entropy     | 0.100148395 |\n",
            "| policy_loss        | -0.01822352 |\n",
            "| serial_timesteps   | 50688       |\n",
            "| time_elapsed       | 1.24e+03    |\n",
            "| total_timesteps    | 405504      |\n",
            "| value_loss         | 0.046133295 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.025775915   |\n",
            "| clipfrac           | 0.057006836   |\n",
            "| ep_len_mean        | 17.1          |\n",
            "| ep_reward_mean     | 0.952         |\n",
            "| explained_variance | 0.927         |\n",
            "| fps                | 371           |\n",
            "| n_updates          | 100           |\n",
            "| policy_entropy     | 0.10006337    |\n",
            "| policy_loss        | -0.0056209546 |\n",
            "| serial_timesteps   | 51200         |\n",
            "| time_elapsed       | 1.25e+03      |\n",
            "| total_timesteps    | 409600        |\n",
            "| value_loss         | 0.014646413   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 78.80 +/- 122.61\n",
            "-------------------------------------\n",
            "| approxkl           | 0.08452435   |\n",
            "| clipfrac           | 0.22468261   |\n",
            "| ep_len_mean        | 29.7         |\n",
            "| ep_reward_mean     | 0.917        |\n",
            "| explained_variance | 0.485        |\n",
            "| fps                | 333          |\n",
            "| n_updates          | 101          |\n",
            "| policy_entropy     | 0.4303511    |\n",
            "| policy_loss        | -0.023406733 |\n",
            "| serial_timesteps   | 51712        |\n",
            "| time_elapsed       | 1.26e+03     |\n",
            "| total_timesteps    | 413696       |\n",
            "| value_loss         | 0.33072698   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03707461   |\n",
            "| clipfrac           | 0.11401367   |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.86         |\n",
            "| fps                | 372          |\n",
            "| n_updates          | 102          |\n",
            "| policy_entropy     | 0.23204055   |\n",
            "| policy_loss        | -0.018713802 |\n",
            "| serial_timesteps   | 52224        |\n",
            "| time_elapsed       | 1.27e+03     |\n",
            "| total_timesteps    | 417792       |\n",
            "| value_loss         | 0.072774395  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.20 +/- 1.94\n",
            "-------------------------------------\n",
            "| approxkl           | 0.058932543  |\n",
            "| clipfrac           | 0.18461914   |\n",
            "| ep_len_mean        | 25.1         |\n",
            "| ep_reward_mean     | 0.928        |\n",
            "| explained_variance | 0.858        |\n",
            "| fps                | 364          |\n",
            "| n_updates          | 103          |\n",
            "| policy_entropy     | 0.25909996   |\n",
            "| policy_loss        | -0.036654238 |\n",
            "| serial_timesteps   | 52736        |\n",
            "| time_elapsed       | 1.28e+03     |\n",
            "| total_timesteps    | 421888       |\n",
            "| value_loss         | 0.14838381   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.033800885  |\n",
            "| clipfrac           | 0.13212891   |\n",
            "| ep_len_mean        | 23           |\n",
            "| ep_reward_mean     | 0.935        |\n",
            "| explained_variance | 0.826        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 104          |\n",
            "| policy_entropy     | 0.23795545   |\n",
            "| policy_loss        | -0.028300751 |\n",
            "| serial_timesteps   | 53248        |\n",
            "| time_elapsed       | 1.3e+03      |\n",
            "| total_timesteps    | 425984       |\n",
            "| value_loss         | 0.11369876   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.00 +/- 1.67\n",
            "-------------------------------------\n",
            "| approxkl           | 0.050251924  |\n",
            "| clipfrac           | 0.14221191   |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.797        |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 105          |\n",
            "| policy_entropy     | 0.2573086    |\n",
            "| policy_loss        | -0.042375527 |\n",
            "| serial_timesteps   | 53760        |\n",
            "| time_elapsed       | 1.31e+03     |\n",
            "| total_timesteps    | 430080       |\n",
            "| value_loss         | 0.06914608   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.029051926  |\n",
            "| clipfrac           | 0.1506836    |\n",
            "| ep_len_mean        | 18.2         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.744        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 106          |\n",
            "| policy_entropy     | 0.37412474   |\n",
            "| policy_loss        | -0.018636001 |\n",
            "| serial_timesteps   | 54272        |\n",
            "| time_elapsed       | 1.32e+03     |\n",
            "| total_timesteps    | 434176       |\n",
            "| value_loss         | 0.14185977   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02728258   |\n",
            "| clipfrac           | 0.1685547    |\n",
            "| ep_len_mean        | 21.1         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.825        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 107          |\n",
            "| policy_entropy     | 0.4372983    |\n",
            "| policy_loss        | -0.023695868 |\n",
            "| serial_timesteps   | 54784        |\n",
            "| time_elapsed       | 1.33e+03     |\n",
            "| total_timesteps    | 438272       |\n",
            "| value_loss         | 0.15250483   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.40 +/- 3.44\n",
            "-------------------------------------\n",
            "| approxkl           | 0.088277884  |\n",
            "| clipfrac           | 0.22072753   |\n",
            "| ep_len_mean        | 24.7         |\n",
            "| ep_reward_mean     | 0.929        |\n",
            "| explained_variance | 0.856        |\n",
            "| fps                | 356          |\n",
            "| n_updates          | 108          |\n",
            "| policy_entropy     | 0.60235846   |\n",
            "| policy_loss        | -0.018540177 |\n",
            "| serial_timesteps   | 55296        |\n",
            "| time_elapsed       | 1.34e+03     |\n",
            "| total_timesteps    | 442368       |\n",
            "| value_loss         | 0.1944451    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024802826  |\n",
            "| clipfrac           | 0.14733887   |\n",
            "| ep_len_mean        | 22           |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.92         |\n",
            "| fps                | 373          |\n",
            "| n_updates          | 109          |\n",
            "| policy_entropy     | 0.53250736   |\n",
            "| policy_loss        | -0.019718898 |\n",
            "| serial_timesteps   | 55808        |\n",
            "| time_elapsed       | 1.35e+03     |\n",
            "| total_timesteps    | 446464       |\n",
            "| value_loss         | 0.11383128   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.20 +/- 1.47\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015268748  |\n",
            "| clipfrac           | 0.13034669   |\n",
            "| ep_len_mean        | 28           |\n",
            "| ep_reward_mean     | 0.919        |\n",
            "| explained_variance | 0.929        |\n",
            "| fps                | 361          |\n",
            "| n_updates          | 110          |\n",
            "| policy_entropy     | 0.54650813   |\n",
            "| policy_loss        | -0.014172368 |\n",
            "| serial_timesteps   | 56320        |\n",
            "| time_elapsed       | 1.36e+03     |\n",
            "| total_timesteps    | 450560       |\n",
            "| value_loss         | 0.10930227   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.030948946  |\n",
            "| clipfrac           | 0.113012694  |\n",
            "| ep_len_mean        | 21.7         |\n",
            "| ep_reward_mean     | 0.939        |\n",
            "| explained_variance | 0.935        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 111          |\n",
            "| policy_entropy     | 0.2890771    |\n",
            "| policy_loss        | -0.030807415 |\n",
            "| serial_timesteps   | 56832        |\n",
            "| time_elapsed       | 1.37e+03     |\n",
            "| total_timesteps    | 454656       |\n",
            "| value_loss         | 0.05770843   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01377167   |\n",
            "| clipfrac           | 0.11376953   |\n",
            "| ep_len_mean        | 21.5         |\n",
            "| ep_reward_mean     | 0.939        |\n",
            "| explained_variance | 0.967        |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 112          |\n",
            "| policy_entropy     | 0.34563172   |\n",
            "| policy_loss        | -0.027706534 |\n",
            "| serial_timesteps   | 57344        |\n",
            "| time_elapsed       | 1.38e+03     |\n",
            "| total_timesteps    | 458752       |\n",
            "| value_loss         | 0.034696914  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.60 +/- 1.62\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010956458  |\n",
            "| clipfrac           | 0.0842041    |\n",
            "| ep_len_mean        | 15.9         |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.969        |\n",
            "| fps                | 359          |\n",
            "| n_updates          | 113          |\n",
            "| policy_entropy     | 0.27645212   |\n",
            "| policy_loss        | -0.024942078 |\n",
            "| serial_timesteps   | 57856        |\n",
            "| time_elapsed       | 1.4e+03      |\n",
            "| total_timesteps    | 462848       |\n",
            "| value_loss         | 0.032055378  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.030718485 |\n",
            "| clipfrac           | 0.11657715  |\n",
            "| ep_len_mean        | 18.9        |\n",
            "| ep_reward_mean     | 0.947       |\n",
            "| explained_variance | 0.98        |\n",
            "| fps                | 373         |\n",
            "| n_updates          | 114         |\n",
            "| policy_entropy     | 0.37950286  |\n",
            "| policy_loss        | -0.02799036 |\n",
            "| serial_timesteps   | 58368       |\n",
            "| time_elapsed       | 1.41e+03    |\n",
            "| total_timesteps    | 466944      |\n",
            "| value_loss         | 0.021436488 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 1.36\n",
            "------------------------------------\n",
            "| approxkl           | 0.09523912  |\n",
            "| clipfrac           | 0.1505371   |\n",
            "| ep_len_mean        | 21.8        |\n",
            "| ep_reward_mean     | 0.939       |\n",
            "| explained_variance | 0.933       |\n",
            "| fps                | 365         |\n",
            "| n_updates          | 115         |\n",
            "| policy_entropy     | 0.33414435  |\n",
            "| policy_loss        | -0.01061831 |\n",
            "| serial_timesteps   | 58880       |\n",
            "| time_elapsed       | 1.42e+03    |\n",
            "| total_timesteps    | 471040      |\n",
            "| value_loss         | 0.07113542  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019039024  |\n",
            "| clipfrac           | 0.0670166    |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.962        |\n",
            "| fps                | 374          |\n",
            "| n_updates          | 116          |\n",
            "| policy_entropy     | 0.18815617   |\n",
            "| policy_loss        | -0.012460986 |\n",
            "| serial_timesteps   | 59392        |\n",
            "| time_elapsed       | 1.43e+03     |\n",
            "| total_timesteps    | 475136       |\n",
            "| value_loss         | 0.02389877   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04278847   |\n",
            "| clipfrac           | 0.13842773   |\n",
            "| ep_len_mean        | 21.3         |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.918        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 117          |\n",
            "| policy_entropy     | 0.3413617    |\n",
            "| policy_loss        | -0.021517638 |\n",
            "| serial_timesteps   | 59904        |\n",
            "| time_elapsed       | 1.44e+03     |\n",
            "| total_timesteps    | 479232       |\n",
            "| value_loss         | 0.08676824   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.60 +/- 1.36\n",
            "------------------------------------\n",
            "| approxkl           | 0.0604981   |\n",
            "| clipfrac           | 0.20678711  |\n",
            "| ep_len_mean        | 24.9        |\n",
            "| ep_reward_mean     | 0.93        |\n",
            "| explained_variance | 0.755       |\n",
            "| fps                | 366         |\n",
            "| n_updates          | 118         |\n",
            "| policy_entropy     | 0.392204    |\n",
            "| policy_loss        | -0.02852481 |\n",
            "| serial_timesteps   | 60416       |\n",
            "| time_elapsed       | 1.45e+03    |\n",
            "| total_timesteps    | 483328      |\n",
            "| value_loss         | 0.19188508  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020159498  |\n",
            "| clipfrac           | 0.12116699   |\n",
            "| ep_len_mean        | 18.1         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.753        |\n",
            "| fps                | 378          |\n",
            "| n_updates          | 119          |\n",
            "| policy_entropy     | 0.273382     |\n",
            "| policy_loss        | -0.026747141 |\n",
            "| serial_timesteps   | 60928        |\n",
            "| time_elapsed       | 1.46e+03     |\n",
            "| total_timesteps    | 487424       |\n",
            "| value_loss         | 0.11056598   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.60 +/- 0.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.021325557  |\n",
            "| clipfrac           | 0.07800293   |\n",
            "| ep_len_mean        | 18.3         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.72         |\n",
            "| fps                | 369          |\n",
            "| n_updates          | 120          |\n",
            "| policy_entropy     | 0.15348738   |\n",
            "| policy_loss        | -0.014932223 |\n",
            "| serial_timesteps   | 61440        |\n",
            "| time_elapsed       | 1.47e+03     |\n",
            "| total_timesteps    | 491520       |\n",
            "| value_loss         | 0.04781481   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.024470976 |\n",
            "| clipfrac           | 0.10056152  |\n",
            "| ep_len_mean        | 17.1        |\n",
            "| ep_reward_mean     | 0.952       |\n",
            "| explained_variance | 0.913       |\n",
            "| fps                | 380         |\n",
            "| n_updates          | 121         |\n",
            "| policy_entropy     | 0.16893804  |\n",
            "| policy_loss        | -0.0237188  |\n",
            "| serial_timesteps   | 61952       |\n",
            "| time_elapsed       | 1.48e+03    |\n",
            "| total_timesteps    | 495616      |\n",
            "| value_loss         | 0.02461503  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.024399694  |\n",
            "| clipfrac           | 0.09345703   |\n",
            "| ep_len_mean        | 18.5         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.777        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 122          |\n",
            "| policy_entropy     | 0.20929614   |\n",
            "| policy_loss        | -0.020240718 |\n",
            "| serial_timesteps   | 62464        |\n",
            "| time_elapsed       | 1.49e+03     |\n",
            "| total_timesteps    | 499712       |\n",
            "| value_loss         | 0.04762931   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.00 +/- 124.00\n",
            "------------------------------------\n",
            "| approxkl           | 0.04724237  |\n",
            "| clipfrac           | 0.15507813  |\n",
            "| ep_len_mean        | 21.4        |\n",
            "| ep_reward_mean     | 0.94        |\n",
            "| explained_variance | 0.668       |\n",
            "| fps                | 331         |\n",
            "| n_updates          | 123         |\n",
            "| policy_entropy     | 0.2755957   |\n",
            "| policy_loss        | -0.03211061 |\n",
            "| serial_timesteps   | 62976       |\n",
            "| time_elapsed       | 1.51e+03    |\n",
            "| total_timesteps    | 503808      |\n",
            "| value_loss         | 0.06768489  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.022565369 |\n",
            "| clipfrac           | 0.13034669  |\n",
            "| ep_len_mean        | 19.8        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.704       |\n",
            "| fps                | 372         |\n",
            "| n_updates          | 124         |\n",
            "| policy_entropy     | 0.3046047   |\n",
            "| policy_loss        | -0.03243779 |\n",
            "| serial_timesteps   | 63488       |\n",
            "| time_elapsed       | 1.52e+03    |\n",
            "| total_timesteps    | 507904      |\n",
            "| value_loss         | 0.059531968 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 75.80 +/- 124.10\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03437998   |\n",
            "| clipfrac           | 0.107861325  |\n",
            "| ep_len_mean        | 18           |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.816        |\n",
            "| fps                | 328          |\n",
            "| n_updates          | 125          |\n",
            "| policy_entropy     | 0.20141144   |\n",
            "| policy_loss        | -0.028491478 |\n",
            "| serial_timesteps   | 64000        |\n",
            "| time_elapsed       | 1.53e+03     |\n",
            "| total_timesteps    | 512000       |\n",
            "| value_loss         | 0.022446427  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.017294409 |\n",
            "| clipfrac           | 0.0857666   |\n",
            "| ep_len_mean        | 17.4        |\n",
            "| ep_reward_mean     | 0.952       |\n",
            "| explained_variance | 0.878       |\n",
            "| fps                | 375         |\n",
            "| n_updates          | 126         |\n",
            "| policy_entropy     | 0.21004733  |\n",
            "| policy_loss        | -0.01500128 |\n",
            "| serial_timesteps   | 64512       |\n",
            "| time_elapsed       | 1.54e+03    |\n",
            "| total_timesteps    | 516096      |\n",
            "| value_loss         | 0.016494945 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 17.00 +/- 1.79\n",
            "------------------------------------\n",
            "| approxkl           | 0.02365509  |\n",
            "| clipfrac           | 0.09616699  |\n",
            "| ep_len_mean        | 19.7        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.855       |\n",
            "| fps                | 363         |\n",
            "| n_updates          | 127         |\n",
            "| policy_entropy     | 0.24048607  |\n",
            "| policy_loss        | -0.02290636 |\n",
            "| serial_timesteps   | 65024       |\n",
            "| time_elapsed       | 1.55e+03    |\n",
            "| total_timesteps    | 520192      |\n",
            "| value_loss         | 0.022576027 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.028643167  |\n",
            "| clipfrac           | 0.196875     |\n",
            "| ep_len_mean        | 21.6         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.797        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 128          |\n",
            "| policy_entropy     | 0.4999996    |\n",
            "| policy_loss        | -0.038174488 |\n",
            "| serial_timesteps   | 65536        |\n",
            "| time_elapsed       | 1.56e+03     |\n",
            "| total_timesteps    | 524288       |\n",
            "| value_loss         | 0.097110674  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.030005097  |\n",
            "| clipfrac           | 0.24155274   |\n",
            "| ep_len_mean        | 33.2         |\n",
            "| ep_reward_mean     | 0.905        |\n",
            "| explained_variance | 0.825        |\n",
            "| fps                | 380          |\n",
            "| n_updates          | 129          |\n",
            "| policy_entropy     | 0.91653347   |\n",
            "| policy_loss        | -0.025724346 |\n",
            "| serial_timesteps   | 66048        |\n",
            "| time_elapsed       | 1.57e+03     |\n",
            "| total_timesteps    | 528384       |\n",
            "| value_loss         | 0.19495834   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.00 +/- 2.10\n",
            "------------------------------------\n",
            "| approxkl           | 0.015931996 |\n",
            "| clipfrac           | 0.1395996   |\n",
            "| ep_len_mean        | 19.5        |\n",
            "| ep_reward_mean     | 0.946       |\n",
            "| explained_variance | 0.863       |\n",
            "| fps                | 369         |\n",
            "| n_updates          | 130         |\n",
            "| policy_entropy     | 0.44466606  |\n",
            "| policy_loss        | -0.03099612 |\n",
            "| serial_timesteps   | 66560       |\n",
            "| time_elapsed       | 1.59e+03    |\n",
            "| total_timesteps    | 532480      |\n",
            "| value_loss         | 0.09995403  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01859797   |\n",
            "| clipfrac           | 0.20629883   |\n",
            "| ep_len_mean        | 30.6         |\n",
            "| ep_reward_mean     | 0.912        |\n",
            "| explained_variance | 0.923        |\n",
            "| fps                | 374          |\n",
            "| n_updates          | 131          |\n",
            "| policy_entropy     | 0.77497023   |\n",
            "| policy_loss        | -0.033083532 |\n",
            "| serial_timesteps   | 67072        |\n",
            "| time_elapsed       | 1.6e+03      |\n",
            "| total_timesteps    | 536576       |\n",
            "| value_loss         | 0.092232674  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 16.40 +/- 3.38\n",
            "------------------------------------\n",
            "| approxkl           | 0.021714646 |\n",
            "| clipfrac           | 0.17678222  |\n",
            "| ep_len_mean        | 28.1        |\n",
            "| ep_reward_mean     | 0.921       |\n",
            "| explained_variance | 0.952       |\n",
            "| fps                | 360         |\n",
            "| n_updates          | 132         |\n",
            "| policy_entropy     | 0.6339265   |\n",
            "| policy_loss        | -0.03149084 |\n",
            "| serial_timesteps   | 67584       |\n",
            "| time_elapsed       | 1.61e+03    |\n",
            "| total_timesteps    | 540672      |\n",
            "| value_loss         | 0.053238653 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.018358383 |\n",
            "| clipfrac           | 0.16105957  |\n",
            "| ep_len_mean        | 23.5        |\n",
            "| ep_reward_mean     | 0.935       |\n",
            "| explained_variance | 0.972       |\n",
            "| fps                | 377         |\n",
            "| n_updates          | 133         |\n",
            "| policy_entropy     | 0.53207976  |\n",
            "| policy_loss        | -0.03288243 |\n",
            "| serial_timesteps   | 68096       |\n",
            "| time_elapsed       | 1.62e+03    |\n",
            "| total_timesteps    | 544768      |\n",
            "| value_loss         | 0.02697722  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013448249  |\n",
            "| clipfrac           | 0.08190918   |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.961        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 134          |\n",
            "| policy_entropy     | 0.34440923   |\n",
            "| policy_loss        | -0.018900614 |\n",
            "| serial_timesteps   | 68608        |\n",
            "| time_elapsed       | 1.63e+03     |\n",
            "| total_timesteps    | 548864       |\n",
            "| value_loss         | 0.030844131  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.20 +/- 3.06\n",
            "------------------------------------\n",
            "| approxkl           | 0.013440494 |\n",
            "| clipfrac           | 0.13105468  |\n",
            "| ep_len_mean        | 18.7        |\n",
            "| ep_reward_mean     | 0.947       |\n",
            "| explained_variance | 0.963       |\n",
            "| fps                | 361         |\n",
            "| n_updates          | 135         |\n",
            "| policy_entropy     | 0.47149557  |\n",
            "| policy_loss        | -0.03140911 |\n",
            "| serial_timesteps   | 69120       |\n",
            "| time_elapsed       | 1.64e+03    |\n",
            "| total_timesteps    | 552960      |\n",
            "| value_loss         | 0.041874997 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0072346115 |\n",
            "| clipfrac           | 0.08491211   |\n",
            "| ep_len_mean        | 19           |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.977        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 136          |\n",
            "| policy_entropy     | 0.33014783   |\n",
            "| policy_loss        | -0.017336814 |\n",
            "| serial_timesteps   | 69632        |\n",
            "| time_elapsed       | 1.65e+03     |\n",
            "| total_timesteps    | 557056       |\n",
            "| value_loss         | 0.019129654  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.00 +/- 124.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.032409836  |\n",
            "| clipfrac           | 0.07026367   |\n",
            "| ep_len_mean        | 17.1         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.96         |\n",
            "| fps                | 327          |\n",
            "| n_updates          | 137          |\n",
            "| policy_entropy     | 0.17391923   |\n",
            "| policy_loss        | -0.020731108 |\n",
            "| serial_timesteps   | 70144        |\n",
            "| time_elapsed       | 1.66e+03     |\n",
            "| total_timesteps    | 561152       |\n",
            "| value_loss         | 0.01981033   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.016943634 |\n",
            "| clipfrac           | 0.10117187  |\n",
            "| ep_len_mean        | 19.9        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.961       |\n",
            "| fps                | 376         |\n",
            "| n_updates          | 138         |\n",
            "| policy_entropy     | 0.3071255   |\n",
            "| policy_loss        | -0.02882503 |\n",
            "| serial_timesteps   | 70656       |\n",
            "| time_elapsed       | 1.68e+03    |\n",
            "| total_timesteps    | 565248      |\n",
            "| value_loss         | 0.029970905 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.18050748   |\n",
            "| clipfrac           | 0.06347656   |\n",
            "| ep_len_mean        | 15.8         |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.829        |\n",
            "| fps                | 374          |\n",
            "| n_updates          | 139          |\n",
            "| policy_entropy     | 0.13629952   |\n",
            "| policy_loss        | -0.018136064 |\n",
            "| serial_timesteps   | 71168        |\n",
            "| time_elapsed       | 1.69e+03     |\n",
            "| total_timesteps    | 569344       |\n",
            "| value_loss         | 0.047203295  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=0.95 +/- 0.00\n",
            "Episode length: 17.20 +/- 1.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.97134715   |\n",
            "| clipfrac           | 0.23161621   |\n",
            "| ep_len_mean        | 21.6         |\n",
            "| ep_reward_mean     | 0.939        |\n",
            "| explained_variance | 0.695        |\n",
            "| fps                | 368          |\n",
            "| n_updates          | 140          |\n",
            "| policy_entropy     | 0.28834814   |\n",
            "| policy_loss        | -0.025366684 |\n",
            "| serial_timesteps   | 71680        |\n",
            "| time_elapsed       | 1.7e+03      |\n",
            "| total_timesteps    | 573440       |\n",
            "| value_loss         | 0.13174368   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031124089  |\n",
            "| clipfrac           | 0.113256834  |\n",
            "| ep_len_mean        | 17.7         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.503        |\n",
            "| fps                | 370          |\n",
            "| n_updates          | 141          |\n",
            "| policy_entropy     | 0.20049553   |\n",
            "| policy_loss        | -0.022180744 |\n",
            "| serial_timesteps   | 72192        |\n",
            "| time_elapsed       | 1.71e+03     |\n",
            "| total_timesteps    | 577536       |\n",
            "| value_loss         | 0.101597235  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.60 +/- 2.15\n",
            "-------------------------------------\n",
            "| approxkl           | 0.029025262  |\n",
            "| clipfrac           | 0.09592285   |\n",
            "| ep_len_mean        | 17.2         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.631        |\n",
            "| fps                | 362          |\n",
            "| n_updates          | 142          |\n",
            "| policy_entropy     | 0.19644329   |\n",
            "| policy_loss        | -0.018929703 |\n",
            "| serial_timesteps   | 72704        |\n",
            "| time_elapsed       | 1.72e+03     |\n",
            "| total_timesteps    | 581632       |\n",
            "| value_loss         | 0.044198997  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.11479969  |\n",
            "| clipfrac           | 0.12680665  |\n",
            "| ep_len_mean        | 18.1        |\n",
            "| ep_reward_mean     | 0.95        |\n",
            "| explained_variance | 0.594       |\n",
            "| fps                | 382         |\n",
            "| n_updates          | 143         |\n",
            "| policy_entropy     | 0.21405523  |\n",
            "| policy_loss        | -0.01905605 |\n",
            "| serial_timesteps   | 73216       |\n",
            "| time_elapsed       | 1.73e+03    |\n",
            "| total_timesteps    | 585728      |\n",
            "| value_loss         | 0.04432311  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.055465437  |\n",
            "| clipfrac           | 0.16115722   |\n",
            "| ep_len_mean        | 19.4         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.746        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 144          |\n",
            "| policy_entropy     | 0.31168097   |\n",
            "| policy_loss        | -0.026791817 |\n",
            "| serial_timesteps   | 73728        |\n",
            "| time_elapsed       | 1.74e+03     |\n",
            "| total_timesteps    | 589824       |\n",
            "| value_loss         | 0.052075185  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=0.95 +/- 0.01\n",
            "Episode length: 17.00 +/- 2.97\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03487823   |\n",
            "| clipfrac           | 0.21113281   |\n",
            "| ep_len_mean        | 23.5         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.741        |\n",
            "| fps                | 365          |\n",
            "| n_updates          | 145          |\n",
            "| policy_entropy     | 0.47765422   |\n",
            "| policy_loss        | -0.019330334 |\n",
            "| serial_timesteps   | 74240        |\n",
            "| time_elapsed       | 1.75e+03     |\n",
            "| total_timesteps    | 593920       |\n",
            "| value_loss         | 0.12173817   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.024128249 |\n",
            "| clipfrac           | 0.09379883  |\n",
            "| ep_len_mean        | 18.6        |\n",
            "| ep_reward_mean     | 0.948       |\n",
            "| explained_variance | 0.82        |\n",
            "| fps                | 377         |\n",
            "| n_updates          | 146         |\n",
            "| policy_entropy     | 0.21193588  |\n",
            "| policy_loss        | -0.01983081 |\n",
            "| serial_timesteps   | 74752       |\n",
            "| time_elapsed       | 1.76e+03    |\n",
            "| total_timesteps    | 598016      |\n",
            "| value_loss         | 0.030211434 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.60 +/- 3.32\n",
            "-------------------------------------\n",
            "| approxkl           | 0.030080507  |\n",
            "| clipfrac           | 0.12810059   |\n",
            "| ep_len_mean        | 19.7         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.844        |\n",
            "| fps                | 365          |\n",
            "| n_updates          | 147          |\n",
            "| policy_entropy     | 0.262952     |\n",
            "| policy_loss        | -0.024237664 |\n",
            "| serial_timesteps   | 75264        |\n",
            "| time_elapsed       | 1.77e+03     |\n",
            "| total_timesteps    | 602112       |\n",
            "| value_loss         | 0.029617012  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013475786  |\n",
            "| clipfrac           | 0.057885744  |\n",
            "| ep_len_mean        | 17.3         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.799        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 148          |\n",
            "| policy_entropy     | 0.119390965  |\n",
            "| policy_loss        | -0.014779232 |\n",
            "| serial_timesteps   | 75776        |\n",
            "| time_elapsed       | 1.79e+03     |\n",
            "| total_timesteps    | 606208       |\n",
            "| value_loss         | 0.010528648  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02294368   |\n",
            "| clipfrac           | 0.075488284  |\n",
            "| ep_len_mean        | 17.3         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.866        |\n",
            "| fps                | 370          |\n",
            "| n_updates          | 149          |\n",
            "| policy_entropy     | 0.116128825  |\n",
            "| policy_loss        | -0.016452875 |\n",
            "| serial_timesteps   | 76288        |\n",
            "| time_elapsed       | 1.8e+03      |\n",
            "| total_timesteps    | 610304       |\n",
            "| value_loss         | 0.0053437776 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03311778   |\n",
            "| clipfrac           | 0.13371582   |\n",
            "| ep_len_mean        | 20.6         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.734        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 150          |\n",
            "| policy_entropy     | 0.2556532    |\n",
            "| policy_loss        | -0.025583362 |\n",
            "| serial_timesteps   | 76800        |\n",
            "| time_elapsed       | 1.81e+03     |\n",
            "| total_timesteps    | 614400       |\n",
            "| value_loss         | 0.04837306   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.021591423  |\n",
            "| clipfrac           | 0.07104492   |\n",
            "| ep_len_mean        | 16.7         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.81         |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 151          |\n",
            "| policy_entropy     | 0.13654009   |\n",
            "| policy_loss        | -0.008317602 |\n",
            "| serial_timesteps   | 77312        |\n",
            "| time_elapsed       | 1.82e+03     |\n",
            "| total_timesteps    | 618496       |\n",
            "| value_loss         | 0.011856118  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.40 +/- 123.81\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031103427  |\n",
            "| clipfrac           | 0.08305664   |\n",
            "| ep_len_mean        | 18           |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.73         |\n",
            "| fps                | 328          |\n",
            "| n_updates          | 152          |\n",
            "| policy_entropy     | 0.12208078   |\n",
            "| policy_loss        | -0.016558664 |\n",
            "| serial_timesteps   | 77824        |\n",
            "| time_elapsed       | 1.83e+03     |\n",
            "| total_timesteps    | 622592       |\n",
            "| value_loss         | 0.02571179   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020735458  |\n",
            "| clipfrac           | 0.05817871   |\n",
            "| ep_len_mean        | 16.2         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.833        |\n",
            "| fps                | 371          |\n",
            "| n_updates          | 153          |\n",
            "| policy_entropy     | 0.10051479   |\n",
            "| policy_loss        | -0.019929454 |\n",
            "| serial_timesteps   | 78336        |\n",
            "| time_elapsed       | 1.84e+03     |\n",
            "| total_timesteps    | 626688       |\n",
            "| value_loss         | 0.0120104235 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 1.79\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015483637  |\n",
            "| clipfrac           | 0.06171875   |\n",
            "| ep_len_mean        | 15.5         |\n",
            "| ep_reward_mean     | 0.957        |\n",
            "| explained_variance | 0.8          |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 154          |\n",
            "| policy_entropy     | 0.13348225   |\n",
            "| policy_loss        | -0.021567795 |\n",
            "| serial_timesteps   | 78848        |\n",
            "| time_elapsed       | 1.85e+03     |\n",
            "| total_timesteps    | 630784       |\n",
            "| value_loss         | 0.015046729  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018295614  |\n",
            "| clipfrac           | 0.12773438   |\n",
            "| ep_len_mean        | 19.3         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.763        |\n",
            "| fps                | 380          |\n",
            "| n_updates          | 155          |\n",
            "| policy_entropy     | 0.32022756   |\n",
            "| policy_loss        | -0.035068452 |\n",
            "| serial_timesteps   | 79360        |\n",
            "| time_elapsed       | 1.86e+03     |\n",
            "| total_timesteps    | 634880       |\n",
            "| value_loss         | 0.048499644  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018797766  |\n",
            "| clipfrac           | 0.09604492   |\n",
            "| ep_len_mean        | 19           |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.815        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 156          |\n",
            "| policy_entropy     | 0.2530198    |\n",
            "| policy_loss        | -0.029919889 |\n",
            "| serial_timesteps   | 79872        |\n",
            "| time_elapsed       | 1.87e+03     |\n",
            "| total_timesteps    | 638976       |\n",
            "| value_loss         | 0.037537728  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.00 +/- 2.10\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020334959  |\n",
            "| clipfrac           | 0.035888672  |\n",
            "| ep_len_mean        | 15.6         |\n",
            "| ep_reward_mean     | 0.957        |\n",
            "| explained_variance | 0.886        |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 157          |\n",
            "| policy_entropy     | 0.08288197   |\n",
            "| policy_loss        | -0.012727283 |\n",
            "| serial_timesteps   | 80384        |\n",
            "| time_elapsed       | 1.89e+03     |\n",
            "| total_timesteps    | 643072       |\n",
            "| value_loss         | 0.0044268044 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04355445   |\n",
            "| clipfrac           | 0.2400879    |\n",
            "| ep_len_mean        | 27.9         |\n",
            "| ep_reward_mean     | 0.921        |\n",
            "| explained_variance | 0.732        |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 158          |\n",
            "| policy_entropy     | 0.52068436   |\n",
            "| policy_loss        | -0.030303955 |\n",
            "| serial_timesteps   | 80896        |\n",
            "| time_elapsed       | 1.9e+03      |\n",
            "| total_timesteps    | 647168       |\n",
            "| value_loss         | 0.13754821   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 138.60 +/- 151.38\n",
            "------------------------------------\n",
            "| approxkl           | 0.032403238 |\n",
            "| clipfrac           | 0.2909668   |\n",
            "| ep_len_mean        | 34.1        |\n",
            "| ep_reward_mean     | 0.901       |\n",
            "| explained_variance | 0.78        |\n",
            "| fps                | 307         |\n",
            "| n_updates          | 159         |\n",
            "| policy_entropy     | 0.8169631   |\n",
            "| policy_loss        | -0.03686778 |\n",
            "| serial_timesteps   | 81408       |\n",
            "| time_elapsed       | 1.91e+03    |\n",
            "| total_timesteps    | 651264      |\n",
            "| value_loss         | 0.1944188   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.015655432 |\n",
            "| clipfrac           | 0.18391113  |\n",
            "| ep_len_mean        | 20.9        |\n",
            "| ep_reward_mean     | 0.941       |\n",
            "| explained_variance | 0.902       |\n",
            "| fps                | 375         |\n",
            "| n_updates          | 160         |\n",
            "| policy_entropy     | 0.62832487  |\n",
            "| policy_loss        | -0.03246308 |\n",
            "| serial_timesteps   | 81920       |\n",
            "| time_elapsed       | 1.92e+03    |\n",
            "| total_timesteps    | 655360      |\n",
            "| value_loss         | 0.06831252  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019626923  |\n",
            "| clipfrac           | 0.19809571   |\n",
            "| ep_len_mean        | 22.8         |\n",
            "| ep_reward_mean     | 0.937        |\n",
            "| explained_variance | 0.93         |\n",
            "| fps                | 378          |\n",
            "| n_updates          | 161          |\n",
            "| policy_entropy     | 0.66478693   |\n",
            "| policy_loss        | -0.036950953 |\n",
            "| serial_timesteps   | 82432        |\n",
            "| time_elapsed       | 1.93e+03     |\n",
            "| total_timesteps    | 659456       |\n",
            "| value_loss         | 0.047304403  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.00 +/- 124.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011289627  |\n",
            "| clipfrac           | 0.13469239   |\n",
            "| ep_len_mean        | 16.6         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.94         |\n",
            "| fps                | 327          |\n",
            "| n_updates          | 162          |\n",
            "| policy_entropy     | 0.47476062   |\n",
            "| policy_loss        | -0.026411554 |\n",
            "| serial_timesteps   | 82944        |\n",
            "| time_elapsed       | 1.94e+03     |\n",
            "| total_timesteps    | 663552       |\n",
            "| value_loss         | 0.042540304  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019633604  |\n",
            "| clipfrac           | 0.16552734   |\n",
            "| ep_len_mean        | 21.2         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.931        |\n",
            "| fps                | 378          |\n",
            "| n_updates          | 163          |\n",
            "| policy_entropy     | 0.5077373    |\n",
            "| policy_loss        | -0.032081448 |\n",
            "| serial_timesteps   | 83456        |\n",
            "| time_elapsed       | 1.96e+03     |\n",
            "| total_timesteps    | 667648       |\n",
            "| value_loss         | 0.059557546  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=0.77 +/- 0.39\n",
            "Episode length: 75.60 +/- 124.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01736159   |\n",
            "| clipfrac           | 0.17565918   |\n",
            "| ep_len_mean        | 22.5         |\n",
            "| ep_reward_mean     | 0.936        |\n",
            "| explained_variance | 0.954        |\n",
            "| fps                | 330          |\n",
            "| n_updates          | 164          |\n",
            "| policy_entropy     | 0.5759697    |\n",
            "| policy_loss        | -0.023481138 |\n",
            "| serial_timesteps   | 83968        |\n",
            "| time_elapsed       | 1.97e+03     |\n",
            "| total_timesteps    | 671744       |\n",
            "| value_loss         | 0.046576627  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01315023   |\n",
            "| clipfrac           | 0.13220215   |\n",
            "| ep_len_mean        | 21.1         |\n",
            "| ep_reward_mean     | 0.94         |\n",
            "| explained_variance | 0.976        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 165          |\n",
            "| policy_entropy     | 0.51609004   |\n",
            "| policy_loss        | -0.023726912 |\n",
            "| serial_timesteps   | 84480        |\n",
            "| time_elapsed       | 1.98e+03     |\n",
            "| total_timesteps    | 675840       |\n",
            "| value_loss         | 0.020485718  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01949518   |\n",
            "| clipfrac           | 0.21662597   |\n",
            "| ep_len_mean        | 24.5         |\n",
            "| ep_reward_mean     | 0.93         |\n",
            "| explained_variance | 0.984        |\n",
            "| fps                | 374          |\n",
            "| n_updates          | 166          |\n",
            "| policy_entropy     | 0.8092287    |\n",
            "| policy_loss        | -0.029356137 |\n",
            "| serial_timesteps   | 84992        |\n",
            "| time_elapsed       | 1.99e+03     |\n",
            "| total_timesteps    | 679936       |\n",
            "| value_loss         | 0.019787014  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.60 +/- 1.62\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013660587  |\n",
            "| clipfrac           | 0.15322265   |\n",
            "| ep_len_mean        | 25.3         |\n",
            "| ep_reward_mean     | 0.927        |\n",
            "| explained_variance | 0.985        |\n",
            "| fps                | 357          |\n",
            "| n_updates          | 167          |\n",
            "| policy_entropy     | 0.699214     |\n",
            "| policy_loss        | -0.024105247 |\n",
            "| serial_timesteps   | 85504        |\n",
            "| time_elapsed       | 2e+03        |\n",
            "| total_timesteps    | 684032       |\n",
            "| value_loss         | 0.022836627  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019341623  |\n",
            "| clipfrac           | 0.1189209    |\n",
            "| ep_len_mean        | 18.1         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.991        |\n",
            "| fps                | 373          |\n",
            "| n_updates          | 168          |\n",
            "| policy_entropy     | 0.43995857   |\n",
            "| policy_loss        | -0.032810513 |\n",
            "| serial_timesteps   | 86016        |\n",
            "| time_elapsed       | 2.01e+03     |\n",
            "| total_timesteps    | 688128       |\n",
            "| value_loss         | 0.009776251  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=0.76 +/- 0.38\n",
            "Episode length: 77.80 +/- 123.12\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009406082  |\n",
            "| clipfrac           | 0.1253418    |\n",
            "| ep_len_mean        | 21.1         |\n",
            "| ep_reward_mean     | 0.939        |\n",
            "| explained_variance | 0.995        |\n",
            "| fps                | 323          |\n",
            "| n_updates          | 169          |\n",
            "| policy_entropy     | 0.46213078   |\n",
            "| policy_loss        | -0.029938791 |\n",
            "| serial_timesteps   | 86528        |\n",
            "| time_elapsed       | 2.02e+03     |\n",
            "| total_timesteps    | 692224       |\n",
            "| value_loss         | 0.0049699284 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.012616922  |\n",
            "| clipfrac           | 0.13432617   |\n",
            "| ep_len_mean        | 29.9         |\n",
            "| ep_reward_mean     | 0.914        |\n",
            "| explained_variance | 0.995        |\n",
            "| fps                | 370          |\n",
            "| n_updates          | 170          |\n",
            "| policy_entropy     | 0.6557887    |\n",
            "| policy_loss        | -0.025217876 |\n",
            "| serial_timesteps   | 87040        |\n",
            "| time_elapsed       | 2.04e+03     |\n",
            "| total_timesteps    | 696320       |\n",
            "| value_loss         | 0.009377727  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=0.77 +/- 0.39\n",
            "Episode length: 75.60 +/- 124.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009470551  |\n",
            "| clipfrac           | 0.14108887   |\n",
            "| ep_len_mean        | 24.8         |\n",
            "| ep_reward_mean     | 0.928        |\n",
            "| explained_variance | 0.997        |\n",
            "| fps                | 329          |\n",
            "| n_updates          | 171          |\n",
            "| policy_entropy     | 0.8211635    |\n",
            "| policy_loss        | -0.025653709 |\n",
            "| serial_timesteps   | 87552        |\n",
            "| time_elapsed       | 2.05e+03     |\n",
            "| total_timesteps    | 700416       |\n",
            "| value_loss         | 0.003840825  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01501278   |\n",
            "| clipfrac           | 0.18798828   |\n",
            "| ep_len_mean        | 30.5         |\n",
            "| ep_reward_mean     | 0.911        |\n",
            "| explained_variance | 0.998        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 172          |\n",
            "| policy_entropy     | 0.989159     |\n",
            "| policy_loss        | -0.030111182 |\n",
            "| serial_timesteps   | 88064        |\n",
            "| time_elapsed       | 2.06e+03     |\n",
            "| total_timesteps    | 704512       |\n",
            "| value_loss         | 0.0047418014 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01119572   |\n",
            "| clipfrac           | 0.086083986  |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.997        |\n",
            "| fps                | 378          |\n",
            "| n_updates          | 173          |\n",
            "| policy_entropy     | 0.46305838   |\n",
            "| policy_loss        | -0.017595446 |\n",
            "| serial_timesteps   | 88576        |\n",
            "| time_elapsed       | 2.07e+03     |\n",
            "| total_timesteps    | 708608       |\n",
            "| value_loss         | 0.005110492  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 16.20 +/- 2.56\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015341127  |\n",
            "| clipfrac           | 0.11779785   |\n",
            "| ep_len_mean        | 18.4         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.993        |\n",
            "| fps                | 359          |\n",
            "| n_updates          | 174          |\n",
            "| policy_entropy     | 0.49646083   |\n",
            "| policy_loss        | -0.019594125 |\n",
            "| serial_timesteps   | 89088        |\n",
            "| time_elapsed       | 2.08e+03     |\n",
            "| total_timesteps    | 712704       |\n",
            "| value_loss         | 0.012769024  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.00947246   |\n",
            "| clipfrac           | 0.06313477   |\n",
            "| ep_len_mean        | 15           |\n",
            "| ep_reward_mean     | 0.958        |\n",
            "| explained_variance | 0.994        |\n",
            "| fps                | 373          |\n",
            "| n_updates          | 175          |\n",
            "| policy_entropy     | 0.22843249   |\n",
            "| policy_loss        | -0.019618927 |\n",
            "| serial_timesteps   | 89600        |\n",
            "| time_elapsed       | 2.09e+03     |\n",
            "| total_timesteps    | 716800       |\n",
            "| value_loss         | 0.0059049428 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 77.20 +/- 123.41\n",
            "-------------------------------------\n",
            "| approxkl           | 0.22678716   |\n",
            "| clipfrac           | 0.11340332   |\n",
            "| ep_len_mean        | 24.8         |\n",
            "| ep_reward_mean     | 0.931        |\n",
            "| explained_variance | 0.973        |\n",
            "| fps                | 328          |\n",
            "| n_updates          | 176          |\n",
            "| policy_entropy     | 0.47005004   |\n",
            "| policy_loss        | -0.013546331 |\n",
            "| serial_timesteps   | 90112        |\n",
            "| time_elapsed       | 2.1e+03      |\n",
            "| total_timesteps    | 720896       |\n",
            "| value_loss         | 0.040920377  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.06688992   |\n",
            "| clipfrac           | 0.34655762   |\n",
            "| ep_len_mean        | 33.3         |\n",
            "| ep_reward_mean     | 0.905        |\n",
            "| explained_variance | 0.793        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 177          |\n",
            "| policy_entropy     | 0.48445067   |\n",
            "| policy_loss        | -0.050427966 |\n",
            "| serial_timesteps   | 90624        |\n",
            "| time_elapsed       | 2.12e+03     |\n",
            "| total_timesteps    | 724992       |\n",
            "| value_loss         | 0.18493012   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.028690139  |\n",
            "| clipfrac           | 0.14919433   |\n",
            "| ep_len_mean        | 23.8         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.902        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 178          |\n",
            "| policy_entropy     | 0.39247888   |\n",
            "| policy_loss        | -0.031418256 |\n",
            "| serial_timesteps   | 91136        |\n",
            "| time_elapsed       | 2.13e+03     |\n",
            "| total_timesteps    | 729088       |\n",
            "| value_loss         | 0.11243074   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 14.60 +/- 1.85\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02925637   |\n",
            "| clipfrac           | 0.07780762   |\n",
            "| ep_len_mean        | 18.2         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.89         |\n",
            "| fps                | 368          |\n",
            "| n_updates          | 179          |\n",
            "| policy_entropy     | 0.1464692    |\n",
            "| policy_loss        | -0.016538655 |\n",
            "| serial_timesteps   | 91648        |\n",
            "| time_elapsed       | 2.14e+03     |\n",
            "| total_timesteps    | 733184       |\n",
            "| value_loss         | 0.03218123   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.04861153   |\n",
            "| clipfrac           | 0.21884766   |\n",
            "| ep_len_mean        | 28.4         |\n",
            "| ep_reward_mean     | 0.919        |\n",
            "| explained_variance | 0.591        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 180          |\n",
            "| policy_entropy     | 0.38656127   |\n",
            "| policy_loss        | -0.022660494 |\n",
            "| serial_timesteps   | 92160        |\n",
            "| time_elapsed       | 2.15e+03     |\n",
            "| total_timesteps    | 737280       |\n",
            "| value_loss         | 0.17082515   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.20 +/- 123.90\n",
            "------------------------------------\n",
            "| approxkl           | 0.032566033 |\n",
            "| clipfrac           | 0.21506348  |\n",
            "| ep_len_mean        | 27.2        |\n",
            "| ep_reward_mean     | 0.924       |\n",
            "| explained_variance | 0.702       |\n",
            "| fps                | 326         |\n",
            "| n_updates          | 181         |\n",
            "| policy_entropy     | 0.48902217  |\n",
            "| policy_loss        | -0.03035655 |\n",
            "| serial_timesteps   | 92672       |\n",
            "| time_elapsed       | 2.16e+03    |\n",
            "| total_timesteps    | 741376      |\n",
            "| value_loss         | 0.107366845 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020954119  |\n",
            "| clipfrac           | 0.1940918    |\n",
            "| ep_len_mean        | 20.5         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.796        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 182          |\n",
            "| policy_entropy     | 0.5466693    |\n",
            "| policy_loss        | -0.027659934 |\n",
            "| serial_timesteps   | 93184        |\n",
            "| time_elapsed       | 2.17e+03     |\n",
            "| total_timesteps    | 745472       |\n",
            "| value_loss         | 0.07031089   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.014205925 |\n",
            "| clipfrac           | 0.15512696  |\n",
            "| ep_len_mean        | 19.8        |\n",
            "| ep_reward_mean     | 0.945       |\n",
            "| explained_variance | 0.814       |\n",
            "| fps                | 376         |\n",
            "| n_updates          | 183         |\n",
            "| policy_entropy     | 0.46508545  |\n",
            "| policy_loss        | -0.02653722 |\n",
            "| serial_timesteps   | 93696       |\n",
            "| time_elapsed       | 2.18e+03    |\n",
            "| total_timesteps    | 749568      |\n",
            "| value_loss         | 0.09636044  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.00 +/- 1.26\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01893573   |\n",
            "| clipfrac           | 0.17766114   |\n",
            "| ep_len_mean        | 25.9         |\n",
            "| ep_reward_mean     | 0.927        |\n",
            "| explained_variance | 0.87         |\n",
            "| fps                | 367          |\n",
            "| n_updates          | 184          |\n",
            "| policy_entropy     | 0.5748427    |\n",
            "| policy_loss        | -0.032232404 |\n",
            "| serial_timesteps   | 94208        |\n",
            "| time_elapsed       | 2.19e+03     |\n",
            "| total_timesteps    | 753664       |\n",
            "| value_loss         | 0.07444239   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014274396  |\n",
            "| clipfrac           | 0.12307129   |\n",
            "| ep_len_mean        | 19.9         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.91         |\n",
            "| fps                | 380          |\n",
            "| n_updates          | 185          |\n",
            "| policy_entropy     | 0.34017608   |\n",
            "| policy_loss        | -0.032089848 |\n",
            "| serial_timesteps   | 94720        |\n",
            "| time_elapsed       | 2.2e+03      |\n",
            "| total_timesteps    | 757760       |\n",
            "| value_loss         | 0.031868942  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 1.41\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01961517   |\n",
            "| clipfrac           | 0.22861329   |\n",
            "| ep_len_mean        | 36.5         |\n",
            "| ep_reward_mean     | 0.894        |\n",
            "| explained_variance | 0.919        |\n",
            "| fps                | 369          |\n",
            "| n_updates          | 186          |\n",
            "| policy_entropy     | 0.8615138    |\n",
            "| policy_loss        | -0.031936806 |\n",
            "| serial_timesteps   | 95232        |\n",
            "| time_elapsed       | 2.22e+03     |\n",
            "| total_timesteps    | 761856       |\n",
            "| value_loss         | 0.08766052   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.016980406 |\n",
            "| clipfrac           | 0.17834473  |\n",
            "| ep_len_mean        | 19.9        |\n",
            "| ep_reward_mean     | 0.944       |\n",
            "| explained_variance | 0.949       |\n",
            "| fps                | 379         |\n",
            "| n_updates          | 187         |\n",
            "| policy_entropy     | 0.5068529   |\n",
            "| policy_loss        | -0.04029102 |\n",
            "| serial_timesteps   | 95744       |\n",
            "| time_elapsed       | 2.23e+03    |\n",
            "| total_timesteps    | 765952      |\n",
            "| value_loss         | 0.036355536 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.40 +/- 2.15\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019920992  |\n",
            "| clipfrac           | 0.09121094   |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.935        |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 188          |\n",
            "| policy_entropy     | 0.25832802   |\n",
            "| policy_loss        | -0.020942766 |\n",
            "| serial_timesteps   | 96256        |\n",
            "| time_elapsed       | 2.24e+03     |\n",
            "| total_timesteps    | 770048       |\n",
            "| value_loss         | 0.03043111   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010148598  |\n",
            "| clipfrac           | 0.08786621   |\n",
            "| ep_len_mean        | 18.6         |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.957        |\n",
            "| fps                | 371          |\n",
            "| n_updates          | 189          |\n",
            "| policy_entropy     | 0.2533649    |\n",
            "| policy_loss        | -0.030423772 |\n",
            "| serial_timesteps   | 96768        |\n",
            "| time_elapsed       | 2.25e+03     |\n",
            "| total_timesteps    | 774144       |\n",
            "| value_loss         | 0.022676637  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011640878  |\n",
            "| clipfrac           | 0.1239502    |\n",
            "| ep_len_mean        | 20.2         |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.973        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 190          |\n",
            "| policy_entropy     | 0.3561111    |\n",
            "| policy_loss        | -0.028537398 |\n",
            "| serial_timesteps   | 97280        |\n",
            "| time_elapsed       | 2.26e+03     |\n",
            "| total_timesteps    | 778240       |\n",
            "| value_loss         | 0.021165885  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=0.96 +/- 0.01\n",
            "Episode length: 15.20 +/- 1.94\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0074940785 |\n",
            "| clipfrac           | 0.079589844  |\n",
            "| ep_len_mean        | 19.3         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.952        |\n",
            "| fps                | 366          |\n",
            "| n_updates          | 191          |\n",
            "| policy_entropy     | 0.31221673   |\n",
            "| policy_loss        | -0.018496636 |\n",
            "| serial_timesteps   | 97792        |\n",
            "| time_elapsed       | 2.27e+03     |\n",
            "| total_timesteps    | 782336       |\n",
            "| value_loss         | 0.03806898   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.048965417  |\n",
            "| clipfrac           | 0.09104004   |\n",
            "| ep_len_mean        | 17.5         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.937        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 192          |\n",
            "| policy_entropy     | 0.25455314   |\n",
            "| policy_loss        | -0.019999744 |\n",
            "| serial_timesteps   | 98304        |\n",
            "| time_elapsed       | 2.28e+03     |\n",
            "| total_timesteps    | 786432       |\n",
            "| value_loss         | 0.019602232  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 77.20 +/- 123.41\n",
            "-------------------------------------\n",
            "| approxkl           | 0.035137117  |\n",
            "| clipfrac           | 0.13381347   |\n",
            "| ep_len_mean        | 18.5         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.537        |\n",
            "| fps                | 324          |\n",
            "| n_updates          | 193          |\n",
            "| policy_entropy     | 0.2167634    |\n",
            "| policy_loss        | -0.024580201 |\n",
            "| serial_timesteps   | 98816        |\n",
            "| time_elapsed       | 2.29e+03     |\n",
            "| total_timesteps    | 790528       |\n",
            "| value_loss         | 0.040010024  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.052476726  |\n",
            "| clipfrac           | 0.3109375    |\n",
            "| ep_len_mean        | 33.7         |\n",
            "| ep_reward_mean     | 0.903        |\n",
            "| explained_variance | 0.551        |\n",
            "| fps                | 383          |\n",
            "| n_updates          | 194          |\n",
            "| policy_entropy     | 0.59091824   |\n",
            "| policy_loss        | -0.034830797 |\n",
            "| serial_timesteps   | 99328        |\n",
            "| time_elapsed       | 2.3e+03      |\n",
            "| total_timesteps    | 794624       |\n",
            "| value_loss         | 0.20043802   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.020960854  |\n",
            "| clipfrac           | 0.22983399   |\n",
            "| ep_len_mean        | 47.5         |\n",
            "| ep_reward_mean     | 0.86         |\n",
            "| explained_variance | 0.606        |\n",
            "| fps                | 384          |\n",
            "| n_updates          | 195          |\n",
            "| policy_entropy     | 0.85985243   |\n",
            "| policy_loss        | -0.009786767 |\n",
            "| serial_timesteps   | 99840        |\n",
            "| time_elapsed       | 2.32e+03     |\n",
            "| total_timesteps    | 798720       |\n",
            "| value_loss         | 0.25649402   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=0.58 +/- 0.47\n",
            "Episode length: 138.20 +/- 151.71\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031027231  |\n",
            "| clipfrac           | 0.2625       |\n",
            "| ep_len_mean        | 33.5         |\n",
            "| ep_reward_mean     | 0.904        |\n",
            "| explained_variance | 0.778        |\n",
            "| fps                | 301          |\n",
            "| n_updates          | 196          |\n",
            "| policy_entropy     | 0.89201003   |\n",
            "| policy_loss        | -0.027046818 |\n",
            "| serial_timesteps   | 100352       |\n",
            "| time_elapsed       | 2.33e+03     |\n",
            "| total_timesteps    | 802816       |\n",
            "| value_loss         | 0.14032963   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.026679149 |\n",
            "| clipfrac           | 0.26330566  |\n",
            "| ep_len_mean        | 43          |\n",
            "| ep_reward_mean     | 0.872       |\n",
            "| explained_variance | 0.776       |\n",
            "| fps                | 384         |\n",
            "| n_updates          | 197         |\n",
            "| policy_entropy     | 1.097583    |\n",
            "| policy_loss        | -0.02188053 |\n",
            "| serial_timesteps   | 100864      |\n",
            "| time_elapsed       | 2.34e+03    |\n",
            "| total_timesteps    | 806912      |\n",
            "| value_loss         | 0.2053      |\n",
            "------------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 13.40 +/- 0.49\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.022210836  |\n",
            "| clipfrac           | 0.19245605   |\n",
            "| ep_len_mean        | 53.5         |\n",
            "| ep_reward_mean     | 0.841        |\n",
            "| explained_variance | 0.851        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 198          |\n",
            "| policy_entropy     | 1.1978395    |\n",
            "| policy_loss        | -0.017806064 |\n",
            "| serial_timesteps   | 101376       |\n",
            "| time_elapsed       | 2.35e+03     |\n",
            "| total_timesteps    | 811008       |\n",
            "| value_loss         | 0.14332905   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.014176324 |\n",
            "| clipfrac           | 0.13779297  |\n",
            "| ep_len_mean        | 25.4        |\n",
            "| ep_reward_mean     | 0.927       |\n",
            "| explained_variance | 0.932       |\n",
            "| fps                | 381         |\n",
            "| n_updates          | 199         |\n",
            "| policy_entropy     | 0.68967557  |\n",
            "| policy_loss        | -0.01854596 |\n",
            "| serial_timesteps   | 101888      |\n",
            "| time_elapsed       | 2.36e+03    |\n",
            "| total_timesteps    | 815104      |\n",
            "| value_loss         | 0.055348825 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009928628  |\n",
            "| clipfrac           | 0.11706543   |\n",
            "| ep_len_mean        | 26.3         |\n",
            "| ep_reward_mean     | 0.925        |\n",
            "| explained_variance | 0.949        |\n",
            "| fps                | 380          |\n",
            "| n_updates          | 200          |\n",
            "| policy_entropy     | 0.7035121    |\n",
            "| policy_loss        | -0.014589226 |\n",
            "| serial_timesteps   | 102400       |\n",
            "| time_elapsed       | 2.37e+03     |\n",
            "| total_timesteps    | 819200       |\n",
            "| value_loss         | 0.04752937   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.20 +/- 1.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01001313   |\n",
            "| clipfrac           | 0.10437012   |\n",
            "| ep_len_mean        | 19.4         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.952        |\n",
            "| fps                | 366          |\n",
            "| n_updates          | 201          |\n",
            "| policy_entropy     | 0.4720959    |\n",
            "| policy_loss        | -0.018811475 |\n",
            "| serial_timesteps   | 102912       |\n",
            "| time_elapsed       | 2.38e+03     |\n",
            "| total_timesteps    | 823296       |\n",
            "| value_loss         | 0.038695823  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01634704   |\n",
            "| clipfrac           | 0.07956543   |\n",
            "| ep_len_mean        | 17.9         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.915        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 202          |\n",
            "| policy_entropy     | 0.27411544   |\n",
            "| policy_loss        | -0.024234306 |\n",
            "| serial_timesteps   | 103424       |\n",
            "| time_elapsed       | 2.39e+03     |\n",
            "| total_timesteps    | 827392       |\n",
            "| value_loss         | 0.03381925   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.60 +/- 123.71\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015063962  |\n",
            "| clipfrac           | 0.08930664   |\n",
            "| ep_len_mean        | 16.4         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.84         |\n",
            "| fps                | 324          |\n",
            "| n_updates          | 203          |\n",
            "| policy_entropy     | 0.2385674    |\n",
            "| policy_loss        | -0.021483675 |\n",
            "| serial_timesteps   | 103936       |\n",
            "| time_elapsed       | 2.4e+03      |\n",
            "| total_timesteps    | 831488       |\n",
            "| value_loss         | 0.049991436  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.12145881   |\n",
            "| clipfrac           | 0.18188477   |\n",
            "| ep_len_mean        | 20           |\n",
            "| ep_reward_mean     | 0.944        |\n",
            "| explained_variance | 0.568        |\n",
            "| fps                | 371          |\n",
            "| n_updates          | 204          |\n",
            "| policy_entropy     | 0.28746083   |\n",
            "| policy_loss        | -0.039331965 |\n",
            "| serial_timesteps   | 104448       |\n",
            "| time_elapsed       | 2.42e+03     |\n",
            "| total_timesteps    | 835584       |\n",
            "| value_loss         | 0.06567629   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03560017   |\n",
            "| clipfrac           | 0.10310058   |\n",
            "| ep_len_mean        | 16.9         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.636        |\n",
            "| fps                | 378          |\n",
            "| n_updates          | 205          |\n",
            "| policy_entropy     | 0.20627299   |\n",
            "| policy_loss        | -0.017956248 |\n",
            "| serial_timesteps   | 104960       |\n",
            "| time_elapsed       | 2.43e+03     |\n",
            "| total_timesteps    | 839680       |\n",
            "| value_loss         | 0.03326728   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.40 +/- 1.02\n",
            "-------------------------------------\n",
            "| approxkl           | 0.060183845  |\n",
            "| clipfrac           | 0.19562988   |\n",
            "| ep_len_mean        | 20.9         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.565        |\n",
            "| fps                | 364          |\n",
            "| n_updates          | 206          |\n",
            "| policy_entropy     | 0.35792583   |\n",
            "| policy_loss        | -0.032291714 |\n",
            "| serial_timesteps   | 105472       |\n",
            "| time_elapsed       | 2.44e+03     |\n",
            "| total_timesteps    | 843776       |\n",
            "| value_loss         | 0.06734339   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.055378646  |\n",
            "| clipfrac           | 0.3401367    |\n",
            "| ep_len_mean        | 30.2         |\n",
            "| ep_reward_mean     | 0.914        |\n",
            "| explained_variance | 0.639        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 207          |\n",
            "| policy_entropy     | 0.5918666    |\n",
            "| policy_loss        | -0.051795907 |\n",
            "| serial_timesteps   | 105984       |\n",
            "| time_elapsed       | 2.45e+03     |\n",
            "| total_timesteps    | 847872       |\n",
            "| value_loss         | 0.15003926   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.20 +/- 1.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.023294218  |\n",
            "| clipfrac           | 0.12521973   |\n",
            "| ep_len_mean        | 18.2         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.797        |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 208          |\n",
            "| policy_entropy     | 0.26468545   |\n",
            "| policy_loss        | -0.029657522 |\n",
            "| serial_timesteps   | 106496       |\n",
            "| time_elapsed       | 2.46e+03     |\n",
            "| total_timesteps    | 851968       |\n",
            "| value_loss         | 0.03569861   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03557079   |\n",
            "| clipfrac           | 0.18435058   |\n",
            "| ep_len_mean        | 20.6         |\n",
            "| ep_reward_mean     | 0.943        |\n",
            "| explained_variance | 0.834        |\n",
            "| fps                | 375          |\n",
            "| n_updates          | 209          |\n",
            "| policy_entropy     | 0.40469676   |\n",
            "| policy_loss        | -0.033501755 |\n",
            "| serial_timesteps   | 107008       |\n",
            "| time_elapsed       | 2.47e+03     |\n",
            "| total_timesteps    | 856064       |\n",
            "| value_loss         | 0.054665815  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.60 +/- 1.36\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02168129   |\n",
            "| clipfrac           | 0.18649903   |\n",
            "| ep_len_mean        | 23.7         |\n",
            "| ep_reward_mean     | 0.932        |\n",
            "| explained_variance | 0.909        |\n",
            "| fps                | 364          |\n",
            "| n_updates          | 210          |\n",
            "| policy_entropy     | 0.57775146   |\n",
            "| policy_loss        | -0.028556123 |\n",
            "| serial_timesteps   | 107520       |\n",
            "| time_elapsed       | 2.48e+03     |\n",
            "| total_timesteps    | 860160       |\n",
            "| value_loss         | 0.05273922   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.013035658 |\n",
            "| clipfrac           | 0.15019532  |\n",
            "| ep_len_mean        | 22.7        |\n",
            "| ep_reward_mean     | 0.937       |\n",
            "| explained_variance | 0.947       |\n",
            "| fps                | 379         |\n",
            "| n_updates          | 211         |\n",
            "| policy_entropy     | 0.54563695  |\n",
            "| policy_loss        | -0.02351705 |\n",
            "| serial_timesteps   | 108032      |\n",
            "| time_elapsed       | 2.49e+03    |\n",
            "| total_timesteps    | 864256      |\n",
            "| value_loss         | 0.039266128 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01258219   |\n",
            "| clipfrac           | 0.11784668   |\n",
            "| ep_len_mean        | 22.1         |\n",
            "| ep_reward_mean     | 0.939        |\n",
            "| explained_variance | 0.96         |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 212          |\n",
            "| policy_entropy     | 0.39654386   |\n",
            "| policy_loss        | -0.021723546 |\n",
            "| serial_timesteps   | 108544       |\n",
            "| time_elapsed       | 2.51e+03     |\n",
            "| total_timesteps    | 868352       |\n",
            "| value_loss         | 0.024202434  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.00 +/- 124.00\n",
            "------------------------------------\n",
            "| approxkl           | 0.01178474  |\n",
            "| clipfrac           | 0.14118652  |\n",
            "| ep_len_mean        | 24.1        |\n",
            "| ep_reward_mean     | 0.931       |\n",
            "| explained_variance | 0.965       |\n",
            "| fps                | 329         |\n",
            "| n_updates          | 213         |\n",
            "| policy_entropy     | 0.47555703  |\n",
            "| policy_loss        | -0.03246412 |\n",
            "| serial_timesteps   | 109056      |\n",
            "| time_elapsed       | 2.52e+03    |\n",
            "| total_timesteps    | 872448      |\n",
            "| value_loss         | 0.028048774 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.007863265  |\n",
            "| clipfrac           | 0.11274414   |\n",
            "| ep_len_mean        | 17.8         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.981        |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 214          |\n",
            "| policy_entropy     | 0.44620806   |\n",
            "| policy_loss        | -0.026500802 |\n",
            "| serial_timesteps   | 109568       |\n",
            "| time_elapsed       | 2.53e+03     |\n",
            "| total_timesteps    | 876544       |\n",
            "| value_loss         | 0.012870558  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 1.36\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009357629  |\n",
            "| clipfrac           | 0.117163084  |\n",
            "| ep_len_mean        | 16.4         |\n",
            "| ep_reward_mean     | 0.954        |\n",
            "| explained_variance | 0.985        |\n",
            "| fps                | 365          |\n",
            "| n_updates          | 215          |\n",
            "| policy_entropy     | 0.4803246    |\n",
            "| policy_loss        | -0.028092142 |\n",
            "| serial_timesteps   | 110080       |\n",
            "| time_elapsed       | 2.54e+03     |\n",
            "| total_timesteps    | 880640       |\n",
            "| value_loss         | 0.013744558  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0113876555 |\n",
            "| clipfrac           | 0.07416992   |\n",
            "| ep_len_mean        | 15.6         |\n",
            "| ep_reward_mean     | 0.957        |\n",
            "| explained_variance | 0.96         |\n",
            "| fps                | 378          |\n",
            "| n_updates          | 216          |\n",
            "| policy_entropy     | 0.21952896   |\n",
            "| policy_loss        | -0.02298318  |\n",
            "| serial_timesteps   | 110592       |\n",
            "| time_elapsed       | 2.55e+03     |\n",
            "| total_timesteps    | 884736       |\n",
            "| value_loss         | 0.01885198   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015291648  |\n",
            "| clipfrac           | 0.050854493  |\n",
            "| ep_len_mean        | 15.7         |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.956        |\n",
            "| fps                | 373          |\n",
            "| n_updates          | 217          |\n",
            "| policy_entropy     | 0.12290317   |\n",
            "| policy_loss        | -0.018571777 |\n",
            "| serial_timesteps   | 111104       |\n",
            "| time_elapsed       | 2.56e+03     |\n",
            "| total_timesteps    | 888832       |\n",
            "| value_loss         | 0.011105669  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.60 +/- 1.20\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01863112   |\n",
            "| clipfrac           | 0.09116211   |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.775        |\n",
            "| fps                | 363          |\n",
            "| n_updates          | 218          |\n",
            "| policy_entropy     | 0.19531251   |\n",
            "| policy_loss        | -0.017535923 |\n",
            "| serial_timesteps   | 111616       |\n",
            "| time_elapsed       | 2.57e+03     |\n",
            "| total_timesteps    | 892928       |\n",
            "| value_loss         | 0.05045262   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013642272  |\n",
            "| clipfrac           | 0.08474121   |\n",
            "| ep_len_mean        | 16.3         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.753        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 219          |\n",
            "| policy_entropy     | 0.15836447   |\n",
            "| policy_loss        | -0.017630847 |\n",
            "| serial_timesteps   | 112128       |\n",
            "| time_elapsed       | 2.58e+03     |\n",
            "| total_timesteps    | 897024       |\n",
            "| value_loss         | 0.031189948  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.02280907   |\n",
            "| clipfrac           | 0.07687988   |\n",
            "| ep_len_mean        | 16.9         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.782        |\n",
            "| fps                | 362          |\n",
            "| n_updates          | 220          |\n",
            "| policy_entropy     | 0.13563623   |\n",
            "| policy_loss        | -0.017819311 |\n",
            "| serial_timesteps   | 112640       |\n",
            "| time_elapsed       | 2.59e+03     |\n",
            "| total_timesteps    | 901120       |\n",
            "| value_loss         | 0.02185091   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031963788  |\n",
            "| clipfrac           | 0.093603514  |\n",
            "| ep_len_mean        | 16           |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.629        |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 221          |\n",
            "| policy_entropy     | 0.14315514   |\n",
            "| policy_loss        | -0.016637225 |\n",
            "| serial_timesteps   | 113152       |\n",
            "| time_elapsed       | 2.61e+03     |\n",
            "| total_timesteps    | 905216       |\n",
            "| value_loss         | 0.025282372  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026925635  |\n",
            "| clipfrac           | 0.074560545  |\n",
            "| ep_len_mean        | 16.1         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.7          |\n",
            "| fps                | 376          |\n",
            "| n_updates          | 222          |\n",
            "| policy_entropy     | 0.10280093   |\n",
            "| policy_loss        | -0.015223687 |\n",
            "| serial_timesteps   | 113664       |\n",
            "| time_elapsed       | 2.62e+03     |\n",
            "| total_timesteps    | 909312       |\n",
            "| value_loss         | 0.012412975  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.60 +/- 0.80\n",
            "-------------------------------------\n",
            "| approxkl           | 0.018704964  |\n",
            "| clipfrac           | 0.059790038  |\n",
            "| ep_len_mean        | 16.1         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.647        |\n",
            "| fps                | 366          |\n",
            "| n_updates          | 223          |\n",
            "| policy_entropy     | 0.09625149   |\n",
            "| policy_loss        | -0.013961087 |\n",
            "| serial_timesteps   | 114176       |\n",
            "| time_elapsed       | 2.63e+03     |\n",
            "| total_timesteps    | 913408       |\n",
            "| value_loss         | 0.008379607  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.034839615  |\n",
            "| clipfrac           | 0.12165527   |\n",
            "| ep_len_mean        | 17.2         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.558        |\n",
            "| fps                | 368          |\n",
            "| n_updates          | 224          |\n",
            "| policy_entropy     | 0.1980963    |\n",
            "| policy_loss        | -0.026502386 |\n",
            "| serial_timesteps   | 114688       |\n",
            "| time_elapsed       | 2.64e+03     |\n",
            "| total_timesteps    | 917504       |\n",
            "| value_loss         | 0.018635046  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.80 +/- 123.61\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01815444   |\n",
            "| clipfrac           | 0.12424316   |\n",
            "| ep_len_mean        | 19.1         |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.695        |\n",
            "| fps                | 326          |\n",
            "| n_updates          | 225          |\n",
            "| policy_entropy     | 0.30376166   |\n",
            "| policy_loss        | -0.023223188 |\n",
            "| serial_timesteps   | 115200       |\n",
            "| time_elapsed       | 2.65e+03     |\n",
            "| total_timesteps    | 921600       |\n",
            "| value_loss         | 0.030696232  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.017807115  |\n",
            "| clipfrac           | 0.2019287    |\n",
            "| ep_len_mean        | 29.1         |\n",
            "| ep_reward_mean     | 0.915        |\n",
            "| explained_variance | 0.586        |\n",
            "| fps                | 386          |\n",
            "| n_updates          | 226          |\n",
            "| policy_entropy     | 0.753124     |\n",
            "| policy_loss        | -0.022945073 |\n",
            "| serial_timesteps   | 115712       |\n",
            "| time_elapsed       | 2.66e+03     |\n",
            "| total_timesteps    | 925696       |\n",
            "| value_loss         | 0.16461349   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010788182  |\n",
            "| clipfrac           | 0.11035156   |\n",
            "| ep_len_mean        | 21.2         |\n",
            "| ep_reward_mean     | 0.941        |\n",
            "| explained_variance | 0.824        |\n",
            "| fps                | 381          |\n",
            "| n_updates          | 227          |\n",
            "| policy_entropy     | 0.38676304   |\n",
            "| policy_loss        | -0.025798272 |\n",
            "| serial_timesteps   | 116224       |\n",
            "| time_elapsed       | 2.67e+03     |\n",
            "| total_timesteps    | 929792       |\n",
            "| value_loss         | 0.03970968   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=0.77 +/- 0.38\n",
            "Episode length: 76.60 +/- 123.70\n",
            "-------------------------------------\n",
            "| approxkl           | 0.00830687   |\n",
            "| clipfrac           | 0.09975586   |\n",
            "| ep_len_mean        | 16.8         |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.9          |\n",
            "| fps                | 333          |\n",
            "| n_updates          | 228          |\n",
            "| policy_entropy     | 0.35835344   |\n",
            "| policy_loss        | -0.023853142 |\n",
            "| serial_timesteps   | 116736       |\n",
            "| time_elapsed       | 2.68e+03     |\n",
            "| total_timesteps    | 933888       |\n",
            "| value_loss         | 0.024893563  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.012592236  |\n",
            "| clipfrac           | 0.13552245   |\n",
            "| ep_len_mean        | 18.2         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.923        |\n",
            "| fps                | 382          |\n",
            "| n_updates          | 229          |\n",
            "| policy_entropy     | 0.6526748    |\n",
            "| policy_loss        | -0.022235364 |\n",
            "| serial_timesteps   | 117248       |\n",
            "| time_elapsed       | 2.7e+03      |\n",
            "| total_timesteps    | 937984       |\n",
            "| value_loss         | 0.045455806  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=0.57 +/- 0.47\n",
            "Episode length: 139.00 +/- 151.05\n",
            "-------------------------------------\n",
            "| approxkl           | 0.00832588   |\n",
            "| clipfrac           | 0.10290527   |\n",
            "| ep_len_mean        | 18.9         |\n",
            "| ep_reward_mean     | 0.947        |\n",
            "| explained_variance | 0.904        |\n",
            "| fps                | 302          |\n",
            "| n_updates          | 230          |\n",
            "| policy_entropy     | 0.37651545   |\n",
            "| policy_loss        | -0.022509808 |\n",
            "| serial_timesteps   | 117760       |\n",
            "| time_elapsed       | 2.71e+03     |\n",
            "| total_timesteps    | 942080       |\n",
            "| value_loss         | 0.0547073    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008419415  |\n",
            "| clipfrac           | 0.04855957   |\n",
            "| ep_len_mean        | 15           |\n",
            "| ep_reward_mean     | 0.958        |\n",
            "| explained_variance | 0.961        |\n",
            "| fps                | 374          |\n",
            "| n_updates          | 231          |\n",
            "| policy_entropy     | 0.17741914   |\n",
            "| policy_loss        | -0.013494613 |\n",
            "| serial_timesteps   | 118272       |\n",
            "| time_elapsed       | 2.72e+03     |\n",
            "| total_timesteps    | 946176       |\n",
            "| value_loss         | 0.010172763  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.00 +/- 1.10\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015417829  |\n",
            "| clipfrac           | 0.06826172   |\n",
            "| ep_len_mean        | 15.6         |\n",
            "| ep_reward_mean     | 0.957        |\n",
            "| explained_variance | 0.977        |\n",
            "| fps                | 368          |\n",
            "| n_updates          | 232          |\n",
            "| policy_entropy     | 0.21224347   |\n",
            "| policy_loss        | -0.022050157 |\n",
            "| serial_timesteps   | 118784       |\n",
            "| time_elapsed       | 2.73e+03     |\n",
            "| total_timesteps    | 950272       |\n",
            "| value_loss         | 0.0067085447 |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.007385289 |\n",
            "| clipfrac           | 0.06303711  |\n",
            "| ep_len_mean        | 16.7        |\n",
            "| ep_reward_mean     | 0.954       |\n",
            "| explained_variance | 0.902       |\n",
            "| fps                | 379         |\n",
            "| n_updates          | 233         |\n",
            "| policy_entropy     | 0.16281073  |\n",
            "| policy_loss        | -0.0184459  |\n",
            "| serial_timesteps   | 119296      |\n",
            "| time_elapsed       | 2.74e+03    |\n",
            "| total_timesteps    | 954368      |\n",
            "| value_loss         | 0.016454473 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.041571837  |\n",
            "| clipfrac           | 0.05114746   |\n",
            "| ep_len_mean        | 15.5         |\n",
            "| ep_reward_mean     | 0.957        |\n",
            "| explained_variance | 0.647        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 234          |\n",
            "| policy_entropy     | 0.08804727   |\n",
            "| policy_loss        | -0.008202116 |\n",
            "| serial_timesteps   | 119808       |\n",
            "| time_elapsed       | 2.75e+03     |\n",
            "| total_timesteps    | 958464       |\n",
            "| value_loss         | 0.017857349  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 15.40 +/- 1.02\n",
            "-------------------------------------\n",
            "| approxkl           | 0.61379874   |\n",
            "| clipfrac           | 0.19477539   |\n",
            "| ep_len_mean        | 26.2         |\n",
            "| ep_reward_mean     | 0.924        |\n",
            "| explained_variance | 0.351        |\n",
            "| fps                | 364          |\n",
            "| n_updates          | 235          |\n",
            "| policy_entropy     | 0.16555738   |\n",
            "| policy_loss        | -0.041959647 |\n",
            "| serial_timesteps   | 120320       |\n",
            "| time_elapsed       | 2.76e+03     |\n",
            "| total_timesteps    | 962560       |\n",
            "| value_loss         | 0.14633337   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.026095003  |\n",
            "| clipfrac           | 0.053491212  |\n",
            "| ep_len_mean        | 16.1         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.694        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 236          |\n",
            "| policy_entropy     | 0.09910532   |\n",
            "| policy_loss        | -0.013462298 |\n",
            "| serial_timesteps   | 120832       |\n",
            "| time_elapsed       | 2.78e+03     |\n",
            "| total_timesteps    | 966656       |\n",
            "| value_loss         | 0.013335395  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.72\n",
            "-------------------------------------\n",
            "| approxkl           | 0.096465826  |\n",
            "| clipfrac           | 0.27817383   |\n",
            "| ep_len_mean        | 25.3         |\n",
            "| ep_reward_mean     | 0.929        |\n",
            "| explained_variance | 0.632        |\n",
            "| fps                | 373          |\n",
            "| n_updates          | 237          |\n",
            "| policy_entropy     | 0.31601143   |\n",
            "| policy_loss        | -0.030757844 |\n",
            "| serial_timesteps   | 121344       |\n",
            "| time_elapsed       | 2.79e+03     |\n",
            "| total_timesteps    | 970752       |\n",
            "| value_loss         | 0.11090443   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.03689914  |\n",
            "| clipfrac           | 0.27946776  |\n",
            "| ep_len_mean        | 22.9        |\n",
            "| ep_reward_mean     | 0.934       |\n",
            "| explained_variance | 0.612       |\n",
            "| fps                | 379         |\n",
            "| n_updates          | 238         |\n",
            "| policy_entropy     | 0.3049832   |\n",
            "| policy_loss        | 0.018580072 |\n",
            "| serial_timesteps   | 121856      |\n",
            "| time_elapsed       | 2.8e+03     |\n",
            "| total_timesteps    | 974848      |\n",
            "| value_loss         | 0.13746555  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.047908034 |\n",
            "| clipfrac           | 0.15700683  |\n",
            "| ep_len_mean        | 19.4        |\n",
            "| ep_reward_mean     | 0.946       |\n",
            "| explained_variance | 0.754       |\n",
            "| fps                | 377         |\n",
            "| n_updates          | 239         |\n",
            "| policy_entropy     | 0.20188543  |\n",
            "| policy_loss        | 0.07445566  |\n",
            "| serial_timesteps   | 122368      |\n",
            "| time_elapsed       | 2.81e+03    |\n",
            "| total_timesteps    | 978944      |\n",
            "| value_loss         | 0.055641323 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.60 +/- 0.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.09855242   |\n",
            "| clipfrac           | 0.07531738   |\n",
            "| ep_len_mean        | 15.7         |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.848        |\n",
            "| fps                | 369          |\n",
            "| n_updates          | 240          |\n",
            "| policy_entropy     | 0.1347876    |\n",
            "| policy_loss        | -0.011207625 |\n",
            "| serial_timesteps   | 122880       |\n",
            "| time_elapsed       | 2.82e+03     |\n",
            "| total_timesteps    | 983040       |\n",
            "| value_loss         | 0.017387576  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.06911071   |\n",
            "| clipfrac           | 0.08334961   |\n",
            "| ep_len_mean        | 18.5         |\n",
            "| ep_reward_mean     | 0.948        |\n",
            "| explained_variance | 0.648        |\n",
            "| fps                | 377          |\n",
            "| n_updates          | 241          |\n",
            "| policy_entropy     | 0.11575633   |\n",
            "| policy_loss        | -0.017425437 |\n",
            "| serial_timesteps   | 123392       |\n",
            "| time_elapsed       | 2.83e+03     |\n",
            "| total_timesteps    | 987136       |\n",
            "| value_loss         | 0.04174084   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 14.80 +/- 1.17\n",
            "--------------------------------------\n",
            "| approxkl           | 0.013964276   |\n",
            "| clipfrac           | 0.053393554   |\n",
            "| ep_len_mean        | 15.8          |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.811         |\n",
            "| fps                | 366           |\n",
            "| n_updates          | 242           |\n",
            "| policy_entropy     | 0.09310523    |\n",
            "| policy_loss        | -0.0021985306 |\n",
            "| serial_timesteps   | 123904        |\n",
            "| time_elapsed       | 2.84e+03      |\n",
            "| total_timesteps    | 991232        |\n",
            "| value_loss         | 0.011314899   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.012134708   |\n",
            "| clipfrac           | 0.027075196   |\n",
            "| ep_len_mean        | 14.9          |\n",
            "| ep_reward_mean     | 0.959         |\n",
            "| explained_variance | 0.81          |\n",
            "| fps                | 376           |\n",
            "| n_updates          | 243           |\n",
            "| policy_entropy     | 0.049179725   |\n",
            "| policy_loss        | -0.0048836344 |\n",
            "| serial_timesteps   | 124416        |\n",
            "| time_elapsed       | 2.85e+03      |\n",
            "| total_timesteps    | 995328        |\n",
            "| value_loss         | 0.006149716   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.03490216   |\n",
            "| clipfrac           | 0.10554199   |\n",
            "| ep_len_mean        | 16.3         |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.808        |\n",
            "| fps                | 379          |\n",
            "| n_updates          | 244          |\n",
            "| policy_entropy     | 0.17888553   |\n",
            "| policy_loss        | -0.025703529 |\n",
            "| serial_timesteps   | 124928       |\n",
            "| time_elapsed       | 2.86e+03     |\n",
            "| total_timesteps    | 999424       |\n",
            "| value_loss         | 0.018679468  |\n",
            "-------------------------------------\n",
            "WARNING:tensorflow:From train.py:431: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "Saving to logs/ppo2/MiniGrid-SimpleCrossingS9N1-v0_1\n",
            "\u001b[0m[957a75675444:01404] *** Process received signal ***\n",
            "[957a75675444:01404] Signal: Segmentation fault (11)\n",
            "[957a75675444:01404] Signal code: Address not mapped (1)\n",
            "[957a75675444:01404] Failing at address: 0x7f17ec6e220d\n",
            "[957a75675444:01404] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f17ef792980]\n",
            "[957a75675444:01404] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f17ef3d18a5]\n",
            "[957a75675444:01404] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f17efc3ce44]\n",
            "[957a75675444:01404] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f17ef3d2735]\n",
            "[957a75675444:01404] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f17efc3acb3]\n",
            "[957a75675444:01404] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bIR_N7R11XI",
        "outputId": "57378ec5-99a9-4b80-9e72-1e16306040b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingEnvUmaze-v0 --gym-packages gym_minigrid\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MiniGrid-SimpleCrossingEnvUmaze-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('cliprange', 0.2),\n",
            "             ('ent_coef', 0.0),\n",
            "             ('env_wrapper',\n",
            "              ['gym_minigrid.wrappers.RGBImgPartialObsWrapper',\n",
            "               'gym_minigrid.wrappers.ImgObsWrapper']),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.95),\n",
            "             ('learning_rate', 0.00025),\n",
            "             ('n_envs', 8),\n",
            "             ('n_steps', 512),\n",
            "             ('n_timesteps', 4000000.0),\n",
            "             ('nminibatches', 32),\n",
            "             ('noptepochs', 10),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 8 environments\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 402, in <module>\n",
            "    model = ALGOS[args.algo](env=env, tensorboard_log=tensorboard_log, verbose=args.verbose, **hyperparams)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py\", line 97, in __init__\n",
            "    self.setup_model()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py\", line 240, in setup_model\n",
            "    tf.global_variables_initializer().run(session=self.sess)  # pylint: disable=E1101\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 2439, in run\n",
            "    _run_using_default_session(self, feed_dict, self.graph, session)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 5442, in _run_using_default_session\n",
            "    session.run(operation, feed_dict)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n",
            "\u001b[0m[8a95a88f41d8:01350] *** Process received signal ***\n",
            "[8a95a88f41d8:01350] Signal: Segmentation fault (11)\n",
            "[8a95a88f41d8:01350] Signal code: Address not mapped (1)\n",
            "[8a95a88f41d8:01350] Failing at address: 0x7f51ca6cd20d\n",
            "[8a95a88f41d8:01350] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f51cd77d980]\n",
            "[8a95a88f41d8:01350] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f51cd3bc8a5]\n",
            "[8a95a88f41d8:01350] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f51cdc27e44]\n",
            "[8a95a88f41d8:01350] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f51cd3bd735]\n",
            "[8a95a88f41d8:01350] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f51cdc25cb3]\n",
            "[8a95a88f41d8:01350] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fHBq73665yD"
      },
      "source": [
        "#### Evaluate trained agent\n",
        "\n",
        "\n",
        "You can remove the `--folder logs/` to evaluate pretrained agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8YuEgU6bT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8d4371-cb9e-4732-e43a-73da211a0541"
      },
      "source": [
        "!python enjoy.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --no-render --n-timesteps 5000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"enjoy.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"enjoy.py\", line 77, in main\n",
            "    model_path = find_saved_model(algo, log_path, env_id, load_best=args.load_best)\n",
            "  File \"/content/rl-baselines-zoo/utils/utils.py\", line 369, in find_saved_model\n",
            "    raise ValueError(\"No model found for {} on {}, path: {}\".format(algo, env_id, model_path))\n",
            "ValueError: No model found for ppo2 on MiniGrid-SimpleCrossingS9N1-v0, path: logs/ppo2/MiniGrid-SimpleCrossingS9N1-v0.zip\n",
            "[8ff3b965f021:02585] *** Process received signal ***\n",
            "[8ff3b965f021:02585] Signal: Segmentation fault (11)\n",
            "[8ff3b965f021:02585] Signal code: Address not mapped (1)\n",
            "[8ff3b965f021:02585] Failing at address: 0x7fbd50a6c20d\n",
            "[8ff3b965f021:02585] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fbd53b1c980]\n",
            "[8ff3b965f021:02585] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fbd5375b8a5]\n",
            "[8ff3b965f021:02585] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fbd53fc6e44]\n",
            "[8ff3b965f021:02585] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fbd5375c735]\n",
            "[8ff3b965f021:02585] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fbd53fc4cb3]\n",
            "[8ff3b965f021:02585] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Il2J0VHPLC"
      },
      "source": [
        "#### Tune Hyperparameters\n",
        "\n",
        "We use [Optuna](https://optuna.org/) for optimizing the hyperparameters.\n",
        "\n",
        "Tune the hyperparameters for PPO2, using a tpe sampler and median pruner, 2 parallels jobs,\n",
        "with a budget of 1000 trials and a maximum of 50000 steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Record  a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS1VBBaQ_emT"
      },
      "source": [
        "!pip install pyglet==1.3.1  # pyglet v1.4.1 throws an error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip3AauLzwNGP"
      },
      "source": [
        "!python -m utils.record_video --algo ppo2 --env MiniGrid-SimpleCrossingEnvUmaze-v0 --gym-packages gym_minigrid --exp-id 0 -f logs/ -n 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBuUfnzI8DN6"
      },
      "source": [
        "### Display the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3OTfpf8CXu"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOjFuwK9HI0"
      },
      "source": [
        "show_videos(prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjdpP0HE8D2p"
      },
      "source": [
        "### Continue Training\n",
        "\n",
        "Here, we will continue training of the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgMZQJJF6u1C"
      },
      "source": [
        "!python train.py --algo a2c --env CartPole-v1 --n-timesteps 50000 -i logs/a2c/CartPole-v1.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSaoyiAE8cVj"
      },
      "source": [
        "!python enjoy.py --algo a2c --env CartPole-v1 --no-render --n-timesteps 1000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9u4I1H-48O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}