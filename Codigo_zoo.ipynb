{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codigo zoo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpg/RL-Agent-for-Unreal-Engine/blob/main/Codigo_zoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy9QoDC7XA7"
      },
      "source": [
        "# RL Baselines Zoo: Training in Colab\n",
        "\n",
        "\n",
        "\n",
        "Github Repo: [https://github.com/araffin/rl-baselines-zoo](https://github.com/araffin/rl-baselines-zoo)\n",
        "\n",
        "Stable-Baselines Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
        "\n",
        "# Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXVDDlTn02M9",
        "outputId": "075b1911-20a1-46bc-b783-97e57774d051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt-get update\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg freeglut3-dev xvfb\n",
        "!pip install stable-baselines[mpi] --upgrade\n",
        "!pip install pybullet\n",
        "!pip install box2d box2d-kengz pyyaml pytablewriter optuna scikit-optimize\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,688 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [252 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,130 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,208 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [54.4 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n",
            "Fetched 7,469 kB in 4s (2,100 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0 xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 1,884 kB of archives.\n",
            "After this operation, 8,089 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.7 [783 kB]\n",
            "Fetched 1,884 kB in 1s (1,379 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 144793 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.7_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines[mpi]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl (240kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->stable-baselines[mpi]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.1\n",
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/61/2fc2c19327966ca4e4133211be3f4dcc56c1ee6f392d71d0da8c6c1a4cba/pybullet-3.0.6-cp36-cp36m-manylinux1_x86_64.whl (102.2MB)\n",
            "\u001b[K     |████████████████████████████████| 102.2MB 99kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.0.6\n",
            "Collecting box2d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 10.2MB/s \n",
            "\u001b[?25hCollecting box2d-kengz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Collecting pytablewriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.3MB/s \n",
            "\u001b[?25hCollecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 32.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.9MB/s \n",
            "\u001b[?25hCollecting DataProperty<2,>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/2d/e5413965af992f4e489b6f5eebf52db9c17953c772962d1223d434b05cef/DataProperty-0.50.0-py3-none-any.whl\n",
            "Collecting tabledata<2,>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/df/b2/264d9707502f0259a3eb82ec48064df98b1735d5a5f315b6a1d7105263f4/tabledata-1.1.3-py3-none-any.whl\n",
            "Collecting mbstrdecoder<2,>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/57/3bb55beafe0a5e9883621f01a560d16bcef6d4f844dc2dd40caa0a8d9182/mbstrdecoder-1.0.0-py3-none-any.whl\n",
            "Collecting tcolorpy<1,>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/02/51/bbb0cc7f30771c285c354634bf83653a2871d58c6923bd29bfddeb9c9cb1/tcolorpy-0.0.8-py3-none-any.whl\n",
            "Collecting msgfy<1,>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl\n",
            "Collecting pathvalidate<3,>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/fa/1a951084aa93940399800e37ed6f096ad5c0de3c26604be62f9464a39fc1/pathvalidate-2.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.6/dist-packages (from pytablewriter) (50.3.2)\n",
            "Collecting typepy[datetime]<2,>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/55/a1111b2eb1f4096c28b14645ca62aec560b1768338af21620e470b60872f/typepy-1.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.4)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/61/5b64d73b01c1218f55c894b5ec0fb89b32c6960b7f7b3ad9f5ac0c373b9d/cliff-3.5.0-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.17.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.20)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 47.4MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/3c/06c76ec8b54b9b1fad7f35e903fd25010fe3e0d41bd94cea5e6f12e0d651/cmaes-0.7.0-py3-none-any.whl\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c8/c16d30bbed11a1722060014c246d124582d1f781b26f5859d8dacc3e08e1/colorlog-4.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.4 in /usr/local/lib/python3.6/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2018.9; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2018.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\" in /usr/local/lib/python3.6/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Collecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/54/af6e2703f064485d717cb311d3f9440cd302a823ba6d80a020b59eae166d/cmd2-1.4.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 32.0MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/a1/004f04ba411a8002b02aadb089fd6868116c12ddc9f6d576175e89d07587/stevedore-3.2.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 37.0MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.2MB/s \n",
            "\u001b[?25hCollecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (2.0.0)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.4.0)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp36-none-any.whl size=359761 sha256=0ff6dba915eab04adfeac33286039363da41c4861dd7555a178d991168145730\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: box2d-kengz, PrettyTable, pyperclip\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp36-cp36m-linux_x86_64.whl size=2017888 sha256=b466708527a49c62cdadaefd4ef01e9391353a4832b2734167ba0ea5d859f239\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-cp36-none-any.whl size=13700 sha256=08b49bc9f67a93fd9fbddb5d04ac9ab10ed0c9460391b103c284cfa1b72cf1f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11119 sha256=0d38ddc977e5c1217ad45af1187a77717835617453e9f6c6aab8b88ee169e4ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built box2d-kengz PrettyTable pyperclip\n",
            "Installing collected packages: box2d, box2d-kengz, mbstrdecoder, typepy, DataProperty, tabledata, tcolorpy, msgfy, pathvalidate, pytablewriter, PrettyTable, pyperclip, colorama, cmd2, pbr, stevedore, cliff, python-editor, Mako, alembic, cmaes, colorlog, optuna, pyaml, scikit-optimize\n",
            "  Found existing installation: prettytable 1.0.1\n",
            "    Uninstalling prettytable-1.0.1:\n",
            "      Successfully uninstalled prettytable-1.0.1\n",
            "Successfully installed DataProperty-0.50.0 Mako-1.1.3 PrettyTable-0.7.2 alembic-1.4.3 box2d-2.3.10 box2d-kengz-2.3.3 cliff-3.5.0 cmaes-0.7.0 cmd2-1.4.0 colorama-0.4.4 colorlog-4.6.2 mbstrdecoder-1.0.0 msgfy-0.1.0 optuna-2.3.0 pathvalidate-2.3.0 pbr-5.5.1 pyaml-20.4.0 pyperclip-1.8.1 pytablewriter-0.58.0 python-editor-1.0.4 scikit-optimize-0.8.1 stevedore-3.2.2 tabledata-1.1.3 tcolorpy-0.0.8 typepy-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjF3qRg7oGH"
      },
      "source": [
        "## Clone RL Baselines Zoo Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjGikdT1DFy",
        "outputId": "dbc43931-5838-4243-b8df-af97e72df0dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines-zoo'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 1834 (delta 15), reused 22 (delta 9), pack-reused 1796\u001b[K\n",
            "Receiving objects: 100% (1834/1834), 375.67 MiB | 35.64 MiB/s, done.\n",
            "Resolving deltas: 100% (1080/1080), done.\n",
            "Checking out files: 100% (333/333), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REMQlh-ezyVt",
        "outputId": "afa17271-6fe7-4ba6-b1b2-76a8611e0e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd rl-baselines-zoo/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rl-baselines-zoo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ-pAbF7zRZ"
      },
      "source": [
        "## Train an RL Agent\n",
        "\n",
        "\n",
        "The train agent can be found in the `logs/` folder.\n",
        "\n",
        "Here we will train A2C on CartPole-v1 environment for 100 000 steps. \n",
        "\n",
        "\n",
        "To train it on Pong (Atari), you just have to pass `--env PongNoFrameskip-v4`\n",
        "\n",
        "Note: You need to update `hyperparams/algo.yml` to support new environments. You can access it in the side panel of Google Colab. (see https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34lXM8ZfMQfG",
        "outputId": "8fcc176b-d622-4608-de99-05a12355768f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gym-minigrid\n",
        "# go to gym-minigrid in \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs\" and replace the modeled crossing env"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-minigrid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/57/15171eff6222dd012cc89c001f5c50ad9e11b8cef385873db3b4c0d89aff/gym_minigrid-1.0.1-py3-none-any.whl (47kB)\n",
            "\r\u001b[K     |███████                         | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 20kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 30kB 20.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 40kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from gym-minigrid) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.6->gym-minigrid) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.9.6->gym-minigrid) (0.16.0)\n",
            "Installing collected packages: gym-minigrid\n",
            "Successfully installed gym-minigrid-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmsGh9A8p4NO",
        "outputId": "ad33af24-cc3d-424d-8729-400f1a2a2932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# moving the mod crossing file (U MAZE MODELED) to the destination path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%rm \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs/crossing.py\"\n",
        "%cp \"/content/drive/My Drive/Colab Notebooks/crossing.py\" \"/usr/local/lib/python3.6/dist-packages/gym_minigrid/envs\"\n",
        "\n",
        "#SIMULTANEAMENTE SE DEBE REEMPLAZAR LA INFORMACIÓN DEL ENTORNO EN LOS HIPERPARAMETROS PPO2 DEL PAQUETE ZOO\n",
        "%rm \"/content/rl-baselines-zoo/hyperparams/ppo2.yml\"\n",
        "%cp \"/content/drive/My Drive/Colab Notebooks/ppo2.yml\" \"/content/rl-baselines-zoo/hyperparams\"\n",
        "\n",
        "#tambien, actualizar el archivo train.py con cambios en los env wrappers.\n",
        "%rm \"/content/rl-baselines-zoo/train.py\"\n",
        "%cp \"/content/drive/My Drive/Colab Notebooks/train.py\" \"/content/rl-baselines-zoo\"\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bIR_N7R11XI",
        "outputId": "db242135-cd54-41f1-de0b-7989f6ba193b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "!python train.py --algo ppo2 --env MiniGrid-SimpleCrossingEnvUmaze-v0 --gym-packages gym_minigrid --n-timesteps 1000000\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MiniGrid-SimpleCrossingEnvUmaze-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('cliprange', 0.2),\n",
            "             ('ent_coef', 0.0),\n",
            "             ('env_wrapper',\n",
            "              ['gym_minigrid.wrappers.RGBImgPartialObsWrapper',\n",
            "               'gym_minigrid.wrappers.ImgObsWrapper']),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.95),\n",
            "             ('learning_rate', 0.00025),\n",
            "             ('n_envs', 8),\n",
            "             ('n_steps', 512),\n",
            "             ('n_timesteps', 4000000.0),\n",
            "             ('nminibatches', 32),\n",
            "             ('noptepochs', 10),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 8 environments\n",
            "Overwriting n_timesteps with n=1000000\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MiniGrid-SimpleCrossingEnvUmaze-v0_1\n",
            "------------------------------------\n",
            "| approxkl           | 0.008288548 |\n",
            "| clipfrac           | 0.1194336   |\n",
            "| ep_len_mean        | 319         |\n",
            "| ep_reward_mean     | 0.0257      |\n",
            "| explained_variance | -1.03       |\n",
            "| fps                | 337         |\n",
            "| n_updates          | 1           |\n",
            "| policy_entropy     | 1.9375238   |\n",
            "| policy_loss        | -0.00972842 |\n",
            "| serial_timesteps   | 512         |\n",
            "| time_elapsed       | 2.19e-05    |\n",
            "| total_timesteps    | 4096        |\n",
            "| value_loss         | 0.20200539  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.009459034  |\n",
            "| clipfrac           | 0.12788086   |\n",
            "| ep_len_mean        | 314          |\n",
            "| ep_reward_mean     | 0.0495       |\n",
            "| explained_variance | -0.0733      |\n",
            "| fps                | 407          |\n",
            "| n_updates          | 2            |\n",
            "| policy_entropy     | 1.9165304    |\n",
            "| policy_loss        | -0.011656886 |\n",
            "| serial_timesteps   | 1024         |\n",
            "| time_elapsed       | 12.2         |\n",
            "| total_timesteps    | 8192         |\n",
            "| value_loss         | 0.39010388   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "New best mean reward!\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008598836   |\n",
            "| clipfrac           | 0.13823242    |\n",
            "| ep_len_mean        | 307           |\n",
            "| ep_reward_mean     | 0.0694        |\n",
            "| explained_variance | -0.0546       |\n",
            "| fps                | 249           |\n",
            "| n_updates          | 3             |\n",
            "| policy_entropy     | 1.8892279     |\n",
            "| policy_loss        | -0.0070531503 |\n",
            "| serial_timesteps   | 1536          |\n",
            "| time_elapsed       | 22.2          |\n",
            "| total_timesteps    | 12288         |\n",
            "| value_loss         | 0.28904167    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0065744645 |\n",
            "| clipfrac           | 0.09362793   |\n",
            "| ep_len_mean        | 287          |\n",
            "| ep_reward_mean     | 0.141        |\n",
            "| explained_variance | -6.33e-05    |\n",
            "| fps                | 414          |\n",
            "| n_updates          | 4            |\n",
            "| policy_entropy     | 1.8575847    |\n",
            "| policy_loss        | -0.01064202  |\n",
            "| serial_timesteps   | 2048         |\n",
            "| time_elapsed       | 38.6         |\n",
            "| total_timesteps    | 16384        |\n",
            "| value_loss         | 1.1041621    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008406256  |\n",
            "| clipfrac           | 0.09963379   |\n",
            "| ep_len_mean        | 255          |\n",
            "| ep_reward_mean     | 0.246        |\n",
            "| explained_variance | 0.00863      |\n",
            "| fps                | 258          |\n",
            "| n_updates          | 5            |\n",
            "| policy_entropy     | 1.8255059    |\n",
            "| policy_loss        | -0.011880587 |\n",
            "| serial_timesteps   | 2560         |\n",
            "| time_elapsed       | 48.5         |\n",
            "| total_timesteps    | 20480        |\n",
            "| value_loss         | 1.7724469    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.009471087 |\n",
            "| clipfrac           | 0.13781738  |\n",
            "| ep_len_mean        | 198         |\n",
            "| ep_reward_mean     | 0.427       |\n",
            "| explained_variance | 0.0027      |\n",
            "| fps                | 401         |\n",
            "| n_updates          | 6           |\n",
            "| policy_entropy     | 1.7559712   |\n",
            "| policy_loss        | -0.01529026 |\n",
            "| serial_timesteps   | 3072        |\n",
            "| time_elapsed       | 64.4        |\n",
            "| total_timesteps    | 24576       |\n",
            "| value_loss         | 2.9393425   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.010943096 |\n",
            "| clipfrac           | 0.16027832  |\n",
            "| ep_len_mean        | 98.7        |\n",
            "| ep_reward_mean     | 0.724       |\n",
            "| explained_variance | 0.0036      |\n",
            "| fps                | 405         |\n",
            "| n_updates          | 7           |\n",
            "| policy_entropy     | 1.6669528   |\n",
            "| policy_loss        | -0.01613478 |\n",
            "| serial_timesteps   | 3584        |\n",
            "| time_elapsed       | 74.6        |\n",
            "| total_timesteps    | 28672       |\n",
            "| value_loss         | 4.2713785   |\n",
            "------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.014421788  |\n",
            "| clipfrac           | 0.21582031   |\n",
            "| ep_len_mean        | 52.4         |\n",
            "| ep_reward_mean     | 0.855        |\n",
            "| explained_variance | 0.00175      |\n",
            "| fps                | 388          |\n",
            "| n_updates          | 8            |\n",
            "| policy_entropy     | 1.515856     |\n",
            "| policy_loss        | -0.020407934 |\n",
            "| serial_timesteps   | 4096         |\n",
            "| time_elapsed       | 84.7         |\n",
            "| total_timesteps    | 32768        |\n",
            "| value_loss         | 5.959792     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.012373338  |\n",
            "| clipfrac           | 0.16005859   |\n",
            "| ep_len_mean        | 49.6         |\n",
            "| ep_reward_mean     | 0.862        |\n",
            "| explained_variance | 0.0243       |\n",
            "| fps                | 403          |\n",
            "| n_updates          | 9            |\n",
            "| policy_entropy     | 1.5377638    |\n",
            "| policy_loss        | -0.010232383 |\n",
            "| serial_timesteps   | 4608         |\n",
            "| time_elapsed       | 95.2         |\n",
            "| total_timesteps    | 36864        |\n",
            "| value_loss         | 5.162838     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.019840518  |\n",
            "| clipfrac           | 0.20834962   |\n",
            "| ep_len_mean        | 69.3         |\n",
            "| ep_reward_mean     | 0.808        |\n",
            "| explained_variance | -0.0159      |\n",
            "| fps                | 246          |\n",
            "| n_updates          | 10           |\n",
            "| policy_entropy     | 1.7320368    |\n",
            "| policy_loss        | -0.009091964 |\n",
            "| serial_timesteps   | 5120         |\n",
            "| time_elapsed       | 105          |\n",
            "| total_timesteps    | 40960        |\n",
            "| value_loss         | 2.7531521    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.016610783  |\n",
            "| clipfrac           | 0.21794434   |\n",
            "| ep_len_mean        | 95.2         |\n",
            "| ep_reward_mean     | 0.735        |\n",
            "| explained_variance | -0.0315      |\n",
            "| fps                | 409          |\n",
            "| n_updates          | 11           |\n",
            "| policy_entropy     | 1.7468879    |\n",
            "| policy_loss        | -0.011676379 |\n",
            "| serial_timesteps   | 5632         |\n",
            "| time_elapsed       | 122          |\n",
            "| total_timesteps    | 45056        |\n",
            "| value_loss         | 1.6729975    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.013291555   |\n",
            "| clipfrac           | 0.15810546    |\n",
            "| ep_len_mean        | 114           |\n",
            "| ep_reward_mean     | 0.683         |\n",
            "| explained_variance | 0.0387        |\n",
            "| fps                | 403           |\n",
            "| n_updates          | 12            |\n",
            "| policy_entropy     | 1.7514279     |\n",
            "| policy_loss        | -0.0058429996 |\n",
            "| serial_timesteps   | 6144          |\n",
            "| time_elapsed       | 132           |\n",
            "| total_timesteps    | 49152         |\n",
            "| value_loss         | 1.4179622     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 324.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010576343  |\n",
            "| clipfrac           | 0.13439941   |\n",
            "| ep_len_mean        | 127          |\n",
            "| ep_reward_mean     | 0.646        |\n",
            "| explained_variance | 0.0747       |\n",
            "| fps                | 250          |\n",
            "| n_updates          | 13           |\n",
            "| policy_entropy     | 1.7415081    |\n",
            "| policy_loss        | -0.011882305 |\n",
            "| serial_timesteps   | 6656         |\n",
            "| time_elapsed       | 142          |\n",
            "| total_timesteps    | 53248        |\n",
            "| value_loss         | 1.4994811    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011463578  |\n",
            "| clipfrac           | 0.16296387   |\n",
            "| ep_len_mean        | 114          |\n",
            "| ep_reward_mean     | 0.681        |\n",
            "| explained_variance | 0.136        |\n",
            "| fps                | 409          |\n",
            "| n_updates          | 14           |\n",
            "| policy_entropy     | 1.6974211    |\n",
            "| policy_loss        | -0.013610095 |\n",
            "| serial_timesteps   | 7168         |\n",
            "| time_elapsed       | 159          |\n",
            "| total_timesteps    | 57344        |\n",
            "| value_loss         | 1.9682062    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.010538231   |\n",
            "| clipfrac           | 0.16276856    |\n",
            "| ep_len_mean        | 86.1          |\n",
            "| ep_reward_mean     | 0.758         |\n",
            "| explained_variance | 0.17          |\n",
            "| fps                | 399           |\n",
            "| n_updates          | 15            |\n",
            "| policy_entropy     | 1.6042407     |\n",
            "| policy_loss        | -0.0147972945 |\n",
            "| serial_timesteps   | 7680          |\n",
            "| time_elapsed       | 169           |\n",
            "| total_timesteps    | 61440         |\n",
            "| value_loss         | 2.5454345     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.012477455 |\n",
            "| clipfrac           | 0.2020996   |\n",
            "| ep_len_mean        | 55.3        |\n",
            "| ep_reward_mean     | 0.846       |\n",
            "| explained_variance | 0.2         |\n",
            "| fps                | 408         |\n",
            "| n_updates          | 16          |\n",
            "| policy_entropy     | 1.4869901   |\n",
            "| policy_loss        | -0.01771925 |\n",
            "| serial_timesteps   | 8192        |\n",
            "| time_elapsed       | 179         |\n",
            "| total_timesteps    | 65536       |\n",
            "| value_loss         | 3.696619    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01471674   |\n",
            "| clipfrac           | 0.2165039    |\n",
            "| ep_len_mean        | 38.6         |\n",
            "| ep_reward_mean     | 0.893        |\n",
            "| explained_variance | 0.22         |\n",
            "| fps                | 403          |\n",
            "| n_updates          | 17           |\n",
            "| policy_entropy     | 1.3443459    |\n",
            "| policy_loss        | -0.016461654 |\n",
            "| serial_timesteps   | 8704         |\n",
            "| time_elapsed       | 189          |\n",
            "| total_timesteps    | 69632        |\n",
            "| value_loss         | 4.3651743    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.013894637  |\n",
            "| clipfrac           | 0.18659668   |\n",
            "| ep_len_mean        | 30.5         |\n",
            "| ep_reward_mean     | 0.915        |\n",
            "| explained_variance | 0.274        |\n",
            "| fps                | 400          |\n",
            "| n_updates          | 18           |\n",
            "| policy_entropy     | 1.1556976    |\n",
            "| policy_loss        | -0.015233481 |\n",
            "| serial_timesteps   | 9216         |\n",
            "| time_elapsed       | 199          |\n",
            "| total_timesteps    | 73728        |\n",
            "| value_loss         | 5.138112     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.021273939 |\n",
            "| clipfrac           | 0.20891114  |\n",
            "| ep_len_mean        | 26          |\n",
            "| ep_reward_mean     | 0.928       |\n",
            "| explained_variance | 0.316       |\n",
            "| fps                | 409         |\n",
            "| n_updates          | 19          |\n",
            "| policy_entropy     | 0.96402675  |\n",
            "| policy_loss        | -0.01843286 |\n",
            "| serial_timesteps   | 9728        |\n",
            "| time_elapsed       | 209         |\n",
            "| total_timesteps    | 77824       |\n",
            "| value_loss         | 5.530483    |\n",
            "------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008413856  |\n",
            "| clipfrac           | 0.12902832   |\n",
            "| ep_len_mean        | 23.8         |\n",
            "| ep_reward_mean     | 0.934        |\n",
            "| explained_variance | 0.32         |\n",
            "| fps                | 391          |\n",
            "| n_updates          | 20           |\n",
            "| policy_entropy     | 0.83179915   |\n",
            "| policy_loss        | -0.013176789 |\n",
            "| serial_timesteps   | 10240        |\n",
            "| time_elapsed       | 219          |\n",
            "| total_timesteps    | 81920        |\n",
            "| value_loss         | 4.915485     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0077354633 |\n",
            "| clipfrac           | 0.0967041    |\n",
            "| ep_len_mean        | 21.1         |\n",
            "| ep_reward_mean     | 0.942        |\n",
            "| explained_variance | 0.396        |\n",
            "| fps                | 398          |\n",
            "| n_updates          | 21           |\n",
            "| policy_entropy     | 0.6832772    |\n",
            "| policy_loss        | -0.011113012 |\n",
            "| serial_timesteps   | 10752        |\n",
            "| time_elapsed       | 230          |\n",
            "| total_timesteps    | 86016        |\n",
            "| value_loss         | 3.889946     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0068055233 |\n",
            "| clipfrac           | 0.08798828   |\n",
            "| ep_len_mean        | 19.6         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.476        |\n",
            "| fps                | 394          |\n",
            "| n_updates          | 22           |\n",
            "| policy_entropy     | 0.56561965   |\n",
            "| policy_loss        | -0.013173844 |\n",
            "| serial_timesteps   | 11264        |\n",
            "| time_elapsed       | 240          |\n",
            "| total_timesteps    | 90112        |\n",
            "| value_loss         | 2.8853583    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010399107  |\n",
            "| clipfrac           | 0.07531738   |\n",
            "| ep_len_mean        | 19.4         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.451        |\n",
            "| fps                | 397          |\n",
            "| n_updates          | 23           |\n",
            "| policy_entropy     | 0.49549112   |\n",
            "| policy_loss        | -0.010769833 |\n",
            "| serial_timesteps   | 11776        |\n",
            "| time_elapsed       | 250          |\n",
            "| total_timesteps    | 94208        |\n",
            "| value_loss         | 1.8059801    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0068715685 |\n",
            "| clipfrac           | 0.06374512   |\n",
            "| ep_len_mean        | 19.4         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.477        |\n",
            "| fps                | 403          |\n",
            "| n_updates          | 24           |\n",
            "| policy_entropy     | 0.44795862   |\n",
            "| policy_loss        | -0.007297373 |\n",
            "| serial_timesteps   | 12288        |\n",
            "| time_elapsed       | 261          |\n",
            "| total_timesteps    | 98304        |\n",
            "| value_loss         | 1.2019571    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0035226867 |\n",
            "| clipfrac           | 0.04902344   |\n",
            "| ep_len_mean        | 18.1         |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.514        |\n",
            "| fps                | 391          |\n",
            "| n_updates          | 25           |\n",
            "| policy_entropy     | 0.3837451    |\n",
            "| policy_loss        | -0.007005612 |\n",
            "| serial_timesteps   | 12800        |\n",
            "| time_elapsed       | 271          |\n",
            "| total_timesteps    | 102400       |\n",
            "| value_loss         | 0.7778451    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.006729161  |\n",
            "| clipfrac           | 0.07727051   |\n",
            "| ep_len_mean        | 18           |\n",
            "| ep_reward_mean     | 0.95         |\n",
            "| explained_variance | 0.527        |\n",
            "| fps                | 399          |\n",
            "| n_updates          | 26           |\n",
            "| policy_entropy     | 0.3672871    |\n",
            "| policy_loss        | -0.015086179 |\n",
            "| serial_timesteps   | 13312        |\n",
            "| time_elapsed       | 281          |\n",
            "| total_timesteps    | 106496       |\n",
            "| value_loss         | 0.44873434   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015077015  |\n",
            "| clipfrac           | 0.026733398   |\n",
            "| ep_len_mean        | 17.3          |\n",
            "| ep_reward_mean     | 0.952         |\n",
            "| explained_variance | 0.479         |\n",
            "| fps                | 389           |\n",
            "| n_updates          | 27            |\n",
            "| policy_entropy     | 0.25252098    |\n",
            "| policy_loss        | -0.0035403147 |\n",
            "| serial_timesteps   | 13824         |\n",
            "| time_elapsed       | 291           |\n",
            "| total_timesteps    | 110592        |\n",
            "| value_loss         | 0.26593786    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.004374235  |\n",
            "| clipfrac           | 0.06262207   |\n",
            "| ep_len_mean        | 17.4         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.363        |\n",
            "| fps                | 391          |\n",
            "| n_updates          | 28           |\n",
            "| policy_entropy     | 0.2352848    |\n",
            "| policy_loss        | -0.010755572 |\n",
            "| serial_timesteps   | 14336        |\n",
            "| time_elapsed       | 302          |\n",
            "| total_timesteps    | 114688       |\n",
            "| value_loss         | 0.11453935   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.015300284  |\n",
            "| clipfrac           | 0.084375     |\n",
            "| ep_len_mean        | 19.4         |\n",
            "| ep_reward_mean     | 0.946        |\n",
            "| explained_variance | 0.274        |\n",
            "| fps                | 400          |\n",
            "| n_updates          | 29           |\n",
            "| policy_entropy     | 0.29771087   |\n",
            "| policy_loss        | -0.016632142 |\n",
            "| serial_timesteps   | 14848        |\n",
            "| time_elapsed       | 312          |\n",
            "| total_timesteps    | 118784       |\n",
            "| value_loss         | 0.07078753   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0034435913 |\n",
            "| clipfrac           | 0.036572266  |\n",
            "| ep_len_mean        | 18.5         |\n",
            "| ep_reward_mean     | 0.949        |\n",
            "| explained_variance | 0.3          |\n",
            "| fps                | 382          |\n",
            "| n_updates          | 30           |\n",
            "| policy_entropy     | 0.31360528   |\n",
            "| policy_loss        | -0.007912556 |\n",
            "| serial_timesteps   | 15360        |\n",
            "| time_elapsed       | 323          |\n",
            "| total_timesteps    | 122880       |\n",
            "| value_loss         | 0.029983688  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0038061545 |\n",
            "| clipfrac           | 0.053833008  |\n",
            "| ep_len_mean        | 17.6         |\n",
            "| ep_reward_mean     | 0.951        |\n",
            "| explained_variance | 0.454        |\n",
            "| fps                | 392          |\n",
            "| n_updates          | 31           |\n",
            "| policy_entropy     | 0.2502879    |\n",
            "| policy_loss        | -0.008925857 |\n",
            "| serial_timesteps   | 15872        |\n",
            "| time_elapsed       | 333          |\n",
            "| total_timesteps    | 126976       |\n",
            "| value_loss         | 0.01946375   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.031877905  |\n",
            "| clipfrac           | 0.068725586  |\n",
            "| ep_len_mean        | 17.3         |\n",
            "| ep_reward_mean     | 0.952        |\n",
            "| explained_variance | 0.698        |\n",
            "| fps                | 387          |\n",
            "| n_updates          | 32           |\n",
            "| policy_entropy     | 0.14989218   |\n",
            "| policy_loss        | -0.009474354 |\n",
            "| serial_timesteps   | 16384        |\n",
            "| time_elapsed       | 344          |\n",
            "| total_timesteps    | 131072       |\n",
            "| value_loss         | 0.012631364  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.006940685  |\n",
            "| clipfrac           | 0.022143554  |\n",
            "| ep_len_mean        | 17           |\n",
            "| ep_reward_mean     | 0.953        |\n",
            "| explained_variance | 0.725        |\n",
            "| fps                | 401          |\n",
            "| n_updates          | 33           |\n",
            "| policy_entropy     | 0.10749002   |\n",
            "| policy_loss        | -0.004775378 |\n",
            "| serial_timesteps   | 16896        |\n",
            "| time_elapsed       | 354          |\n",
            "| total_timesteps    | 135168       |\n",
            "| value_loss         | 0.009738028  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001281573   |\n",
            "| clipfrac           | 0.013891602   |\n",
            "| ep_len_mean        | 16.4          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.758         |\n",
            "| fps                | 400           |\n",
            "| n_updates          | 34            |\n",
            "| policy_entropy     | 0.083283715   |\n",
            "| policy_loss        | -0.0028753926 |\n",
            "| serial_timesteps   | 17408         |\n",
            "| time_elapsed       | 365           |\n",
            "| total_timesteps    | 139264        |\n",
            "| value_loss         | 0.008012602   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00028451753 |\n",
            "| clipfrac           | 0.0051513673  |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.684         |\n",
            "| fps                | 385           |\n",
            "| n_updates          | 35            |\n",
            "| policy_entropy     | 0.06285315    |\n",
            "| policy_loss        | -0.0009283771 |\n",
            "| serial_timesteps   | 17920         |\n",
            "| time_elapsed       | 375           |\n",
            "| total_timesteps    | 143360        |\n",
            "| value_loss         | 0.0076376162  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00064074283 |\n",
            "| clipfrac           | 0.0067382813  |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.729         |\n",
            "| fps                | 392           |\n",
            "| n_updates          | 36            |\n",
            "| policy_entropy     | 0.069100216   |\n",
            "| policy_loss        | -0.0006342217 |\n",
            "| serial_timesteps   | 18432         |\n",
            "| time_elapsed       | 385           |\n",
            "| total_timesteps    | 147456        |\n",
            "| value_loss         | 0.0070558577  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0006304761   |\n",
            "| clipfrac           | 0.0063720704   |\n",
            "| ep_len_mean        | 16.3           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.763          |\n",
            "| fps                | 380            |\n",
            "| n_updates          | 37             |\n",
            "| policy_entropy     | 0.05917483     |\n",
            "| policy_loss        | -0.00092894083 |\n",
            "| serial_timesteps   | 18944          |\n",
            "| time_elapsed       | 396            |\n",
            "| total_timesteps    | 151552         |\n",
            "| value_loss         | 0.0070403414   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0005921788  |\n",
            "| clipfrac           | 0.0076171877  |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.768         |\n",
            "| fps                | 399           |\n",
            "| n_updates          | 38            |\n",
            "| policy_entropy     | 0.05246014    |\n",
            "| policy_loss        | -0.0014418727 |\n",
            "| serial_timesteps   | 19456         |\n",
            "| time_elapsed       | 407           |\n",
            "| total_timesteps    | 155648        |\n",
            "| value_loss         | 0.006656443   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00054638117 |\n",
            "| clipfrac           | 0.0054443358  |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.776         |\n",
            "| fps                | 397           |\n",
            "| n_updates          | 39            |\n",
            "| policy_entropy     | 0.04138705    |\n",
            "| policy_loss        | -0.0013882043 |\n",
            "| serial_timesteps   | 19968         |\n",
            "| time_elapsed       | 417           |\n",
            "| total_timesteps    | 159744        |\n",
            "| value_loss         | 0.006351348   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0006120754  |\n",
            "| clipfrac           | 0.0022216798  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.779         |\n",
            "| fps                | 378           |\n",
            "| n_updates          | 40            |\n",
            "| policy_entropy     | 0.03412956    |\n",
            "| policy_loss        | -0.0002856354 |\n",
            "| serial_timesteps   | 20480         |\n",
            "| time_elapsed       | 427           |\n",
            "| total_timesteps    | 163840        |\n",
            "| value_loss         | 0.00609308    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0001844708  |\n",
            "| clipfrac           | 0.003442383   |\n",
            "| ep_len_mean        | 16.3          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.766         |\n",
            "| fps                | 390           |\n",
            "| n_updates          | 41            |\n",
            "| policy_entropy     | 0.03512975    |\n",
            "| policy_loss        | -0.0008793125 |\n",
            "| serial_timesteps   | 20992         |\n",
            "| time_elapsed       | 438           |\n",
            "| total_timesteps    | 167936        |\n",
            "| value_loss         | 0.0064049875  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00021405422 |\n",
            "| clipfrac           | 0.002709961   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.776         |\n",
            "| fps                | 379           |\n",
            "| n_updates          | 42            |\n",
            "| policy_entropy     | 0.031498805   |\n",
            "| policy_loss        | -0.0005993572 |\n",
            "| serial_timesteps   | 21504         |\n",
            "| time_elapsed       | 449           |\n",
            "| total_timesteps    | 172032        |\n",
            "| value_loss         | 0.006025696   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00039674545  |\n",
            "| clipfrac           | 0.0029296875   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.777          |\n",
            "| fps                | 392            |\n",
            "| n_updates          | 43             |\n",
            "| policy_entropy     | 0.033177566    |\n",
            "| policy_loss        | -0.00058285735 |\n",
            "| serial_timesteps   | 22016          |\n",
            "| time_elapsed       | 459            |\n",
            "| total_timesteps    | 176128         |\n",
            "| value_loss         | 0.0058885855   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0004286807   |\n",
            "| clipfrac           | 0.003857422    |\n",
            "| ep_len_mean        | 16.2           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.778          |\n",
            "| fps                | 382            |\n",
            "| n_updates          | 44             |\n",
            "| policy_entropy     | 0.033913516    |\n",
            "| policy_loss        | -0.00052840094 |\n",
            "| serial_timesteps   | 22528          |\n",
            "| time_elapsed       | 470            |\n",
            "| total_timesteps    | 180224         |\n",
            "| value_loss         | 0.005817135    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00011985951  |\n",
            "| clipfrac           | 0.0020507812   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.78           |\n",
            "| fps                | 392            |\n",
            "| n_updates          | 45             |\n",
            "| policy_entropy     | 0.02890166     |\n",
            "| policy_loss        | -0.00048607882 |\n",
            "| serial_timesteps   | 23040          |\n",
            "| time_elapsed       | 480            |\n",
            "| total_timesteps    | 184320         |\n",
            "| value_loss         | 0.0057655736   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00022248519 |\n",
            "| clipfrac           | 0.002319336   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.778         |\n",
            "| fps                | 394           |\n",
            "| n_updates          | 46            |\n",
            "| policy_entropy     | 0.025527883   |\n",
            "| policy_loss        | -0.0005337036 |\n",
            "| serial_timesteps   | 23552         |\n",
            "| time_elapsed       | 491           |\n",
            "| total_timesteps    | 188416        |\n",
            "| value_loss         | 0.0056603104  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00019864496  |\n",
            "| clipfrac           | 0.0011474609   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 385            |\n",
            "| n_updates          | 47             |\n",
            "| policy_entropy     | 0.02453859     |\n",
            "| policy_loss        | -0.00024013934 |\n",
            "| serial_timesteps   | 24064          |\n",
            "| time_elapsed       | 501            |\n",
            "| total_timesteps    | 192512         |\n",
            "| value_loss         | 0.0055614677   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00015013767 |\n",
            "| clipfrac           | 0.0019042969  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.78          |\n",
            "| fps                | 404           |\n",
            "| n_updates          | 48            |\n",
            "| policy_entropy     | 0.025283683   |\n",
            "| policy_loss        | -0.0002556534 |\n",
            "| serial_timesteps   | 24576         |\n",
            "| time_elapsed       | 512           |\n",
            "| total_timesteps    | 196608        |\n",
            "| value_loss         | 0.005521805   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00027411716 |\n",
            "| clipfrac           | 0.0021972656  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.78          |\n",
            "| fps                | 383           |\n",
            "| n_updates          | 49            |\n",
            "| policy_entropy     | 0.026699591   |\n",
            "| policy_loss        | -6.540292e-05 |\n",
            "| serial_timesteps   | 25088         |\n",
            "| time_elapsed       | 522           |\n",
            "| total_timesteps    | 200704        |\n",
            "| value_loss         | 0.0054703867  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00024784054  |\n",
            "| clipfrac           | 0.0025878907   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.779          |\n",
            "| fps                | 396            |\n",
            "| n_updates          | 50             |\n",
            "| policy_entropy     | 0.02950946     |\n",
            "| policy_loss        | -0.00053124595 |\n",
            "| serial_timesteps   | 25600          |\n",
            "| time_elapsed       | 533            |\n",
            "| total_timesteps    | 204800         |\n",
            "| value_loss         | 0.0054811803   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0001977344   |\n",
            "| clipfrac           | 0.0018310547   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.778          |\n",
            "| fps                | 394            |\n",
            "| n_updates          | 51             |\n",
            "| policy_entropy     | 0.027736584    |\n",
            "| policy_loss        | -0.00032874563 |\n",
            "| serial_timesteps   | 26112          |\n",
            "| time_elapsed       | 543            |\n",
            "| total_timesteps    | 208896         |\n",
            "| value_loss         | 0.005329375    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.004177878   |\n",
            "| clipfrac           | 0.008959961   |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.777         |\n",
            "| fps                | 382           |\n",
            "| n_updates          | 52            |\n",
            "| policy_entropy     | 0.048147127   |\n",
            "| policy_loss        | -0.0016800858 |\n",
            "| serial_timesteps   | 26624         |\n",
            "| time_elapsed       | 553           |\n",
            "| total_timesteps    | 212992        |\n",
            "| value_loss         | 0.005202072   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014902417  |\n",
            "| clipfrac           | 0.013208007   |\n",
            "| ep_len_mean        | 16.4          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.766         |\n",
            "| fps                | 401           |\n",
            "| n_updates          | 53            |\n",
            "| policy_entropy     | 0.089996226   |\n",
            "| policy_loss        | 3.4285844e-05 |\n",
            "| serial_timesteps   | 27136         |\n",
            "| time_elapsed       | 564           |\n",
            "| total_timesteps    | 217088        |\n",
            "| value_loss         | 0.0057231053  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007436146  |\n",
            "| clipfrac           | 0.010253906   |\n",
            "| ep_len_mean        | 16.4          |\n",
            "| ep_reward_mean     | 0.954         |\n",
            "| explained_variance | 0.769         |\n",
            "| fps                | 390           |\n",
            "| n_updates          | 54            |\n",
            "| policy_entropy     | 0.080585435   |\n",
            "| policy_loss        | -0.0008429163 |\n",
            "| serial_timesteps   | 27648         |\n",
            "| time_elapsed       | 574           |\n",
            "| total_timesteps    | 221184        |\n",
            "| value_loss         | 0.005682693   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.29372638   |\n",
            "| clipfrac           | 0.07763672   |\n",
            "| ep_len_mean        | 19.3         |\n",
            "| ep_reward_mean     | 0.945        |\n",
            "| explained_variance | 0.735        |\n",
            "| fps                | 402          |\n",
            "| n_updates          | 55           |\n",
            "| policy_entropy     | 0.09123742   |\n",
            "| policy_loss        | -0.036181483 |\n",
            "| serial_timesteps   | 28160        |\n",
            "| time_elapsed       | 585          |\n",
            "| total_timesteps    | 225280       |\n",
            "| value_loss         | 0.02967971   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007666984  |\n",
            "| clipfrac           | 0.00834961    |\n",
            "| ep_len_mean        | 16.3          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.775         |\n",
            "| fps                | 400           |\n",
            "| n_updates          | 56            |\n",
            "| policy_entropy     | 0.048584513   |\n",
            "| policy_loss        | -0.0011619176 |\n",
            "| serial_timesteps   | 28672         |\n",
            "| time_elapsed       | 595           |\n",
            "| total_timesteps    | 229376        |\n",
            "| value_loss         | 0.0055384603  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0008105403   |\n",
            "| clipfrac           | 0.008178711    |\n",
            "| ep_len_mean        | 16.2           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.774          |\n",
            "| fps                | 380            |\n",
            "| n_updates          | 57             |\n",
            "| policy_entropy     | 0.047607314    |\n",
            "| policy_loss        | -0.00057435594 |\n",
            "| serial_timesteps   | 29184          |\n",
            "| time_elapsed       | 605            |\n",
            "| total_timesteps    | 233472         |\n",
            "| value_loss         | 0.0053876406   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0006365228   |\n",
            "| clipfrac           | 0.0061035156   |\n",
            "| ep_len_mean        | 16.2           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.777          |\n",
            "| fps                | 387            |\n",
            "| n_updates          | 58             |\n",
            "| policy_entropy     | 0.03914695     |\n",
            "| policy_loss        | -0.00076216774 |\n",
            "| serial_timesteps   | 29696          |\n",
            "| time_elapsed       | 616            |\n",
            "| total_timesteps    | 237568         |\n",
            "| value_loss         | 0.0053010895   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00038118346 |\n",
            "| clipfrac           | 0.0046875     |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.778         |\n",
            "| fps                | 379           |\n",
            "| n_updates          | 59            |\n",
            "| policy_entropy     | 0.035877474   |\n",
            "| policy_loss        | -0.0005365708 |\n",
            "| serial_timesteps   | 30208         |\n",
            "| time_elapsed       | 627           |\n",
            "| total_timesteps    | 241664        |\n",
            "| value_loss         | 0.0051933867  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00029085815 |\n",
            "| clipfrac           | 0.0021972656  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.779         |\n",
            "| fps                | 393           |\n",
            "| n_updates          | 60            |\n",
            "| policy_entropy     | 0.03501365    |\n",
            "| policy_loss        | 3.473925e-05  |\n",
            "| serial_timesteps   | 30720         |\n",
            "| time_elapsed       | 637           |\n",
            "| total_timesteps    | 245760        |\n",
            "| value_loss         | 0.0049945833  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00030355048 |\n",
            "| clipfrac           | 0.0044677733  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.778         |\n",
            "| fps                | 386           |\n",
            "| n_updates          | 61            |\n",
            "| policy_entropy     | 0.030941108   |\n",
            "| policy_loss        | -0.000765717  |\n",
            "| serial_timesteps   | 31232         |\n",
            "| time_elapsed       | 648           |\n",
            "| total_timesteps    | 249856        |\n",
            "| value_loss         | 0.0050274157  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00027380596 |\n",
            "| clipfrac           | 0.0037109375  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.777         |\n",
            "| fps                | 392           |\n",
            "| n_updates          | 62            |\n",
            "| policy_entropy     | 0.026578328   |\n",
            "| policy_loss        | -0.000525337  |\n",
            "| serial_timesteps   | 31744         |\n",
            "| time_elapsed       | 658           |\n",
            "| total_timesteps    | 253952        |\n",
            "| value_loss         | 0.0050165574  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003163445  |\n",
            "| clipfrac           | 0.0020996095  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.781         |\n",
            "| fps                | 396           |\n",
            "| n_updates          | 63            |\n",
            "| policy_entropy     | 0.023108322   |\n",
            "| policy_loss        | -0.0005630593 |\n",
            "| serial_timesteps   | 32256         |\n",
            "| time_elapsed       | 669           |\n",
            "| total_timesteps    | 258048        |\n",
            "| value_loss         | 0.0048801256  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00020622122  |\n",
            "| clipfrac           | 0.0021972656   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 373            |\n",
            "| n_updates          | 64             |\n",
            "| policy_entropy     | 0.018881777    |\n",
            "| policy_loss        | -0.00027084208 |\n",
            "| serial_timesteps   | 32768          |\n",
            "| time_elapsed       | 679            |\n",
            "| total_timesteps    | 262144         |\n",
            "| value_loss         | 0.0049268673   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00027713057  |\n",
            "| clipfrac           | 0.0028564453   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.78           |\n",
            "| fps                | 401            |\n",
            "| n_updates          | 65             |\n",
            "| policy_entropy     | 0.016512845    |\n",
            "| policy_loss        | -0.00060609746 |\n",
            "| serial_timesteps   | 33280          |\n",
            "| time_elapsed       | 690            |\n",
            "| total_timesteps    | 266240         |\n",
            "| value_loss         | 0.0049716076   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00013723796  |\n",
            "| clipfrac           | 0.0014160157   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 375            |\n",
            "| n_updates          | 66             |\n",
            "| policy_entropy     | 0.016829688    |\n",
            "| policy_loss        | -0.00032647204 |\n",
            "| serial_timesteps   | 33792          |\n",
            "| time_elapsed       | 700            |\n",
            "| total_timesteps    | 270336         |\n",
            "| value_loss         | 0.0048880135   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.667483e-05  |\n",
            "| clipfrac           | 0.00095214846 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 394           |\n",
            "| n_updates          | 67            |\n",
            "| policy_entropy     | 0.016337316   |\n",
            "| policy_loss        | -2.531634e-05 |\n",
            "| serial_timesteps   | 34304         |\n",
            "| time_elapsed       | 711           |\n",
            "| total_timesteps    | 274432        |\n",
            "| value_loss         | 0.0048400727  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.680811e-05   |\n",
            "| clipfrac           | 0.0011962891   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 388            |\n",
            "| n_updates          | 68             |\n",
            "| policy_entropy     | 0.013338029    |\n",
            "| policy_loss        | -0.00035639014 |\n",
            "| serial_timesteps   | 34816          |\n",
            "| time_elapsed       | 722            |\n",
            "| total_timesteps    | 278528         |\n",
            "| value_loss         | 0.004718113    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 7.416757e-05   |\n",
            "| clipfrac           | 0.0010009765   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 384            |\n",
            "| n_updates          | 69             |\n",
            "| policy_entropy     | 0.016630374    |\n",
            "| policy_loss        | -0.00024158104 |\n",
            "| serial_timesteps   | 35328          |\n",
            "| time_elapsed       | 732            |\n",
            "| total_timesteps    | 282624         |\n",
            "| value_loss         | 0.004723491    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00045650863 |\n",
            "| clipfrac           | 0.0017089844  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.78          |\n",
            "| fps                | 385           |\n",
            "| n_updates          | 70            |\n",
            "| policy_entropy     | 0.01730865    |\n",
            "| policy_loss        | 0.00010396342 |\n",
            "| serial_timesteps   | 35840         |\n",
            "| time_elapsed       | 743           |\n",
            "| total_timesteps    | 286720        |\n",
            "| value_loss         | 0.004685518   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00019029486 |\n",
            "| clipfrac           | 0.0011962891  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 376           |\n",
            "| n_updates          | 71            |\n",
            "| policy_entropy     | 0.019630898   |\n",
            "| policy_loss        | 6.267885e-05  |\n",
            "| serial_timesteps   | 36352         |\n",
            "| time_elapsed       | 753           |\n",
            "| total_timesteps    | 290816        |\n",
            "| value_loss         | 0.00460836    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00027700182  |\n",
            "| clipfrac           | 0.0025146485   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 391            |\n",
            "| n_updates          | 72             |\n",
            "| policy_entropy     | 0.019151894    |\n",
            "| policy_loss        | -0.00037542713 |\n",
            "| serial_timesteps   | 36864          |\n",
            "| time_elapsed       | 764            |\n",
            "| total_timesteps    | 294912         |\n",
            "| value_loss         | 0.0047733346   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00028412588  |\n",
            "| clipfrac           | 0.0025634766   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.78           |\n",
            "| fps                | 392            |\n",
            "| n_updates          | 73             |\n",
            "| policy_entropy     | 0.019598994    |\n",
            "| policy_loss        | -0.00030812772 |\n",
            "| serial_timesteps   | 37376          |\n",
            "| time_elapsed       | 775            |\n",
            "| total_timesteps    | 299008         |\n",
            "| value_loss         | 0.004788483    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00021706198 |\n",
            "| clipfrac           | 0.0025878907  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.781         |\n",
            "| fps                | 377           |\n",
            "| n_updates          | 74            |\n",
            "| policy_entropy     | 0.016892876   |\n",
            "| policy_loss        | -0.0008102084 |\n",
            "| serial_timesteps   | 37888         |\n",
            "| time_elapsed       | 785           |\n",
            "| total_timesteps    | 303104        |\n",
            "| value_loss         | 0.004567691   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0002474119  |\n",
            "| clipfrac           | 0.0020996095  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.781         |\n",
            "| fps                | 391           |\n",
            "| n_updates          | 75            |\n",
            "| policy_entropy     | 0.012696627   |\n",
            "| policy_loss        | -0.0002880212 |\n",
            "| serial_timesteps   | 38400         |\n",
            "| time_elapsed       | 796           |\n",
            "| total_timesteps    | 307200        |\n",
            "| value_loss         | 0.0046122507  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 6.750236e-05  |\n",
            "| clipfrac           | 0.0013183594  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 384           |\n",
            "| n_updates          | 76            |\n",
            "| policy_entropy     | 0.0119198     |\n",
            "| policy_loss        | -8.479406e-05 |\n",
            "| serial_timesteps   | 38912         |\n",
            "| time_elapsed       | 807           |\n",
            "| total_timesteps    | 311296        |\n",
            "| value_loss         | 0.0045540025  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00018506774  |\n",
            "| clipfrac           | 0.0018066406   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 399            |\n",
            "| n_updates          | 77             |\n",
            "| policy_entropy     | 0.010259878    |\n",
            "| policy_loss        | -0.00028885077 |\n",
            "| serial_timesteps   | 39424          |\n",
            "| time_elapsed       | 817            |\n",
            "| total_timesteps    | 315392         |\n",
            "| value_loss         | 0.004628632    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00011112679 |\n",
            "| clipfrac           | 0.0008789062  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 394           |\n",
            "| n_updates          | 78            |\n",
            "| policy_entropy     | 0.009374832   |\n",
            "| policy_loss        | -0.0003794055 |\n",
            "| serial_timesteps   | 39936         |\n",
            "| time_elapsed       | 827           |\n",
            "| total_timesteps    | 319488        |\n",
            "| value_loss         | 0.0045237644  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 3.3363878e-05  |\n",
            "| clipfrac           | 0.0005615234   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 386            |\n",
            "| n_updates          | 79             |\n",
            "| policy_entropy     | 0.00925971     |\n",
            "| policy_loss        | -0.00015187843 |\n",
            "| serial_timesteps   | 40448          |\n",
            "| time_elapsed       | 838            |\n",
            "| total_timesteps    | 323584         |\n",
            "| value_loss         | 0.0044600917   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00014702506  |\n",
            "| clipfrac           | 0.00080566405  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 399            |\n",
            "| n_updates          | 80             |\n",
            "| policy_entropy     | 0.013557787    |\n",
            "| policy_loss        | -0.00029512096 |\n",
            "| serial_timesteps   | 40960          |\n",
            "| time_elapsed       | 848            |\n",
            "| total_timesteps    | 327680         |\n",
            "| value_loss         | 0.0045134015   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0036473311   |\n",
            "| clipfrac           | 0.0064208983   |\n",
            "| ep_len_mean        | 16.2           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.773          |\n",
            "| fps                | 381            |\n",
            "| n_updates          | 81             |\n",
            "| policy_entropy     | 0.021202799    |\n",
            "| policy_loss        | -0.00021416496 |\n",
            "| serial_timesteps   | 41472          |\n",
            "| time_elapsed       | 859            |\n",
            "| total_timesteps    | 331776         |\n",
            "| value_loss         | 0.0047918702   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0031230643  |\n",
            "| clipfrac           | 0.0044921874  |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.776         |\n",
            "| fps                | 393           |\n",
            "| n_updates          | 82            |\n",
            "| policy_entropy     | 0.013976078   |\n",
            "| policy_loss        | -0.0005888484 |\n",
            "| serial_timesteps   | 41984         |\n",
            "| time_elapsed       | 869           |\n",
            "| total_timesteps    | 335872        |\n",
            "| value_loss         | 0.0047616777  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00038938885 |\n",
            "| clipfrac           | 0.0015136718  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.78          |\n",
            "| fps                | 395           |\n",
            "| n_updates          | 83            |\n",
            "| policy_entropy     | 0.015083162   |\n",
            "| policy_loss        | 2.3329503e-05 |\n",
            "| serial_timesteps   | 42496         |\n",
            "| time_elapsed       | 880           |\n",
            "| total_timesteps    | 339968        |\n",
            "| value_loss         | 0.004560402   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00035887538 |\n",
            "| clipfrac           | 0.0025146485  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.78          |\n",
            "| fps                | 379           |\n",
            "| n_updates          | 84            |\n",
            "| policy_entropy     | 0.014784738   |\n",
            "| policy_loss        | -0.000447799  |\n",
            "| serial_timesteps   | 43008         |\n",
            "| time_elapsed       | 890           |\n",
            "| total_timesteps    | 344064        |\n",
            "| value_loss         | 0.004590557   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0006092351   |\n",
            "| clipfrac           | 0.00092773436  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 397            |\n",
            "| n_updates          | 85             |\n",
            "| policy_entropy     | 0.008369421    |\n",
            "| policy_loss        | -1.9671112e-05 |\n",
            "| serial_timesteps   | 43520          |\n",
            "| time_elapsed       | 901            |\n",
            "| total_timesteps    | 348160         |\n",
            "| value_loss         | 0.00444734     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00017279372 |\n",
            "| clipfrac           | 0.0011962891  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 382           |\n",
            "| n_updates          | 86            |\n",
            "| policy_entropy     | 0.010315934   |\n",
            "| policy_loss        | -0.0003428645 |\n",
            "| serial_timesteps   | 44032         |\n",
            "| time_elapsed       | 911           |\n",
            "| total_timesteps    | 352256        |\n",
            "| value_loss         | 0.0044320547  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00013279595 |\n",
            "| clipfrac           | 0.0008789062  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 392           |\n",
            "| n_updates          | 87            |\n",
            "| policy_entropy     | 0.010686355   |\n",
            "| policy_loss        | 5.6829605e-05 |\n",
            "| serial_timesteps   | 44544         |\n",
            "| time_elapsed       | 922           |\n",
            "| total_timesteps    | 356352        |\n",
            "| value_loss         | 0.0044478057  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 7.841028e-05 |\n",
            "| clipfrac           | 0.0005371094 |\n",
            "| ep_len_mean        | 16           |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.784        |\n",
            "| fps                | 388          |\n",
            "| n_updates          | 88           |\n",
            "| policy_entropy     | 0.00838307   |\n",
            "| policy_loss        | -8.76503e-05 |\n",
            "| serial_timesteps   | 45056        |\n",
            "| time_elapsed       | 932          |\n",
            "| total_timesteps    | 360448       |\n",
            "| value_loss         | 0.0045015803 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003702236  |\n",
            "| clipfrac           | 0.0017333984  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 396           |\n",
            "| n_updates          | 89            |\n",
            "| policy_entropy     | 0.005501707   |\n",
            "| policy_loss        | -0.0007050932 |\n",
            "| serial_timesteps   | 45568         |\n",
            "| time_elapsed       | 943           |\n",
            "| total_timesteps    | 364544        |\n",
            "| value_loss         | 0.004487629   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.953068e-05   |\n",
            "| clipfrac           | 0.0010498047   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 391            |\n",
            "| n_updates          | 90             |\n",
            "| policy_entropy     | 0.0046360656   |\n",
            "| policy_loss        | -0.00015400321 |\n",
            "| serial_timesteps   | 46080          |\n",
            "| time_elapsed       | 953            |\n",
            "| total_timesteps    | 368640         |\n",
            "| value_loss         | 0.0044308463   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 5.550308e-05   |\n",
            "| clipfrac           | 0.00036621094  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 378            |\n",
            "| n_updates          | 91             |\n",
            "| policy_entropy     | 0.0051362263   |\n",
            "| policy_loss        | -0.00012579307 |\n",
            "| serial_timesteps   | 46592          |\n",
            "| time_elapsed       | 964            |\n",
            "| total_timesteps    | 372736         |\n",
            "| value_loss         | 0.004381337    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0004943233   |\n",
            "| clipfrac           | 0.00092773436  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 400            |\n",
            "| n_updates          | 92             |\n",
            "| policy_entropy     | 0.00441165     |\n",
            "| policy_loss        | -0.00033568047 |\n",
            "| serial_timesteps   | 47104          |\n",
            "| time_elapsed       | 975            |\n",
            "| total_timesteps    | 376832         |\n",
            "| value_loss         | 0.00442483     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 2.605434e-05  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 386           |\n",
            "| n_updates          | 93            |\n",
            "| policy_entropy     | 0.0043323427  |\n",
            "| policy_loss        | -8.663843e-05 |\n",
            "| serial_timesteps   | 47616         |\n",
            "| time_elapsed       | 985           |\n",
            "| total_timesteps    | 380928        |\n",
            "| value_loss         | 0.004362977   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.662083e-05   |\n",
            "| clipfrac           | 0.0001953125   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 395            |\n",
            "| n_updates          | 94             |\n",
            "| policy_entropy     | 0.004810674    |\n",
            "| policy_loss        | -1.1860835e-05 |\n",
            "| serial_timesteps   | 48128          |\n",
            "| time_elapsed       | 995            |\n",
            "| total_timesteps    | 385024         |\n",
            "| value_loss         | 0.0043234425   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0011902467   |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 391            |\n",
            "| n_updates          | 95             |\n",
            "| policy_entropy     | 0.0046520755   |\n",
            "| policy_loss        | -0.00014797055 |\n",
            "| serial_timesteps   | 48640          |\n",
            "| time_elapsed       | 1.01e+03       |\n",
            "| total_timesteps    | 389120         |\n",
            "| value_loss         | 0.0043775323   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00033521312 |\n",
            "| clipfrac           | 0.0006591797  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 387           |\n",
            "| n_updates          | 96            |\n",
            "| policy_entropy     | 0.0037533753  |\n",
            "| policy_loss        | 8.3298146e-05 |\n",
            "| serial_timesteps   | 49152         |\n",
            "| time_elapsed       | 1.02e+03      |\n",
            "| total_timesteps    | 393216        |\n",
            "| value_loss         | 0.004360023   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00048305653  |\n",
            "| clipfrac           | 0.00070800784  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 399            |\n",
            "| n_updates          | 97             |\n",
            "| policy_entropy     | 0.0035098933   |\n",
            "| policy_loss        | -0.00023059167 |\n",
            "| serial_timesteps   | 49664          |\n",
            "| time_elapsed       | 1.03e+03       |\n",
            "| total_timesteps    | 397312         |\n",
            "| value_loss         | 0.00433234     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 7.9469056e-11 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 374           |\n",
            "| n_updates          | 98            |\n",
            "| policy_entropy     | 0.0031768451  |\n",
            "| policy_loss        | 4.7165667e-08 |\n",
            "| serial_timesteps   | 50176         |\n",
            "| time_elapsed       | 1.04e+03      |\n",
            "| total_timesteps    | 401408        |\n",
            "| value_loss         | 0.004354913   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 9.278111e-11  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 403           |\n",
            "| n_updates          | 99            |\n",
            "| policy_entropy     | 0.0031963326  |\n",
            "| policy_loss        | 1.3284007e-07 |\n",
            "| serial_timesteps   | 50688         |\n",
            "| time_elapsed       | 1.05e+03      |\n",
            "| total_timesteps    | 405504        |\n",
            "| value_loss         | 0.0043222355  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00071060716  |\n",
            "| clipfrac           | 0.00068359374  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 392            |\n",
            "| n_updates          | 100            |\n",
            "| policy_entropy     | 0.0041809715   |\n",
            "| policy_loss        | -0.00021850041 |\n",
            "| serial_timesteps   | 51200          |\n",
            "| time_elapsed       | 1.06e+03       |\n",
            "| total_timesteps    | 409600         |\n",
            "| value_loss         | 0.0043652505   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00033384791  |\n",
            "| clipfrac           | 0.00046386718  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 388            |\n",
            "| n_updates          | 101            |\n",
            "| policy_entropy     | 0.0034872298   |\n",
            "| policy_loss        | -0.00019111735 |\n",
            "| serial_timesteps   | 51712          |\n",
            "| time_elapsed       | 1.07e+03       |\n",
            "| total_timesteps    | 413696         |\n",
            "| value_loss         | 0.0042937193   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00022025415  |\n",
            "| clipfrac           | 0.00046386718  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 400            |\n",
            "| n_updates          | 102            |\n",
            "| policy_entropy     | 0.0028232487   |\n",
            "| policy_loss        | -0.00017873004 |\n",
            "| serial_timesteps   | 52224          |\n",
            "| time_elapsed       | 1.08e+03       |\n",
            "| total_timesteps    | 417792         |\n",
            "| value_loss         | 0.0041467114   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 9.052304e-05  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 385           |\n",
            "| n_updates          | 103           |\n",
            "| policy_entropy     | 0.002423218   |\n",
            "| policy_loss        | -9.898752e-05 |\n",
            "| serial_timesteps   | 52736         |\n",
            "| time_elapsed       | 1.09e+03      |\n",
            "| total_timesteps    | 421888        |\n",
            "| value_loss         | 0.004123803   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0027862187  |\n",
            "| clipfrac           | 0.00046386718 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 392           |\n",
            "| n_updates          | 104           |\n",
            "| policy_entropy     | 0.0042703     |\n",
            "| policy_loss        | 1.0742349e-05 |\n",
            "| serial_timesteps   | 53248         |\n",
            "| time_elapsed       | 1.1e+03       |\n",
            "| total_timesteps    | 425984        |\n",
            "| value_loss         | 0.0042075426  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0010196025   |\n",
            "| clipfrac           | 0.00046386718  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 380            |\n",
            "| n_updates          | 105            |\n",
            "| policy_entropy     | 0.0016382523   |\n",
            "| policy_loss        | -0.00022025514 |\n",
            "| serial_timesteps   | 53760          |\n",
            "| time_elapsed       | 1.11e+03       |\n",
            "| total_timesteps    | 430080         |\n",
            "| value_loss         | 0.0042617833   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.9587006e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 402            |\n",
            "| n_updates          | 106            |\n",
            "| policy_entropy     | 0.0015159348   |\n",
            "| policy_loss        | -2.6019407e-07 |\n",
            "| serial_timesteps   | 54272          |\n",
            "| time_elapsed       | 1.12e+03       |\n",
            "| total_timesteps    | 434176         |\n",
            "| value_loss         | 0.004224427    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| approxkl           | 8.297608e-11    |\n",
            "| clipfrac           | 0.0             |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.956           |\n",
            "| explained_variance | 0.784           |\n",
            "| fps                | 394             |\n",
            "| n_updates          | 107             |\n",
            "| policy_entropy     | 0.0014889783    |\n",
            "| policy_loss        | -1.04620995e-07 |\n",
            "| serial_timesteps   | 54784           |\n",
            "| time_elapsed       | 1.13e+03        |\n",
            "| total_timesteps    | 438272          |\n",
            "| value_loss         | 0.0041726353    |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 4.1450593e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 382            |\n",
            "| n_updates          | 108            |\n",
            "| policy_entropy     | 0.0015136151   |\n",
            "| policy_loss        | -2.4148903e-08 |\n",
            "| serial_timesteps   | 55296          |\n",
            "| time_elapsed       | 1.14e+03       |\n",
            "| total_timesteps    | 442368         |\n",
            "| value_loss         | 0.0041833045   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.0349807e-10  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 397            |\n",
            "| n_updates          | 109            |\n",
            "| policy_entropy     | 0.0014911776   |\n",
            "| policy_loss        | -2.3347965e-07 |\n",
            "| serial_timesteps   | 55808          |\n",
            "| time_elapsed       | 1.15e+03       |\n",
            "| total_timesteps    | 446464         |\n",
            "| value_loss         | 0.0041888654   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00055175606  |\n",
            "| clipfrac           | 0.0012207031   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 387            |\n",
            "| n_updates          | 110            |\n",
            "| policy_entropy     | 0.00557334     |\n",
            "| policy_loss        | -0.00014155102 |\n",
            "| serial_timesteps   | 56320          |\n",
            "| time_elapsed       | 1.16e+03       |\n",
            "| total_timesteps    | 450560         |\n",
            "| value_loss         | 0.004214621    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.4274589e-09 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 397           |\n",
            "| n_updates          | 111           |\n",
            "| policy_entropy     | 0.001956187   |\n",
            "| policy_loss        | -9.369076e-07 |\n",
            "| serial_timesteps   | 56832         |\n",
            "| time_elapsed       | 1.17e+03      |\n",
            "| total_timesteps    | 454656        |\n",
            "| value_loss         | 0.0041810996  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 8.991123e-05  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 393           |\n",
            "| n_updates          | 112           |\n",
            "| policy_entropy     | 0.0025321525  |\n",
            "| policy_loss        | -9.91369e-05  |\n",
            "| serial_timesteps   | 57344         |\n",
            "| time_elapsed       | 1.18e+03      |\n",
            "| total_timesteps    | 458752        |\n",
            "| value_loss         | 0.0041397903  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 9.561316e-05  |\n",
            "| clipfrac           | 0.0004394531  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 388           |\n",
            "| n_updates          | 113           |\n",
            "| policy_entropy     | 0.0043441732  |\n",
            "| policy_loss        | -5.831654e-05 |\n",
            "| serial_timesteps   | 57856         |\n",
            "| time_elapsed       | 1.19e+03      |\n",
            "| total_timesteps    | 462848        |\n",
            "| value_loss         | 0.0041479077  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 5.4438726e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 395            |\n",
            "| n_updates          | 114            |\n",
            "| policy_entropy     | 0.005317615    |\n",
            "| policy_loss        | -1.0780614e-05 |\n",
            "| serial_timesteps   | 58368          |\n",
            "| time_elapsed       | 1.2e+03        |\n",
            "| total_timesteps    | 466944         |\n",
            "| value_loss         | 0.00414194     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.008152678    |\n",
            "| clipfrac           | 0.0019042969   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 383            |\n",
            "| n_updates          | 115            |\n",
            "| policy_entropy     | 0.0016873127   |\n",
            "| policy_loss        | -0.00029598552 |\n",
            "| serial_timesteps   | 58880          |\n",
            "| time_elapsed       | 1.21e+03       |\n",
            "| total_timesteps    | 471040         |\n",
            "| value_loss         | 0.0042808834   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 4.6088255e-11  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 399            |\n",
            "| n_updates          | 116            |\n",
            "| policy_entropy     | 0.0016168684   |\n",
            "| policy_loss        | -2.6205817e-07 |\n",
            "| serial_timesteps   | 59392          |\n",
            "| time_elapsed       | 1.23e+03       |\n",
            "| total_timesteps    | 475136         |\n",
            "| value_loss         | 0.004207001    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 7.399732e-05  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 393           |\n",
            "| n_updates          | 117           |\n",
            "| policy_entropy     | 0.0019016487  |\n",
            "| policy_loss        | -1.596038e-05 |\n",
            "| serial_timesteps   | 59904         |\n",
            "| time_elapsed       | 1.24e+03      |\n",
            "| total_timesteps    | 479232        |\n",
            "| value_loss         | 0.0042740754  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 3.845578e-11 |\n",
            "| clipfrac           | 0.0          |\n",
            "| ep_len_mean        | 16           |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.784        |\n",
            "| fps                | 373          |\n",
            "| n_updates          | 118          |\n",
            "| policy_entropy     | 0.0019716432 |\n",
            "| policy_loss        | -9.57516e-09 |\n",
            "| serial_timesteps   | 60416        |\n",
            "| time_elapsed       | 1.25e+03     |\n",
            "| total_timesteps    | 483328       |\n",
            "| value_loss         | 0.0041558    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016338794  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 380           |\n",
            "| n_updates          | 119           |\n",
            "| policy_entropy     | 0.0024329838  |\n",
            "| policy_loss        | -9.96989e-05  |\n",
            "| serial_timesteps   | 60928         |\n",
            "| time_elapsed       | 1.26e+03      |\n",
            "| total_timesteps    | 487424        |\n",
            "| value_loss         | 0.004144267   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003247171  |\n",
            "| clipfrac           | 0.00092773436 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 376           |\n",
            "| n_updates          | 120           |\n",
            "| policy_entropy     | 0.010877561   |\n",
            "| policy_loss        | -0.0001206888 |\n",
            "| serial_timesteps   | 61440         |\n",
            "| time_elapsed       | 1.27e+03      |\n",
            "| total_timesteps    | 491520        |\n",
            "| value_loss         | 0.004104446   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007938033  |\n",
            "| clipfrac           | 0.002416992   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 398           |\n",
            "| n_updates          | 121           |\n",
            "| policy_entropy     | 0.010114253   |\n",
            "| policy_loss        | 0.00052656996 |\n",
            "| serial_timesteps   | 61952         |\n",
            "| time_elapsed       | 1.28e+03      |\n",
            "| total_timesteps    | 495616        |\n",
            "| value_loss         | 0.004196954   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016645466  |\n",
            "| clipfrac           | 0.0032958984  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 395           |\n",
            "| n_updates          | 122           |\n",
            "| policy_entropy     | 0.010360742   |\n",
            "| policy_loss        | 0.00064349326 |\n",
            "| serial_timesteps   | 62464         |\n",
            "| time_elapsed       | 1.29e+03      |\n",
            "| total_timesteps    | 499712        |\n",
            "| value_loss         | 0.0041922247  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00019788272 |\n",
            "| clipfrac           | 0.0019775392  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 381           |\n",
            "| n_updates          | 123           |\n",
            "| policy_entropy     | 0.013098054   |\n",
            "| policy_loss        | 0.00035127316 |\n",
            "| serial_timesteps   | 62976         |\n",
            "| time_elapsed       | 1.3e+03       |\n",
            "| total_timesteps    | 503808        |\n",
            "| value_loss         | 0.0041722124  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00021979019  |\n",
            "| clipfrac           | 0.00234375     |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 388            |\n",
            "| n_updates          | 124            |\n",
            "| policy_entropy     | 0.00677189     |\n",
            "| policy_loss        | -0.00031125062 |\n",
            "| serial_timesteps   | 63488          |\n",
            "| time_elapsed       | 1.31e+03       |\n",
            "| total_timesteps    | 507904         |\n",
            "| value_loss         | 0.00418501     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 4.1736985e-05  |\n",
            "| clipfrac           | 0.00061035156  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 365            |\n",
            "| n_updates          | 125            |\n",
            "| policy_entropy     | 0.005610681    |\n",
            "| policy_loss        | -0.00017766313 |\n",
            "| serial_timesteps   | 64000          |\n",
            "| time_elapsed       | 1.32e+03       |\n",
            "| total_timesteps    | 512000         |\n",
            "| value_loss         | 0.004183877    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 6.553674e-05  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 382           |\n",
            "| n_updates          | 126           |\n",
            "| policy_entropy     | 0.006244992   |\n",
            "| policy_loss        | -8.456366e-05 |\n",
            "| serial_timesteps   | 64512         |\n",
            "| time_elapsed       | 1.33e+03      |\n",
            "| total_timesteps    | 516096        |\n",
            "| value_loss         | 0.0041582827  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00017884346 |\n",
            "| clipfrac           | 0.00092773436 |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 383           |\n",
            "| n_updates          | 127           |\n",
            "| policy_entropy     | 0.0050357627  |\n",
            "| policy_loss        | -7.023073e-05 |\n",
            "| serial_timesteps   | 65024         |\n",
            "| time_elapsed       | 1.34e+03      |\n",
            "| total_timesteps    | 520192        |\n",
            "| value_loss         | 0.0041728285  |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 8.597893e-06 |\n",
            "| clipfrac           | 0.0001953125 |\n",
            "| ep_len_mean        | 16           |\n",
            "| ep_reward_mean     | 0.956        |\n",
            "| explained_variance | 0.783        |\n",
            "| fps                | 394          |\n",
            "| n_updates          | 128          |\n",
            "| policy_entropy     | 0.0052911527 |\n",
            "| policy_loss        | -4.30496e-05 |\n",
            "| serial_timesteps   | 65536        |\n",
            "| time_elapsed       | 1.35e+03     |\n",
            "| total_timesteps    | 524288       |\n",
            "| value_loss         | 0.0041588983 |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00017153737  |\n",
            "| clipfrac           | 0.0006347656   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 389            |\n",
            "| n_updates          | 129            |\n",
            "| policy_entropy     | 0.0039111176   |\n",
            "| policy_loss        | -5.8308244e-05 |\n",
            "| serial_timesteps   | 66048          |\n",
            "| time_elapsed       | 1.36e+03       |\n",
            "| total_timesteps    | 528384         |\n",
            "| value_loss         | 0.004157176    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 9.316012e-05   |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 383            |\n",
            "| n_updates          | 130            |\n",
            "| policy_entropy     | 0.0057859607   |\n",
            "| policy_loss        | -0.00010258144 |\n",
            "| serial_timesteps   | 66560          |\n",
            "| time_elapsed       | 1.37e+03       |\n",
            "| total_timesteps    | 532480         |\n",
            "| value_loss         | 0.0040667644   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.1451191e-05 |\n",
            "| clipfrac           | 0.00017089843 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 385           |\n",
            "| n_updates          | 131           |\n",
            "| policy_entropy     | 0.008293127   |\n",
            "| policy_loss        | -2.125049e-05 |\n",
            "| serial_timesteps   | 67072         |\n",
            "| time_elapsed       | 1.38e+03      |\n",
            "| total_timesteps    | 536576        |\n",
            "| value_loss         | 0.0041026855  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0002424973   |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 378            |\n",
            "| n_updates          | 132            |\n",
            "| policy_entropy     | 0.013428668    |\n",
            "| policy_loss        | -0.00012912782 |\n",
            "| serial_timesteps   | 67584          |\n",
            "| time_elapsed       | 1.4e+03        |\n",
            "| total_timesteps    | 540672         |\n",
            "| value_loss         | 0.004072135    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011123948  |\n",
            "| clipfrac           | 0.0032226562  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 399           |\n",
            "| n_updates          | 133           |\n",
            "| policy_entropy     | 0.031388838   |\n",
            "| policy_loss        | -0.0005209538 |\n",
            "| serial_timesteps   | 68096         |\n",
            "| time_elapsed       | 1.41e+03      |\n",
            "| total_timesteps    | 544768        |\n",
            "| value_loss         | 0.0040118964  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0015803163   |\n",
            "| clipfrac           | 0.010253906    |\n",
            "| ep_len_mean        | 16.3           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.772          |\n",
            "| fps                | 395            |\n",
            "| n_updates          | 134            |\n",
            "| policy_entropy     | 0.046388175    |\n",
            "| policy_loss        | -7.8777564e-05 |\n",
            "| serial_timesteps   | 68608          |\n",
            "| time_elapsed       | 1.42e+03       |\n",
            "| total_timesteps    | 548864         |\n",
            "| value_loss         | 0.0043357485   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014417318  |\n",
            "| clipfrac           | 0.00847168    |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.775         |\n",
            "| fps                | 374           |\n",
            "| n_updates          | 135           |\n",
            "| policy_entropy     | 0.039886046   |\n",
            "| policy_loss        | -0.0004678478 |\n",
            "| serial_timesteps   | 69120         |\n",
            "| time_elapsed       | 1.43e+03      |\n",
            "| total_timesteps    | 552960        |\n",
            "| value_loss         | 0.00436064    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| approxkl           | 0.0004506643    |\n",
            "| clipfrac           | 0.004663086     |\n",
            "| ep_len_mean        | 16.1            |\n",
            "| ep_reward_mean     | 0.955           |\n",
            "| explained_variance | 0.78            |\n",
            "| fps                | 401             |\n",
            "| n_updates          | 136             |\n",
            "| policy_entropy     | 0.037990924     |\n",
            "| policy_loss        | -0.000113737726 |\n",
            "| serial_timesteps   | 69632           |\n",
            "| time_elapsed       | 1.44e+03        |\n",
            "| total_timesteps    | 557056          |\n",
            "| value_loss         | 0.004203375     |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00026593092  |\n",
            "| clipfrac           | 0.003930664    |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.78           |\n",
            "| fps                | 384            |\n",
            "| n_updates          | 137            |\n",
            "| policy_entropy     | 0.02736927     |\n",
            "| policy_loss        | -0.00037469887 |\n",
            "| serial_timesteps   | 70144          |\n",
            "| time_elapsed       | 1.45e+03       |\n",
            "| total_timesteps    | 561152         |\n",
            "| value_loss         | 0.0042020353   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000177149   |\n",
            "| clipfrac           | 0.0028808594  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.78          |\n",
            "| fps                | 399           |\n",
            "| n_updates          | 138           |\n",
            "| policy_entropy     | 0.023519464   |\n",
            "| policy_loss        | -0.0001550894 |\n",
            "| serial_timesteps   | 70656         |\n",
            "| time_elapsed       | 1.46e+03      |\n",
            "| total_timesteps    | 565248        |\n",
            "| value_loss         | 0.004228201   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00022660758  |\n",
            "| clipfrac           | 0.0026123046   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 399            |\n",
            "| n_updates          | 139            |\n",
            "| policy_entropy     | 0.01966491     |\n",
            "| policy_loss        | -0.00057796284 |\n",
            "| serial_timesteps   | 71168          |\n",
            "| time_elapsed       | 1.47e+03       |\n",
            "| total_timesteps    | 569344         |\n",
            "| value_loss         | 0.0042058574   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 8.2328144e-05  |\n",
            "| clipfrac           | 0.0014404297   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 376            |\n",
            "| n_updates          | 140            |\n",
            "| policy_entropy     | 0.019186469    |\n",
            "| policy_loss        | -3.0392035e-05 |\n",
            "| serial_timesteps   | 71680          |\n",
            "| time_elapsed       | 1.48e+03       |\n",
            "| total_timesteps    | 573440         |\n",
            "| value_loss         | 0.0041418085   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00014192313 |\n",
            "| clipfrac           | 0.0020263672  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 390           |\n",
            "| n_updates          | 141           |\n",
            "| policy_entropy     | 0.015140315   |\n",
            "| policy_loss        | -6.151416e-05 |\n",
            "| serial_timesteps   | 72192         |\n",
            "| time_elapsed       | 1.49e+03      |\n",
            "| total_timesteps    | 577536        |\n",
            "| value_loss         | 0.00411389    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 6.547257e-05   |\n",
            "| clipfrac           | 0.0013183594   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 386            |\n",
            "| n_updates          | 142            |\n",
            "| policy_entropy     | 0.013506343    |\n",
            "| policy_loss        | -0.00014955716 |\n",
            "| serial_timesteps   | 72704          |\n",
            "| time_elapsed       | 1.5e+03        |\n",
            "| total_timesteps    | 581632         |\n",
            "| value_loss         | 0.00411166     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00015176591 |\n",
            "| clipfrac           | 0.0022460937  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 398           |\n",
            "| n_updates          | 143           |\n",
            "| policy_entropy     | 0.01307293    |\n",
            "| policy_loss        | -0.0003822972 |\n",
            "| serial_timesteps   | 73216         |\n",
            "| time_elapsed       | 1.51e+03      |\n",
            "| total_timesteps    | 585728        |\n",
            "| value_loss         | 0.004107292   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00012671918  |\n",
            "| clipfrac           | 0.0017578125   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 390            |\n",
            "| n_updates          | 144            |\n",
            "| policy_entropy     | 0.0116508845   |\n",
            "| policy_loss        | -0.00030995568 |\n",
            "| serial_timesteps   | 73728          |\n",
            "| time_elapsed       | 1.52e+03       |\n",
            "| total_timesteps    | 589824         |\n",
            "| value_loss         | 0.0041799406   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 8.50987e-05    |\n",
            "| clipfrac           | 0.0014404297   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 389            |\n",
            "| n_updates          | 145            |\n",
            "| policy_entropy     | 0.0097769415   |\n",
            "| policy_loss        | -0.00032969355 |\n",
            "| serial_timesteps   | 74240          |\n",
            "| time_elapsed       | 1.53e+03       |\n",
            "| total_timesteps    | 593920         |\n",
            "| value_loss         | 0.00413572     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.5461606e-05  |\n",
            "| clipfrac           | 0.0005859375   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 394            |\n",
            "| n_updates          | 146            |\n",
            "| policy_entropy     | 0.008466169    |\n",
            "| policy_loss        | -0.00013803574 |\n",
            "| serial_timesteps   | 74752          |\n",
            "| time_elapsed       | 1.54e+03       |\n",
            "| total_timesteps    | 598016         |\n",
            "| value_loss         | 0.004025585    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| approxkl           | 3.525279e-05    |\n",
            "| clipfrac           | 0.0004394531    |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.956           |\n",
            "| explained_variance | 0.784           |\n",
            "| fps                | 380             |\n",
            "| n_updates          | 147             |\n",
            "| policy_entropy     | 0.010177689     |\n",
            "| policy_loss        | -0.000101202226 |\n",
            "| serial_timesteps   | 75264           |\n",
            "| time_elapsed       | 1.55e+03        |\n",
            "| total_timesteps    | 602112          |\n",
            "| value_loss         | 0.003963706     |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0002770285 |\n",
            "| clipfrac           | 0.0013916015 |\n",
            "| ep_len_mean        | 16           |\n",
            "| ep_reward_mean     | 0.955        |\n",
            "| explained_variance | 0.784        |\n",
            "| fps                | 399          |\n",
            "| n_updates          | 148          |\n",
            "| policy_entropy     | 0.008183725  |\n",
            "| policy_loss        | -0.000266334 |\n",
            "| serial_timesteps   | 75776        |\n",
            "| time_elapsed       | 1.56e+03     |\n",
            "| total_timesteps    | 606208       |\n",
            "| value_loss         | 0.0039884234 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 4.0071875e-05 |\n",
            "| clipfrac           | 0.0005859375  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 386           |\n",
            "| n_updates          | 149           |\n",
            "| policy_entropy     | 0.0057692854  |\n",
            "| policy_loss        | -0.0001152762 |\n",
            "| serial_timesteps   | 76288         |\n",
            "| time_elapsed       | 1.57e+03      |\n",
            "| total_timesteps    | 610304        |\n",
            "| value_loss         | 0.00391153    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| approxkl           | 0.00013904765   |\n",
            "| clipfrac           | 0.0013916015    |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.956           |\n",
            "| explained_variance | 0.783           |\n",
            "| fps                | 397             |\n",
            "| n_updates          | 150             |\n",
            "| policy_entropy     | 0.0052344645    |\n",
            "| policy_loss        | -0.000117865995 |\n",
            "| serial_timesteps   | 76800           |\n",
            "| time_elapsed       | 1.58e+03        |\n",
            "| total_timesteps    | 614400          |\n",
            "| value_loss         | 0.004024858     |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00020689031 |\n",
            "| clipfrac           | 0.0014404297  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 398           |\n",
            "| n_updates          | 151           |\n",
            "| policy_entropy     | 0.0047623315  |\n",
            "| policy_loss        | -0.0002930599 |\n",
            "| serial_timesteps   | 77312         |\n",
            "| time_elapsed       | 1.59e+03      |\n",
            "| total_timesteps    | 618496        |\n",
            "| value_loss         | 0.004038092   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 4.176409e-05  |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 378           |\n",
            "| n_updates          | 152           |\n",
            "| policy_entropy     | 0.0042593693  |\n",
            "| policy_loss        | -9.767494e-05 |\n",
            "| serial_timesteps   | 77824         |\n",
            "| time_elapsed       | 1.61e+03      |\n",
            "| total_timesteps    | 622592        |\n",
            "| value_loss         | 0.003995406   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 4.741639e-05   |\n",
            "| clipfrac           | 0.00068359374  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 396            |\n",
            "| n_updates          | 153            |\n",
            "| policy_entropy     | 0.004167389    |\n",
            "| policy_loss        | -0.00024191054 |\n",
            "| serial_timesteps   | 78336          |\n",
            "| time_elapsed       | 1.62e+03       |\n",
            "| total_timesteps    | 626688         |\n",
            "| value_loss         | 0.0039934176   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 7.458564e-05  |\n",
            "| clipfrac           | 0.0005859375  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 381           |\n",
            "| n_updates          | 154           |\n",
            "| policy_entropy     | 0.005385723   |\n",
            "| policy_loss        | -9.388155e-05 |\n",
            "| serial_timesteps   | 78848         |\n",
            "| time_elapsed       | 1.63e+03      |\n",
            "| total_timesteps    | 630784        |\n",
            "| value_loss         | 0.004047794   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0046018413   |\n",
            "| clipfrac           | 0.003100586    |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.779          |\n",
            "| fps                | 396            |\n",
            "| n_updates          | 155            |\n",
            "| policy_entropy     | 0.006921912    |\n",
            "| policy_loss        | -0.00018608398 |\n",
            "| serial_timesteps   | 79360          |\n",
            "| time_elapsed       | 1.64e+03       |\n",
            "| total_timesteps    | 634880         |\n",
            "| value_loss         | 0.0040177335   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00069013995  |\n",
            "| clipfrac           | 0.0010742188   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 394            |\n",
            "| n_updates          | 156            |\n",
            "| policy_entropy     | 0.004495906    |\n",
            "| policy_loss        | -0.00011335843 |\n",
            "| serial_timesteps   | 79872          |\n",
            "| time_elapsed       | 1.65e+03       |\n",
            "| total_timesteps    | 638976         |\n",
            "| value_loss         | 0.004053506    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00053306774 |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 390           |\n",
            "| n_updates          | 157           |\n",
            "| policy_entropy     | 0.0034209844  |\n",
            "| policy_loss        | -0.0001191284 |\n",
            "| serial_timesteps   | 80384         |\n",
            "| time_elapsed       | 1.66e+03      |\n",
            "| total_timesteps    | 643072        |\n",
            "| value_loss         | 0.004012188   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 3.1800642e-05 |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 399           |\n",
            "| n_updates          | 158           |\n",
            "| policy_entropy     | 0.002852729   |\n",
            "| policy_loss        | -9.17121e-05  |\n",
            "| serial_timesteps   | 80896         |\n",
            "| time_elapsed       | 1.67e+03      |\n",
            "| total_timesteps    | 647168        |\n",
            "| value_loss         | 0.0040106876  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 2.353085e-05   |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 392            |\n",
            "| n_updates          | 159            |\n",
            "| policy_entropy     | 0.0025457572   |\n",
            "| policy_loss        | -8.9520705e-05 |\n",
            "| serial_timesteps   | 81408          |\n",
            "| time_elapsed       | 1.68e+03       |\n",
            "| total_timesteps    | 651264         |\n",
            "| value_loss         | 0.0040260204   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.0241408e-10  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 400            |\n",
            "| n_updates          | 160            |\n",
            "| policy_entropy     | 0.0025780457   |\n",
            "| policy_loss        | -1.9083382e-08 |\n",
            "| serial_timesteps   | 81920          |\n",
            "| time_elapsed       | 1.69e+03       |\n",
            "| total_timesteps    | 655360         |\n",
            "| value_loss         | 0.0039745155   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 6.8118265e-05  |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 399            |\n",
            "| n_updates          | 161            |\n",
            "| policy_entropy     | 0.002692141    |\n",
            "| policy_loss        | -0.00013699594 |\n",
            "| serial_timesteps   | 82432          |\n",
            "| time_elapsed       | 1.7e+03        |\n",
            "| total_timesteps    | 659456         |\n",
            "| value_loss         | 0.0040603536   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 4.242309e-05  |\n",
            "| clipfrac           | 0.0004394531  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 386           |\n",
            "| n_updates          | 162           |\n",
            "| policy_entropy     | 0.003201597   |\n",
            "| policy_loss        | -6.269342e-05 |\n",
            "| serial_timesteps   | 82944         |\n",
            "| time_elapsed       | 1.71e+03      |\n",
            "| total_timesteps    | 663552        |\n",
            "| value_loss         | 0.0040262714  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0015042608   |\n",
            "| clipfrac           | 0.000390625    |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 391            |\n",
            "| n_updates          | 163            |\n",
            "| policy_entropy     | 0.0035944849   |\n",
            "| policy_loss        | -0.00016132886 |\n",
            "| serial_timesteps   | 83456          |\n",
            "| time_elapsed       | 1.72e+03       |\n",
            "| total_timesteps    | 667648         |\n",
            "| value_loss         | 0.0040022917   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 7.8318255e-05 |\n",
            "| clipfrac           | 0.00036621094 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 391           |\n",
            "| n_updates          | 164           |\n",
            "| policy_entropy     | 0.0027660935  |\n",
            "| policy_loss        | 3.0492014e-05 |\n",
            "| serial_timesteps   | 83968         |\n",
            "| time_elapsed       | 1.73e+03      |\n",
            "| total_timesteps    | 671744        |\n",
            "| value_loss         | 0.0040127686  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 3.6527053e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 401            |\n",
            "| n_updates          | 165            |\n",
            "| policy_entropy     | 0.004077495    |\n",
            "| policy_loss        | -5.3140975e-05 |\n",
            "| serial_timesteps   | 84480          |\n",
            "| time_elapsed       | 1.74e+03       |\n",
            "| total_timesteps    | 675840         |\n",
            "| value_loss         | 0.0040042624   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0009766074  |\n",
            "| clipfrac           | 0.0008789062  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 398           |\n",
            "| n_updates          | 166           |\n",
            "| policy_entropy     | 0.017457966   |\n",
            "| policy_loss        | -0.0002293908 |\n",
            "| serial_timesteps   | 84992         |\n",
            "| time_elapsed       | 1.75e+03      |\n",
            "| total_timesteps    | 679936        |\n",
            "| value_loss         | 0.004012852   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.003571881   |\n",
            "| clipfrac           | 0.008422852   |\n",
            "| ep_len_mean        | 16.2          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.777         |\n",
            "| fps                | 387           |\n",
            "| n_updates          | 167           |\n",
            "| policy_entropy     | 0.026164219   |\n",
            "| policy_loss        | -0.0006268227 |\n",
            "| serial_timesteps   | 85504         |\n",
            "| time_elapsed       | 1.76e+03      |\n",
            "| total_timesteps    | 684032        |\n",
            "| value_loss         | 0.0041298633  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0007223855   |\n",
            "| clipfrac           | 0.0057373047   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.779          |\n",
            "| fps                | 406            |\n",
            "| n_updates          | 168            |\n",
            "| policy_entropy     | 0.025357138    |\n",
            "| policy_loss        | -0.00080481777 |\n",
            "| serial_timesteps   | 86016          |\n",
            "| time_elapsed       | 1.77e+03       |\n",
            "| total_timesteps    | 688128         |\n",
            "| value_loss         | 0.0041122874   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00020983073 |\n",
            "| clipfrac           | 0.002758789   |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 391           |\n",
            "| n_updates          | 169           |\n",
            "| policy_entropy     | 0.02502339    |\n",
            "| policy_loss        | 0.00015531768 |\n",
            "| serial_timesteps   | 86528         |\n",
            "| time_elapsed       | 1.78e+03      |\n",
            "| total_timesteps    | 692224        |\n",
            "| value_loss         | 0.0040516905  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00013136734 |\n",
            "| clipfrac           | 0.0021728515  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.781         |\n",
            "| fps                | 398           |\n",
            "| n_updates          | 170           |\n",
            "| policy_entropy     | 0.027054807   |\n",
            "| policy_loss        | 0.00021146899 |\n",
            "| serial_timesteps   | 87040         |\n",
            "| time_elapsed       | 1.79e+03      |\n",
            "| total_timesteps    | 696320        |\n",
            "| value_loss         | 0.004026881   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000345944   |\n",
            "| clipfrac           | 0.0044189454  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.78          |\n",
            "| fps                | 392           |\n",
            "| n_updates          | 171           |\n",
            "| policy_entropy     | 0.018984327   |\n",
            "| policy_loss        | -0.0010254778 |\n",
            "| serial_timesteps   | 87552         |\n",
            "| time_elapsed       | 1.8e+03       |\n",
            "| total_timesteps    | 700416        |\n",
            "| value_loss         | 0.0040171924  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00013513291  |\n",
            "| clipfrac           | 0.0014892578   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 406            |\n",
            "| n_updates          | 172            |\n",
            "| policy_entropy     | 0.0173988      |\n",
            "| policy_loss        | -1.0731628e-05 |\n",
            "| serial_timesteps   | 88064          |\n",
            "| time_elapsed       | 1.81e+03       |\n",
            "| total_timesteps    | 704512         |\n",
            "| value_loss         | 0.004056181    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00019500485 |\n",
            "| clipfrac           | 0.0022949218  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 399           |\n",
            "| n_updates          | 173           |\n",
            "| policy_entropy     | 0.017136069   |\n",
            "| policy_loss        | -0.0002855282 |\n",
            "| serial_timesteps   | 88576         |\n",
            "| time_elapsed       | 1.82e+03      |\n",
            "| total_timesteps    | 708608        |\n",
            "| value_loss         | 0.003961119   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 9.832719e-05   |\n",
            "| clipfrac           | 0.0017822266   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 387            |\n",
            "| n_updates          | 174            |\n",
            "| policy_entropy     | 0.015943201    |\n",
            "| policy_loss        | -5.2470376e-05 |\n",
            "| serial_timesteps   | 89088          |\n",
            "| time_elapsed       | 1.83e+03       |\n",
            "| total_timesteps    | 712704         |\n",
            "| value_loss         | 0.0039862874   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.000114053764 |\n",
            "| clipfrac           | 0.0021484375   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 401            |\n",
            "| n_updates          | 175            |\n",
            "| policy_entropy     | 0.013790366    |\n",
            "| policy_loss        | -0.0006211578  |\n",
            "| serial_timesteps   | 89600          |\n",
            "| time_elapsed       | 1.84e+03       |\n",
            "| total_timesteps    | 716800         |\n",
            "| value_loss         | 0.0039628977   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 7.583566e-05  |\n",
            "| clipfrac           | 0.0010986328  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 385           |\n",
            "| n_updates          | 176           |\n",
            "| policy_entropy     | 0.012476965   |\n",
            "| policy_loss        | -7.266337e-05 |\n",
            "| serial_timesteps   | 90112         |\n",
            "| time_elapsed       | 1.85e+03      |\n",
            "| total_timesteps    | 720896        |\n",
            "| value_loss         | 0.0040521445  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00019705805 |\n",
            "| clipfrac           | 0.0013916015  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 403           |\n",
            "| n_updates          | 177           |\n",
            "| policy_entropy     | 0.0096420245  |\n",
            "| policy_loss        | -0.0006044259 |\n",
            "| serial_timesteps   | 90624         |\n",
            "| time_elapsed       | 1.86e+03      |\n",
            "| total_timesteps    | 724992        |\n",
            "| value_loss         | 0.0039040544  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.803084e-05   |\n",
            "| clipfrac           | 0.0006591797   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 402            |\n",
            "| n_updates          | 178            |\n",
            "| policy_entropy     | 0.010445014    |\n",
            "| policy_loss        | -0.00014147702 |\n",
            "| serial_timesteps   | 91136          |\n",
            "| time_elapsed       | 1.88e+03       |\n",
            "| total_timesteps    | 729088         |\n",
            "| value_loss         | 0.003927344    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| approxkl           | 8.220396e-05    |\n",
            "| clipfrac           | 0.00095214846   |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.955           |\n",
            "| explained_variance | 0.783           |\n",
            "| fps                | 382             |\n",
            "| n_updates          | 179             |\n",
            "| policy_entropy     | 0.009310513     |\n",
            "| policy_loss        | -0.000107137996 |\n",
            "| serial_timesteps   | 91648           |\n",
            "| time_elapsed       | 1.89e+03        |\n",
            "| total_timesteps    | 733184          |\n",
            "| value_loss         | 0.003884732     |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.6862494e-05 |\n",
            "| clipfrac           | 0.0004394531  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 393           |\n",
            "| n_updates          | 180           |\n",
            "| policy_entropy     | 0.009069548   |\n",
            "| policy_loss        | -5.277861e-05 |\n",
            "| serial_timesteps   | 92160         |\n",
            "| time_elapsed       | 1.9e+03       |\n",
            "| total_timesteps    | 737280        |\n",
            "| value_loss         | 0.003929657   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021583256  |\n",
            "| clipfrac           | 0.0029296875  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.768         |\n",
            "| fps                | 384           |\n",
            "| n_updates          | 181           |\n",
            "| policy_entropy     | 0.00850445    |\n",
            "| policy_loss        | -0.0010730843 |\n",
            "| serial_timesteps   | 92672         |\n",
            "| time_elapsed       | 1.91e+03      |\n",
            "| total_timesteps    | 741376        |\n",
            "| value_loss         | 0.0043453546  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 9.0885274e-05  |\n",
            "| clipfrac           | 0.0008789062   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 402            |\n",
            "| n_updates          | 182            |\n",
            "| policy_entropy     | 0.007833996    |\n",
            "| policy_loss        | -0.00015185139 |\n",
            "| serial_timesteps   | 93184          |\n",
            "| time_elapsed       | 1.92e+03       |\n",
            "| total_timesteps    | 745472         |\n",
            "| value_loss         | 0.004003984    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.203834e-05   |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 399            |\n",
            "| n_updates          | 183            |\n",
            "| policy_entropy     | 0.007320029    |\n",
            "| policy_loss        | -9.6888405e-05 |\n",
            "| serial_timesteps   | 93696          |\n",
            "| time_elapsed       | 1.93e+03       |\n",
            "| total_timesteps    | 749568         |\n",
            "| value_loss         | 0.003903513    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 9.230915e-06   |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 393            |\n",
            "| n_updates          | 184            |\n",
            "| policy_entropy     | 0.0063793026   |\n",
            "| policy_loss        | -9.4291885e-05 |\n",
            "| serial_timesteps   | 94208          |\n",
            "| time_elapsed       | 1.94e+03       |\n",
            "| total_timesteps    | 753664         |\n",
            "| value_loss         | 0.0038949344   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 5.076357e-05   |\n",
            "| clipfrac           | 0.0006591797   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 404            |\n",
            "| n_updates          | 185            |\n",
            "| policy_entropy     | 0.005516296    |\n",
            "| policy_loss        | -0.00011066402 |\n",
            "| serial_timesteps   | 94720          |\n",
            "| time_elapsed       | 1.95e+03       |\n",
            "| total_timesteps    | 757760         |\n",
            "| value_loss         | 0.003839548    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 1.3150645e-09 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 388           |\n",
            "| n_updates          | 186           |\n",
            "| policy_entropy     | 0.0054429546  |\n",
            "| policy_loss        | -5.579503e-07 |\n",
            "| serial_timesteps   | 95232         |\n",
            "| time_elapsed       | 1.96e+03      |\n",
            "| total_timesteps    | 761856        |\n",
            "| value_loss         | 0.003904275   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00026512204  |\n",
            "| clipfrac           | 0.0010009765   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 403            |\n",
            "| n_updates          | 187            |\n",
            "| policy_entropy     | 0.0043144138   |\n",
            "| policy_loss        | -0.00011453638 |\n",
            "| serial_timesteps   | 95744          |\n",
            "| time_elapsed       | 1.97e+03       |\n",
            "| total_timesteps    | 765952         |\n",
            "| value_loss         | 0.0039299377   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 6.91854e-05   |\n",
            "| clipfrac           | 0.0008789062  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 396           |\n",
            "| n_updates          | 188           |\n",
            "| policy_entropy     | 0.004504511   |\n",
            "| policy_loss        | -0.0002370001 |\n",
            "| serial_timesteps   | 96256         |\n",
            "| time_elapsed       | 1.98e+03      |\n",
            "| total_timesteps    | 770048        |\n",
            "| value_loss         | 0.0039670644  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.4850184e-05 |\n",
            "| clipfrac           | 0.0001953125  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 398           |\n",
            "| n_updates          | 189           |\n",
            "| policy_entropy     | 0.005661777   |\n",
            "| policy_loss        | -4.19504e-05  |\n",
            "| serial_timesteps   | 96768         |\n",
            "| time_elapsed       | 1.99e+03      |\n",
            "| total_timesteps    | 774144        |\n",
            "| value_loss         | 0.003927264   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.5468634e-05 |\n",
            "| clipfrac           | 0.0006591797  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 397           |\n",
            "| n_updates          | 190           |\n",
            "| policy_entropy     | 0.004685476   |\n",
            "| policy_loss        | -0.0002393352 |\n",
            "| serial_timesteps   | 97280         |\n",
            "| time_elapsed       | 2e+03         |\n",
            "| total_timesteps    | 778240        |\n",
            "| value_loss         | 0.003911159   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00015090269  |\n",
            "| clipfrac           | 0.00041503907  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 397            |\n",
            "| n_updates          | 191            |\n",
            "| policy_entropy     | 0.005286279    |\n",
            "| policy_loss        | -0.00012158663 |\n",
            "| serial_timesteps   | 97792          |\n",
            "| time_elapsed       | 2.01e+03       |\n",
            "| total_timesteps    | 782336         |\n",
            "| value_loss         | 0.0038683533   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00056437484  |\n",
            "| clipfrac           | 0.0008544922   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 409            |\n",
            "| n_updates          | 192            |\n",
            "| policy_entropy     | 0.012309311    |\n",
            "| policy_loss        | -0.00020388358 |\n",
            "| serial_timesteps   | 98304          |\n",
            "| time_elapsed       | 2.02e+03       |\n",
            "| total_timesteps    | 786432         |\n",
            "| value_loss         | 0.0038748034   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0008527927  |\n",
            "| clipfrac           | 0.003149414   |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 393           |\n",
            "| n_updates          | 193           |\n",
            "| policy_entropy     | 0.016098097   |\n",
            "| policy_loss        | 0.00030664387 |\n",
            "| serial_timesteps   | 98816         |\n",
            "| time_elapsed       | 2.03e+03      |\n",
            "| total_timesteps    | 790528        |\n",
            "| value_loss         | 0.0038970243  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00021282875  |\n",
            "| clipfrac           | 0.0026367188   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 401            |\n",
            "| n_updates          | 194            |\n",
            "| policy_entropy     | 0.016769864    |\n",
            "| policy_loss        | -0.00042866528 |\n",
            "| serial_timesteps   | 99328          |\n",
            "| time_elapsed       | 2.04e+03       |\n",
            "| total_timesteps    | 794624         |\n",
            "| value_loss         | 0.003870679    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00021063237 |\n",
            "| clipfrac           | 0.0013671875  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.782         |\n",
            "| fps                | 403           |\n",
            "| n_updates          | 195           |\n",
            "| policy_entropy     | 0.016173584   |\n",
            "| policy_loss        | -0.0002311267 |\n",
            "| serial_timesteps   | 99840         |\n",
            "| time_elapsed       | 2.05e+03      |\n",
            "| total_timesteps    | 798720        |\n",
            "| value_loss         | 0.0039762123  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00013178524  |\n",
            "| clipfrac           | 0.0022216798   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.78           |\n",
            "| fps                | 375            |\n",
            "| n_updates          | 196            |\n",
            "| policy_entropy     | 0.014175272    |\n",
            "| policy_loss        | -0.00026541873 |\n",
            "| serial_timesteps   | 100352         |\n",
            "| time_elapsed       | 2.06e+03       |\n",
            "| total_timesteps    | 802816         |\n",
            "| value_loss         | 0.003998623    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.8971927e-05 |\n",
            "| clipfrac           | 0.0003173828  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 402           |\n",
            "| n_updates          | 197           |\n",
            "| policy_entropy     | 0.0136522595  |\n",
            "| policy_loss        | 9.568364e-05  |\n",
            "| serial_timesteps   | 100864        |\n",
            "| time_elapsed       | 2.07e+03      |\n",
            "| total_timesteps    | 806912        |\n",
            "| value_loss         | 0.0038946276  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.000139597    |\n",
            "| clipfrac           | 0.0019042969   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 386            |\n",
            "| n_updates          | 198            |\n",
            "| policy_entropy     | 0.01125682     |\n",
            "| policy_loss        | -0.00031506707 |\n",
            "| serial_timesteps   | 101376         |\n",
            "| time_elapsed       | 2.08e+03       |\n",
            "| total_timesteps    | 811008         |\n",
            "| value_loss         | 0.0038901616   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 3.1100735e-05 |\n",
            "| clipfrac           | 0.0006347656  |\n",
            "| ep_len_mean        | 16.1          |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 404           |\n",
            "| n_updates          | 199           |\n",
            "| policy_entropy     | 0.012614961   |\n",
            "| policy_loss        | 0.00011418662 |\n",
            "| serial_timesteps   | 101888        |\n",
            "| time_elapsed       | 2.09e+03      |\n",
            "| total_timesteps    | 815104        |\n",
            "| value_loss         | 0.0038608175  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00012746212  |\n",
            "| clipfrac           | 0.0018798828   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 398            |\n",
            "| n_updates          | 200            |\n",
            "| policy_entropy     | 0.011260988    |\n",
            "| policy_loss        | -0.00022641386 |\n",
            "| serial_timesteps   | 102400         |\n",
            "| time_elapsed       | 2.1e+03        |\n",
            "| total_timesteps    | 819200         |\n",
            "| value_loss         | 0.0039445474   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 3.3325294e-05 |\n",
            "| clipfrac           | 0.00041503907 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 395           |\n",
            "| n_updates          | 201           |\n",
            "| policy_entropy     | 0.011181999   |\n",
            "| policy_loss        | 7.371309e-05  |\n",
            "| serial_timesteps   | 102912        |\n",
            "| time_elapsed       | 2.11e+03      |\n",
            "| total_timesteps    | 823296        |\n",
            "| value_loss         | 0.0039872215  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 5.9849513e-05  |\n",
            "| clipfrac           | 0.0010498047   |\n",
            "| ep_len_mean        | 16.1           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 404            |\n",
            "| n_updates          | 202            |\n",
            "| policy_entropy     | 0.009399733    |\n",
            "| policy_loss        | -0.00013053951 |\n",
            "| serial_timesteps   | 103424         |\n",
            "| time_elapsed       | 2.12e+03       |\n",
            "| total_timesteps    | 827392         |\n",
            "| value_loss         | 0.0038837034   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 4.2498305e-05  |\n",
            "| clipfrac           | 0.00080566405  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 391            |\n",
            "| n_updates          | 203            |\n",
            "| policy_entropy     | 0.009708491    |\n",
            "| policy_loss        | -0.00022547328 |\n",
            "| serial_timesteps   | 103936         |\n",
            "| time_elapsed       | 2.13e+03       |\n",
            "| total_timesteps    | 831488         |\n",
            "| value_loss         | 0.0038645058   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.000118172284 |\n",
            "| clipfrac           | 0.0006347656   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 401            |\n",
            "| n_updates          | 204            |\n",
            "| policy_entropy     | 0.008581025    |\n",
            "| policy_loss        | -0.00020968038 |\n",
            "| serial_timesteps   | 104448         |\n",
            "| time_elapsed       | 2.14e+03       |\n",
            "| total_timesteps    | 835584         |\n",
            "| value_loss         | 0.0039583095   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00010741807  |\n",
            "| clipfrac           | 0.00068359374  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 402            |\n",
            "| n_updates          | 205            |\n",
            "| policy_entropy     | 0.008364101    |\n",
            "| policy_loss        | -0.00018603055 |\n",
            "| serial_timesteps   | 104960         |\n",
            "| time_elapsed       | 2.15e+03       |\n",
            "| total_timesteps    | 839680         |\n",
            "| value_loss         | 0.0039075953   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| approxkl           | 3.899204e-05    |\n",
            "| clipfrac           | 0.0005615234    |\n",
            "| ep_len_mean        | 16              |\n",
            "| ep_reward_mean     | 0.956           |\n",
            "| explained_variance | 0.783           |\n",
            "| fps                | 393             |\n",
            "| n_updates          | 206             |\n",
            "| policy_entropy     | 0.007562919     |\n",
            "| policy_loss        | -0.000113087786 |\n",
            "| serial_timesteps   | 105472          |\n",
            "| time_elapsed       | 2.16e+03        |\n",
            "| total_timesteps    | 843776          |\n",
            "| value_loss         | 0.003874023     |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00026712444 |\n",
            "| clipfrac           | 0.0013916015  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 400           |\n",
            "| n_updates          | 207           |\n",
            "| policy_entropy     | 0.0074654566  |\n",
            "| policy_loss        | -0.0004098919 |\n",
            "| serial_timesteps   | 105984        |\n",
            "| time_elapsed       | 2.17e+03      |\n",
            "| total_timesteps    | 847872        |\n",
            "| value_loss         | 0.003798067   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 6.7068075e-05 |\n",
            "| clipfrac           | 0.0006347656  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 382           |\n",
            "| n_updates          | 208           |\n",
            "| policy_entropy     | 0.0067163743  |\n",
            "| policy_loss        | -4.350869e-05 |\n",
            "| serial_timesteps   | 106496        |\n",
            "| time_elapsed       | 2.18e+03      |\n",
            "| total_timesteps    | 851968        |\n",
            "| value_loss         | 0.0038257104  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 6.519532e-05   |\n",
            "| clipfrac           | 0.00083007815  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 396            |\n",
            "| n_updates          | 209            |\n",
            "| policy_entropy     | 0.005830121    |\n",
            "| policy_loss        | -0.00021451036 |\n",
            "| serial_timesteps   | 107008         |\n",
            "| time_elapsed       | 2.2e+03        |\n",
            "| total_timesteps    | 856064         |\n",
            "| value_loss         | 0.0038668965   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00023857364  |\n",
            "| clipfrac           | 0.001171875    |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 384            |\n",
            "| n_updates          | 210            |\n",
            "| policy_entropy     | 0.005468042    |\n",
            "| policy_loss        | -0.00049377687 |\n",
            "| serial_timesteps   | 107520         |\n",
            "| time_elapsed       | 2.21e+03       |\n",
            "| total_timesteps    | 860160         |\n",
            "| value_loss         | 0.0039182827   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0034549884   |\n",
            "| clipfrac           | 0.00095214846  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.781          |\n",
            "| fps                | 396            |\n",
            "| n_updates          | 211            |\n",
            "| policy_entropy     | 0.0057199355   |\n",
            "| policy_loss        | -0.00017970399 |\n",
            "| serial_timesteps   | 108032         |\n",
            "| time_elapsed       | 2.22e+03       |\n",
            "| total_timesteps    | 864256         |\n",
            "| value_loss         | 0.0039619594   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.000116664996 |\n",
            "| clipfrac           | 0.0005859375   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 408            |\n",
            "| n_updates          | 212            |\n",
            "| policy_entropy     | 0.00577538     |\n",
            "| policy_loss        | 8.037909e-05   |\n",
            "| serial_timesteps   | 108544         |\n",
            "| time_elapsed       | 2.23e+03       |\n",
            "| total_timesteps    | 868352         |\n",
            "| value_loss         | 0.003902649    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 4.9106242e-05  |\n",
            "| clipfrac           | 0.00080566405  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 397            |\n",
            "| n_updates          | 213            |\n",
            "| policy_entropy     | 0.0056024175   |\n",
            "| policy_loss        | -7.5643926e-05 |\n",
            "| serial_timesteps   | 109056         |\n",
            "| time_elapsed       | 2.24e+03       |\n",
            "| total_timesteps    | 872448         |\n",
            "| value_loss         | 0.0038778991   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 5.821384e-05   |\n",
            "| clipfrac           | 0.0006347656   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 412            |\n",
            "| n_updates          | 214            |\n",
            "| policy_entropy     | 0.0042036013   |\n",
            "| policy_loss        | -0.00023584829 |\n",
            "| serial_timesteps   | 109568         |\n",
            "| time_elapsed       | 2.25e+03       |\n",
            "| total_timesteps    | 876544         |\n",
            "| value_loss         | 0.0038328213   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 3.1610274e-05  |\n",
            "| clipfrac           | 0.00034179687  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 385            |\n",
            "| n_updates          | 215            |\n",
            "| policy_entropy     | 0.0041298512   |\n",
            "| policy_loss        | -0.00012481272 |\n",
            "| serial_timesteps   | 110080         |\n",
            "| time_elapsed       | 2.26e+03       |\n",
            "| total_timesteps    | 880640         |\n",
            "| value_loss         | 0.0038909938   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.954048e-05   |\n",
            "| clipfrac           | 0.0005126953   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 401            |\n",
            "| n_updates          | 216            |\n",
            "| policy_entropy     | 0.0041415785   |\n",
            "| policy_loss        | -1.0674472e-05 |\n",
            "| serial_timesteps   | 110592         |\n",
            "| time_elapsed       | 2.27e+03       |\n",
            "| total_timesteps    | 884736         |\n",
            "| value_loss         | 0.0039580287   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 7.652055e-05   |\n",
            "| clipfrac           | 0.0006591797   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 394            |\n",
            "| n_updates          | 217            |\n",
            "| policy_entropy     | 0.0038591984   |\n",
            "| policy_loss        | -6.6737724e-05 |\n",
            "| serial_timesteps   | 111104         |\n",
            "| time_elapsed       | 2.28e+03       |\n",
            "| total_timesteps    | 888832         |\n",
            "| value_loss         | 0.003867792    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 3.0579053e-05 |\n",
            "| clipfrac           | 0.0005859375  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 390           |\n",
            "| n_updates          | 218           |\n",
            "| policy_entropy     | 0.0043807207  |\n",
            "| policy_loss        | 2.0067539e-05 |\n",
            "| serial_timesteps   | 111616        |\n",
            "| time_elapsed       | 2.29e+03      |\n",
            "| total_timesteps    | 892928        |\n",
            "| value_loss         | 0.0037693873  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.2889475e-05  |\n",
            "| clipfrac           | 0.00034179687  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 402            |\n",
            "| n_updates          | 219            |\n",
            "| policy_entropy     | 0.0036998105   |\n",
            "| policy_loss        | -3.0370964e-05 |\n",
            "| serial_timesteps   | 112128         |\n",
            "| time_elapsed       | 2.3e+03        |\n",
            "| total_timesteps    | 897024         |\n",
            "| value_loss         | 0.0037941758   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 1.9401755e-10 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 392           |\n",
            "| n_updates          | 220           |\n",
            "| policy_entropy     | 0.003742456   |\n",
            "| policy_loss        | 1.5259047e-08 |\n",
            "| serial_timesteps   | 112640        |\n",
            "| time_elapsed       | 2.31e+03      |\n",
            "| total_timesteps    | 901120        |\n",
            "| value_loss         | 0.003781176   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.5810301e-05  |\n",
            "| clipfrac           | 0.00029296876  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 395            |\n",
            "| n_updates          | 221            |\n",
            "| policy_entropy     | 0.0035081967   |\n",
            "| policy_loss        | -1.5971105e-05 |\n",
            "| serial_timesteps   | 113152         |\n",
            "| time_elapsed       | 2.32e+03       |\n",
            "| total_timesteps    | 905216         |\n",
            "| value_loss         | 0.0038417627   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 3.3269822e-05 |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 397           |\n",
            "| n_updates          | 222           |\n",
            "| policy_entropy     | 0.0031703315  |\n",
            "| policy_loss        | -9.240941e-05 |\n",
            "| serial_timesteps   | 113664        |\n",
            "| time_elapsed       | 2.33e+03      |\n",
            "| total_timesteps    | 909312        |\n",
            "| value_loss         | 0.0038584427  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 5.662001e-05  |\n",
            "| clipfrac           | 0.00068359374 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.786         |\n",
            "| fps                | 395           |\n",
            "| n_updates          | 223           |\n",
            "| policy_entropy     | 0.003223069   |\n",
            "| policy_loss        | -8.018239e-06 |\n",
            "| serial_timesteps   | 114176        |\n",
            "| time_elapsed       | 2.34e+03      |\n",
            "| total_timesteps    | 913408        |\n",
            "| value_loss         | 0.0038938418  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00013276146  |\n",
            "| clipfrac           | 0.0008544922   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 389            |\n",
            "| n_updates          | 224            |\n",
            "| policy_entropy     | 0.004051339    |\n",
            "| policy_loss        | -0.00017964302 |\n",
            "| serial_timesteps   | 114688         |\n",
            "| time_elapsed       | 2.35e+03       |\n",
            "| total_timesteps    | 917504         |\n",
            "| value_loss         | 0.0038119783   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0001463589  |\n",
            "| clipfrac           | 0.0009033203  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.955         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 386           |\n",
            "| n_updates          | 225           |\n",
            "| policy_entropy     | 0.004554534   |\n",
            "| policy_loss        | 6.8708185e-05 |\n",
            "| serial_timesteps   | 115200        |\n",
            "| time_elapsed       | 2.36e+03      |\n",
            "| total_timesteps    | 921600        |\n",
            "| value_loss         | 0.0037884682  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0013712087   |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 395            |\n",
            "| n_updates          | 226            |\n",
            "| policy_entropy     | 0.0051412187   |\n",
            "| policy_loss        | -0.00011335523 |\n",
            "| serial_timesteps   | 115712         |\n",
            "| time_elapsed       | 2.37e+03       |\n",
            "| total_timesteps    | 925696         |\n",
            "| value_loss         | 0.0038136381   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 6.9396134e-05  |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 403            |\n",
            "| n_updates          | 227            |\n",
            "| policy_entropy     | 0.0046375236   |\n",
            "| policy_loss        | -0.00013221591 |\n",
            "| serial_timesteps   | 116224         |\n",
            "| time_elapsed       | 2.38e+03       |\n",
            "| total_timesteps    | 929792         |\n",
            "| value_loss         | 0.0039324476   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 1.5561009e-05 |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 381           |\n",
            "| n_updates          | 228           |\n",
            "| policy_entropy     | 0.0041248393  |\n",
            "| policy_loss        | -8.946363e-05 |\n",
            "| serial_timesteps   | 116736        |\n",
            "| time_elapsed       | 2.39e+03      |\n",
            "| total_timesteps    | 933888        |\n",
            "| value_loss         | 0.0037846926  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 4.282057e-06   |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 384            |\n",
            "| n_updates          | 229            |\n",
            "| policy_entropy     | 0.004235089    |\n",
            "| policy_loss        | -4.7316462e-05 |\n",
            "| serial_timesteps   | 117248         |\n",
            "| time_elapsed       | 2.4e+03        |\n",
            "| total_timesteps    | 937984         |\n",
            "| value_loss         | 0.0038083226   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 4.4210421e-10 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 383           |\n",
            "| n_updates          | 230           |\n",
            "| policy_entropy     | 0.0041774213  |\n",
            "| policy_loss        | -5.219772e-08 |\n",
            "| serial_timesteps   | 117760        |\n",
            "| time_elapsed       | 2.41e+03      |\n",
            "| total_timesteps    | 942080        |\n",
            "| value_loss         | 0.0038077522  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.9666324e-05  |\n",
            "| clipfrac           | 0.00041503907  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 400            |\n",
            "| n_updates          | 231            |\n",
            "| policy_entropy     | 0.004385638    |\n",
            "| policy_loss        | -0.00013576742 |\n",
            "| serial_timesteps   | 118272         |\n",
            "| time_elapsed       | 2.42e+03       |\n",
            "| total_timesteps    | 946176         |\n",
            "| value_loss         | 0.0038427007   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0001183541   |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 382            |\n",
            "| n_updates          | 232            |\n",
            "| policy_entropy     | 0.0049733277   |\n",
            "| policy_loss        | -0.00018898695 |\n",
            "| serial_timesteps   | 118784         |\n",
            "| time_elapsed       | 2.43e+03       |\n",
            "| total_timesteps    | 950272         |\n",
            "| value_loss         | 0.0038805208   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0012255465   |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 390            |\n",
            "| n_updates          | 233            |\n",
            "| policy_entropy     | 0.0047550127   |\n",
            "| policy_loss        | -7.1284674e-05 |\n",
            "| serial_timesteps   | 119296         |\n",
            "| time_elapsed       | 2.45e+03       |\n",
            "| total_timesteps    | 954368         |\n",
            "| value_loss         | 0.0038788903   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 7.142704e-05   |\n",
            "| clipfrac           | 0.0004394531   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.783          |\n",
            "| fps                | 388            |\n",
            "| n_updates          | 234            |\n",
            "| policy_entropy     | 0.005123212    |\n",
            "| policy_loss        | -0.00013067429 |\n",
            "| serial_timesteps   | 119808         |\n",
            "| time_elapsed       | 2.46e+03       |\n",
            "| total_timesteps    | 958464         |\n",
            "| value_loss         | 0.0038579102   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00015371208  |\n",
            "| clipfrac           | 0.0006347656   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 376            |\n",
            "| n_updates          | 235            |\n",
            "| policy_entropy     | 0.0042234594   |\n",
            "| policy_loss        | -0.00012853523 |\n",
            "| serial_timesteps   | 120320         |\n",
            "| time_elapsed       | 2.47e+03       |\n",
            "| total_timesteps    | 962560         |\n",
            "| value_loss         | 0.0038308098   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.9783425e-09  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 395            |\n",
            "| n_updates          | 236            |\n",
            "| policy_entropy     | 0.004374949    |\n",
            "| policy_loss        | -1.1330991e-06 |\n",
            "| serial_timesteps   | 120832         |\n",
            "| time_elapsed       | 2.48e+03       |\n",
            "| total_timesteps    | 966656         |\n",
            "| value_loss         | 0.0037907318   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 3.8212944e-05 |\n",
            "| clipfrac           | 0.00021972656 |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.785         |\n",
            "| fps                | 389           |\n",
            "| n_updates          | 237           |\n",
            "| policy_entropy     | 0.004891043   |\n",
            "| policy_loss        | -4.482378e-05 |\n",
            "| serial_timesteps   | 121344        |\n",
            "| time_elapsed       | 2.49e+03      |\n",
            "| total_timesteps    | 970752        |\n",
            "| value_loss         | 0.0037558987  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.6024405e-05  |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.785          |\n",
            "| fps                | 396            |\n",
            "| n_updates          | 238            |\n",
            "| policy_entropy     | 0.0052727694   |\n",
            "| policy_loss        | -4.6939964e-05 |\n",
            "| serial_timesteps   | 121856         |\n",
            "| time_elapsed       | 2.5e+03        |\n",
            "| total_timesteps    | 974848         |\n",
            "| value_loss         | 0.0038047351   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 7.921533e-05   |\n",
            "| clipfrac           | 0.00021972656  |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 401            |\n",
            "| n_updates          | 239            |\n",
            "| policy_entropy     | 0.008386631    |\n",
            "| policy_loss        | -4.5521174e-05 |\n",
            "| serial_timesteps   | 122368         |\n",
            "| time_elapsed       | 2.51e+03       |\n",
            "| total_timesteps    | 978944         |\n",
            "| value_loss         | 0.003798694    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00032218322  |\n",
            "| clipfrac           | 0.0012451172   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 391            |\n",
            "| n_updates          | 240            |\n",
            "| policy_entropy     | 0.019276615    |\n",
            "| policy_loss        | -0.00010732887 |\n",
            "| serial_timesteps   | 122880         |\n",
            "| time_elapsed       | 2.52e+03       |\n",
            "| total_timesteps    | 983040         |\n",
            "| value_loss         | 0.0038066239   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0143608395   |\n",
            "| clipfrac           | 0.0069091795   |\n",
            "| ep_len_mean        | 16.2           |\n",
            "| ep_reward_mean     | 0.955          |\n",
            "| explained_variance | 0.773          |\n",
            "| fps                | 398            |\n",
            "| n_updates          | 241            |\n",
            "| policy_entropy     | 0.014252773    |\n",
            "| policy_loss        | -0.00049138867 |\n",
            "| serial_timesteps   | 123392         |\n",
            "| time_elapsed       | 2.53e+03       |\n",
            "| total_timesteps    | 987136         |\n",
            "| value_loss         | 0.004184981    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=0.96 +/- 0.00\n",
            "Episode length: 16.00 +/- 0.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0011902356   |\n",
            "| clipfrac           | 0.0013916015   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.782          |\n",
            "| fps                | 381            |\n",
            "| n_updates          | 242            |\n",
            "| policy_entropy     | 0.008422137    |\n",
            "| policy_loss        | -0.00039738757 |\n",
            "| serial_timesteps   | 123904         |\n",
            "| time_elapsed       | 2.54e+03       |\n",
            "| total_timesteps    | 991232         |\n",
            "| value_loss         | 0.0039143553   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 3.548547e-05  |\n",
            "| clipfrac           | 0.0005371094  |\n",
            "| ep_len_mean        | 16            |\n",
            "| ep_reward_mean     | 0.956         |\n",
            "| explained_variance | 0.784         |\n",
            "| fps                | 404           |\n",
            "| n_updates          | 243           |\n",
            "| policy_entropy     | 0.007255816   |\n",
            "| policy_loss        | -0.0001552066 |\n",
            "| serial_timesteps   | 124416        |\n",
            "| time_elapsed       | 2.55e+03      |\n",
            "| total_timesteps    | 995328        |\n",
            "| value_loss         | 0.0038702663  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00020456784  |\n",
            "| clipfrac           | 0.0006347656   |\n",
            "| ep_len_mean        | 16             |\n",
            "| ep_reward_mean     | 0.956          |\n",
            "| explained_variance | 0.784          |\n",
            "| fps                | 397            |\n",
            "| n_updates          | 244            |\n",
            "| policy_entropy     | 0.0067005395   |\n",
            "| policy_loss        | -0.00021934067 |\n",
            "| serial_timesteps   | 124928         |\n",
            "| time_elapsed       | 2.56e+03       |\n",
            "| total_timesteps    | 999424         |\n",
            "| value_loss         | 0.0038789758   |\n",
            "---------------------------------------\n",
            "WARNING:tensorflow:From train.py:431: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "Saving to logs/ppo2/MiniGrid-SimpleCrossingEnvUmaze-v0_1\n",
            "\u001b[0m[b0deb1fd44a7:01396] *** Process received signal ***\n",
            "[b0deb1fd44a7:01396] Signal: Segmentation fault (11)\n",
            "[b0deb1fd44a7:01396] Signal code: Address not mapped (1)\n",
            "[b0deb1fd44a7:01396] Failing at address: 0x7f562a54e20d\n",
            "[b0deb1fd44a7:01396] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f562cdef980]\n",
            "[b0deb1fd44a7:01396] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f562ca2e8a5]\n",
            "[b0deb1fd44a7:01396] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f562d299e44]\n",
            "[b0deb1fd44a7:01396] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f562ca2f735]\n",
            "[b0deb1fd44a7:01396] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f562d297cb3]\n",
            "[b0deb1fd44a7:01396] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fHBq73665yD"
      },
      "source": [
        "#### Evaluate trained agent\n",
        "\n",
        "\n",
        "You can remove the `--folder logs/` to evaluate pretrained agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8YuEgU6bT3",
        "outputId": "1216c13d-837b-42ac-c15a-85ec3126ad10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python enjoy.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --no-render --n-timesteps 5000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"enjoy.py\", line 182, in <module>\n",
            "    main()\n",
            "  File \"enjoy.py\", line 77, in main\n",
            "    model_path = find_saved_model(algo, log_path, env_id, load_best=args.load_best)\n",
            "  File \"/content/rl-baselines-zoo/utils/utils.py\", line 369, in find_saved_model\n",
            "    raise ValueError(\"No model found for {} on {}, path: {}\".format(algo, env_id, model_path))\n",
            "ValueError: No model found for ppo2 on MiniGrid-SimpleCrossingS9N1-v0, path: logs/ppo2/MiniGrid-SimpleCrossingS9N1-v0.zip\n",
            "[821dccaef3a6:03091] *** Process received signal ***\n",
            "[821dccaef3a6:03091] Signal: Segmentation fault (11)\n",
            "[821dccaef3a6:03091] Signal code: Address not mapped (1)\n",
            "[821dccaef3a6:03091] Failing at address: 0x7fec1431620d\n",
            "[821dccaef3a6:03091] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fec16dbb980]\n",
            "[821dccaef3a6:03091] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fec169fa8a5]\n",
            "[821dccaef3a6:03091] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fec17265e44]\n",
            "[821dccaef3a6:03091] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fec169fb735]\n",
            "[821dccaef3a6:03091] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fec17263cb3]\n",
            "[821dccaef3a6:03091] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Il2J0VHPLC"
      },
      "source": [
        "#### Tune Hyperparameters\n",
        "\n",
        "We use [Optuna](https://optuna.org/) for optimizing the hyperparameters.\n",
        "\n",
        "Tune the hyperparameters for PPO2, using a tpe sampler and median pruner, 2 parallels jobs,\n",
        "with a budget of 1000 trials and a maximum of 50000 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sC22eGHTH-"
      },
      "source": [
        "!python -m train.py --algo ppo2 --env MiniGrid-SimpleCrossingS9N1-v0 --gym-packages gym_minigrid -n 5000 -optimize --n-trials 100 --n-jobs 8 --sampler tpe --pruner median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Record  a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS1VBBaQ_emT"
      },
      "source": [
        "!pip install pyglet==1.3.1  # pyglet v1.4.1 throws an error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip3AauLzwNGP"
      },
      "source": [
        "!python -m utils.record_video --algo ppo2 --env MiniGrid-SimpleCrossingEnvUmaze-v0 --gym-packages gym_minigrid --exp-id 0 -f logs/ -n 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBuUfnzI8DN6"
      },
      "source": [
        "### Display the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3OTfpf8CXu"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOjFuwK9HI0"
      },
      "source": [
        "show_videos(prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjdpP0HE8D2p"
      },
      "source": [
        "### Continue Training\n",
        "\n",
        "Here, we will continue training of the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgMZQJJF6u1C"
      },
      "source": [
        "!python train.py --algo a2c --env CartPole-v1 --n-timesteps 50000 -i logs/a2c/CartPole-v1.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSaoyiAE8cVj"
      },
      "source": [
        "!python enjoy.py --algo a2c --env CartPole-v1 --no-render --n-timesteps 1000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9u4I1H-48O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}