{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codigo StableBaselines.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KLpmSgmeN6PX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpg/RL-Agent-for-Unreal-Engine/blob/main/Codigo_StableBaselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmxXV_TQMVTV"
      },
      "source": [
        "### Descarga de Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho3Jqzp-yr7r"
      },
      "source": [
        "!git clone https://github.com/maximecb/gym-minigrid.git\n",
        "%cd gym-minigrid\n",
        "!python setup.py install\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip3 install box2d-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56usX4V4MgTq"
      },
      "source": [
        "### Importar Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "outputId": "899b15b7-0909-4615-e2e1-ce3a2e5727c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "import gym\n",
        "import gym_minigrid\n",
        "import numpy as np\n",
        "from stable_baselines.common import make_vec_env\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2, DDPG, DQN, A2C, ACER, TRPO\n",
        "from stable_baselines.common.policies import MlpPolicy, CnnPolicy, MlpLstmPolicy, MlpLnLstmPolicy, CnnLstmPolicy, CnnLnLstmPolicy\n",
        "# from stable_baselines.deepq.policies import MlpPolicy, CnnPolicy\n",
        "from stable_baselines.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_X-G5NnMr2i"
      },
      "source": [
        "### Carga de Entorno Minigrid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "outputId": "91223397-a3fe-457b-ff62-33d3b560103c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Eenvironement for learning\n",
        "# Use a separate environement for evaluation\n",
        "from gym_minigrid.wrappers import *\n",
        "env = gym.make('MiniGrid-DoorKey-5x5-v0')\n",
        "# env = make_vec_env(env, n_envs=2)\n",
        "env = RGBImgObsWrapper(env) # Get pixel observations\n",
        "env = ImgObsWrapper(env) # Get rid of the 'mission' field\n",
        "# env = DirectionObsWrapper(env)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "#env = gym.make('MiniGrid-DoorKey-5x5-v0')\n",
        "print(env.observation_space)\n",
        "print(env.action_space) # 7\n",
        "eval_env = env\n",
        "# vectorized environments allow to easily multiprocess training\n",
        "# we demonstrate its usefulness in the next examples\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(40, 40, 3)\n",
            "Discrete(7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xz3YmHXN_Nw",
        "outputId": "4d18356f-6c6e-42bc-dfeb-cba00d465d56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%cd /content/gym-minigrid/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gym-minigrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "source": [
        "# Use a separate environement for evaluation\n",
        "# eval_env = gym.make('CartPole-v1')\n",
        "\n",
        "# Random Agent, before training\n",
        "# mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "# print(f\"mean_reward for random agent:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJyygEePNZB5"
      },
      "source": [
        "### Definicion de Modelo del Agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDCys-Vg-yYR"
      },
      "source": [
        "# Activate all extensions\n",
        "kwargs = {'double_q': True, 'prioritized_replay': True, 'policy_kwargs': dict(dueling=True)}\n",
        "\n",
        "agent_model = A2C('CnnLnLstmPolicy', env, verbose=1, n_steps=10,seed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k9PbbcWNn68"
      },
      "source": [
        "### Evaluar Agente Ingenuo/Aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDQTdpYv9xJN",
        "outputId": "86ebab7c-c642-4bfd-d1e4-98c80e00860b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Random Agent, before training using property\n",
        "from stable_baselines.common.evaluation import evaluate_policy\n",
        "mean_reward, std_reward = evaluate_policy(agent_model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_reward:0.00 +/- 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pjAP8NWNu_6"
      },
      "source": [
        "### Entrenar Agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koHGB-VN-81O"
      },
      "source": [
        "agent_model.learn(total_timesteps=2000, log_interval=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcsEVysINz9j"
      },
      "source": [
        "### Evaluar Agente Entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuCkk1M-kPGP",
        "outputId": "bfb3b8cd-45ce-4684-cb02-f1d76191facc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Trained Agent\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(agent_model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_reward:0.00 +/- 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLpmSgmeN6PX"
      },
      "source": [
        "### Extras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIe43E_dSBRy"
      },
      "source": [
        "import gym_minigrid\n",
        "\n",
        "env = gym.make('MiniGrid-SimpleCrossingEnvUmaze-v0')\n",
        "for i_episode in range(3):\n",
        "    obs = env.reset()\n",
        "    for t_steps in range(100):\n",
        "        # env.render()\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        print(obs)\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t_steps+1))\n",
        "            break\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df08vXY_MtHg"
      },
      "source": [
        "!apt update && apt install xvfb\n",
        "!pip install gym-notebook-wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqks9c0T-IDm"
      },
      "source": [
        "import gnwrapper\n",
        "import gym\n",
        "\n",
        "def evaluate(model, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single Environment\n",
        "    # env = gnwrapper.LoopAnimation(model.get_env()) # Start Xvfb\n",
        "    env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            # _states are only useful when using LSTM policies\n",
        "            action, _states = model.predict(obs)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            # env.render()\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "        \n",
        "\n",
        "    # env.display()\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
        "\n",
        "    return mean_episode_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4A_IOZ_9fzW"
      },
      "source": [
        "# Random Agent, before training using function\n",
        "mean_reward_before_train = evaluate(agent_model, num_episodes=100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}