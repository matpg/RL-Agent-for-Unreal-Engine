{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codigo StableBaselines.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KLpmSgmeN6PX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpg/RL-Agent-for-Unreal-Engine/blob/main/Codigo_StableBaselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmxXV_TQMVTV"
      },
      "source": [
        "### Descarga de Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho3Jqzp-yr7r"
      },
      "source": [
        "!git clone https://github.com/maximecb/gym-minigrid.git\n",
        "%cd gym-minigrid\n",
        "!python setup.py install\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip3 install box2d-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56usX4V4MgTq"
      },
      "source": [
        "### Importar Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import gym\n",
        "import gym_minigrid\n",
        "import numpy as np\n",
        "from stable_baselines.common.vec_env import *\n",
        "from stable_baselines.bench.monitor import *\n",
        "from stable_baselines.common import set_global_seeds\n",
        "from stable_baselines import PPO2, DDPG, DQN, A2C, ACER, TRPO\n",
        "from stable_baselines.common.policies import MlpPolicy, CnnPolicy, MlpLstmPolicy, MlpLnLstmPolicy, CnnLstmPolicy, CnnLnLstmPolicy\n",
        "# from stable_baselines.deepq.policies import MlpPolicy, CnnPolicy\n",
        "from stable_baselines.common.evaluation import evaluate_policy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_X-G5NnMr2i"
      },
      "source": [
        "### Carga de Entorno Minigrid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "source": [
        "# Eenvironement for learning\n",
        "# Use a separate environement for evaluation\n",
        "from gym_minigrid.wrappers import *\n",
        "\n",
        "env_id = 'MiniGrid-LavaCrossingS9N1-v0'\n",
        "eval_env = gym.make(env_id)\n",
        "# env = make_vec_env(env, n_envs=2)\n",
        "eval_env = FlatObsWrapper(eval_env) # Get pixel observations\n",
        "# env = DirectionObsWrapper(env)\n",
        "# env = FlatObsWrapper(env)\n",
        "# env = gym.make('MiniGrid-DoorKey-5x5-v0')\n",
        "# print(env.observation_space)\n",
        "# print(env.action_space) # 7\n",
        "\n",
        "# vectorized environments allow to easily multiprocess training\n",
        "# we demonstrate its usefulness in the next examples\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggH5T4vIJLqM"
      },
      "source": [
        "def make_env(env_id, rank, seed=0):\n",
        "    \"\"\"\n",
        "    Utility function for multiprocessed env.\n",
        "    \n",
        "    :param env_id: (str) the environment ID\n",
        "    :param seed: (int) the inital seed for RNG\n",
        "    :param rank: (int) index of the subprocess\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        env = FlatObsWrapper(env) # Get pixel observations\n",
        "        # Important: use a different seed for each environment\n",
        "        env.seed(seed + rank)\n",
        "        return env\n",
        "    set_global_seeds(seed)\n",
        "    return _init"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqAcx8k-JMtm",
        "outputId": "c09e0625-2b8e-4c97-d950-4d32c4e1c26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_process = 8\n",
        "env = DummyVecEnv([make_env(env_id, i) for i in range(n_process)])\n",
        "train_env = VecNormalize(env)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xz3YmHXN_Nw",
        "outputId": "4d18356f-6c6e-42bc-dfeb-cba00d465d56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%cd /content/gym-minigrid/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gym-minigrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "source": [
        "# Use a separate environement for evaluation\n",
        "# eval_env = gym.make('CartPole-v1')\n",
        "\n",
        "# Random Agent, before training\n",
        "# mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "# print(f\"mean_reward for random agent:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJyygEePNZB5"
      },
      "source": [
        "### Definicion de Modelo del Agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDCys-Vg-yYR",
        "outputId": "b699735b-ce4f-4448-bfaf-fd14cb258685",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "agent_model = PPO2(policy='MlpPolicy', env=train_env, n_steps=512, ent_coef=0.0, nminibatches=32, noptepochs=10, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k9PbbcWNn68"
      },
      "source": [
        "### Evaluar Agente Ingenuo/Aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqks9c0T-IDm"
      },
      "source": [
        "def evaluate(model, eval_env, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single Environment\n",
        "    # env = gnwrapper.LoopAnimation(model.get_env()) # Start Xvfb\n",
        "    env = eval_env\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            # _states are only useful when using LSTM policies\n",
        "            action, _states = model.predict(obs)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            # env.render()\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "        \n",
        "\n",
        "    # env.display()\n",
        "    max_episode_reward = np.amax(all_episode_rewards)\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    min_episode_reward = np.amin(all_episode_rewards)\n",
        "    print(\n",
        "        \"Max reward:\", max_episode_reward,\n",
        "        \"Mean reward:\", mean_episode_reward,\n",
        "        \"Min reward:\", min_episode_reward,\n",
        "         \"Num episodes:\", num_episodes)\n",
        "\n",
        "    return max_episode_reward, mean_episode_reward, min_episode_reward"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDQTdpYv9xJN",
        "outputId": "ba860b06-bebc-49a3-af82-4c82d314255f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Random Agent, before training using property\n",
        "# from stable_baselines.common.evaluation import evaluate_policy\n",
        "max_r, mean_r, min_r = evaluate(agent_model, eval_env, num_episodes=100)\n",
        "\n",
        "# print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max reward: 0.7611111111111111 Mean reward: 0.007611111111111111 Min reward: 0.0 Num episodes: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pjAP8NWNu_6"
      },
      "source": [
        "### Entrenar Agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koHGB-VN-81O",
        "outputId": "9d903f78-4dd2-4b5c-ce0f-99e5ff9f7eb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "agent_model.learn(total_timesteps=2000000, log_interval=10)\n",
        "train_env.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------\n",
            "| approxkl           | 0.0038542286  |\n",
            "| clipfrac           | 0.062402345   |\n",
            "| explained_variance | -0.1          |\n",
            "| fps                | 907           |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 1.9420274     |\n",
            "| policy_loss        | -0.0063044047 |\n",
            "| serial_timesteps   | 512           |\n",
            "| time_elapsed       | 2.1e-05       |\n",
            "| total_timesteps    | 4096          |\n",
            "| value_loss         | 0.20859082    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.003936189  |\n",
            "| clipfrac           | 0.035668947  |\n",
            "| explained_variance | 0.0654       |\n",
            "| fps                | 922          |\n",
            "| n_updates          | 10           |\n",
            "| policy_entropy     | 1.7578052    |\n",
            "| policy_loss        | -0.003921165 |\n",
            "| serial_timesteps   | 5120         |\n",
            "| time_elapsed       | 37.9         |\n",
            "| total_timesteps    | 40960        |\n",
            "| value_loss         | 0.6553676    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.009148559   |\n",
            "| clipfrac           | 0.113378905   |\n",
            "| explained_variance | 0.267         |\n",
            "| fps                | 954           |\n",
            "| n_updates          | 20            |\n",
            "| policy_entropy     | 1.5939639     |\n",
            "| policy_loss        | -0.0077718855 |\n",
            "| serial_timesteps   | 10240         |\n",
            "| time_elapsed       | 80.9          |\n",
            "| total_timesteps    | 81920         |\n",
            "| value_loss         | 0.23940189    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.006178775   |\n",
            "| clipfrac           | 0.059106447   |\n",
            "| explained_variance | 0.321         |\n",
            "| fps                | 959           |\n",
            "| n_updates          | 30            |\n",
            "| policy_entropy     | 1.4125224     |\n",
            "| policy_loss        | -0.0052092923 |\n",
            "| serial_timesteps   | 15360         |\n",
            "| time_elapsed       | 124           |\n",
            "| total_timesteps    | 122880        |\n",
            "| value_loss         | 0.47101197    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.003331021   |\n",
            "| clipfrac           | 0.045703124   |\n",
            "| explained_variance | 0.558         |\n",
            "| fps                | 957           |\n",
            "| n_updates          | 40            |\n",
            "| policy_entropy     | 1.044483      |\n",
            "| policy_loss        | -0.0029121293 |\n",
            "| serial_timesteps   | 20480         |\n",
            "| time_elapsed       | 166           |\n",
            "| total_timesteps    | 163840        |\n",
            "| value_loss         | 1.6008629     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.004157595  |\n",
            "| clipfrac           | 0.037475586  |\n",
            "| explained_variance | 0.233        |\n",
            "| fps                | 922          |\n",
            "| n_updates          | 50           |\n",
            "| policy_entropy     | 0.5943379    |\n",
            "| policy_loss        | -0.005218803 |\n",
            "| serial_timesteps   | 25600        |\n",
            "| time_elapsed       | 209          |\n",
            "| total_timesteps    | 204800       |\n",
            "| value_loss         | 4.7095118    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0035413573  |\n",
            "| clipfrac           | 0.04030762    |\n",
            "| explained_variance | 0.437         |\n",
            "| fps                | 965           |\n",
            "| n_updates          | 60            |\n",
            "| policy_entropy     | 1.2739758     |\n",
            "| policy_loss        | -0.0040964847 |\n",
            "| serial_timesteps   | 30720         |\n",
            "| time_elapsed       | 253           |\n",
            "| total_timesteps    | 245760        |\n",
            "| value_loss         | 1.5878133     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0041741743 |\n",
            "| clipfrac           | 0.059179686  |\n",
            "| explained_variance | 0.683        |\n",
            "| fps                | 948          |\n",
            "| n_updates          | 70           |\n",
            "| policy_entropy     | 0.8493555    |\n",
            "| policy_loss        | -0.005259345 |\n",
            "| serial_timesteps   | 35840        |\n",
            "| time_elapsed       | 296          |\n",
            "| total_timesteps    | 286720       |\n",
            "| value_loss         | 0.5420242    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010813225  |\n",
            "| clipfrac           | 0.08618164   |\n",
            "| explained_variance | 0.693        |\n",
            "| fps                | 967          |\n",
            "| n_updates          | 80           |\n",
            "| policy_entropy     | 0.67164755   |\n",
            "| policy_loss        | -0.004247065 |\n",
            "| serial_timesteps   | 40960        |\n",
            "| time_elapsed       | 338          |\n",
            "| total_timesteps    | 327680       |\n",
            "| value_loss         | 0.1465388    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0045559835 |\n",
            "| clipfrac           | 0.05698242   |\n",
            "| explained_variance | 0.519        |\n",
            "| fps                | 941          |\n",
            "| n_updates          | 90           |\n",
            "| policy_entropy     | 0.8847555    |\n",
            "| policy_loss        | -0.006682577 |\n",
            "| serial_timesteps   | 46080        |\n",
            "| time_elapsed       | 381          |\n",
            "| total_timesteps    | 368640       |\n",
            "| value_loss         | 0.5126723    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0060040904  |\n",
            "| clipfrac           | 0.07348633    |\n",
            "| explained_variance | 0.587         |\n",
            "| fps                | 983           |\n",
            "| n_updates          | 100           |\n",
            "| policy_entropy     | 0.9087679     |\n",
            "| policy_loss        | -0.0066802003 |\n",
            "| serial_timesteps   | 51200         |\n",
            "| time_elapsed       | 424           |\n",
            "| total_timesteps    | 409600        |\n",
            "| value_loss         | 0.13908266    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005572412   |\n",
            "| clipfrac           | 0.06462403    |\n",
            "| explained_variance | 0.778         |\n",
            "| fps                | 933           |\n",
            "| n_updates          | 110           |\n",
            "| policy_entropy     | 0.68169737    |\n",
            "| policy_loss        | -0.0025262237 |\n",
            "| serial_timesteps   | 56320         |\n",
            "| time_elapsed       | 467           |\n",
            "| total_timesteps    | 450560        |\n",
            "| value_loss         | 0.039594833   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005940727   |\n",
            "| clipfrac           | 0.080004886   |\n",
            "| explained_variance | 0.664         |\n",
            "| fps                | 924           |\n",
            "| n_updates          | 120           |\n",
            "| policy_entropy     | 0.83894587    |\n",
            "| policy_loss        | -0.0034868158 |\n",
            "| serial_timesteps   | 61440         |\n",
            "| time_elapsed       | 510           |\n",
            "| total_timesteps    | 491520        |\n",
            "| value_loss         | 0.06381126    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0042968635  |\n",
            "| clipfrac           | 0.040014647   |\n",
            "| explained_variance | 0.677         |\n",
            "| fps                | 950           |\n",
            "| n_updates          | 130           |\n",
            "| policy_entropy     | 0.7599371     |\n",
            "| policy_loss        | -0.0028306975 |\n",
            "| serial_timesteps   | 66560         |\n",
            "| time_elapsed       | 554           |\n",
            "| total_timesteps    | 532480        |\n",
            "| value_loss         | 0.23659582    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.01493713   |\n",
            "| clipfrac           | 0.0720459    |\n",
            "| explained_variance | 0.207        |\n",
            "| fps                | 946          |\n",
            "| n_updates          | 140          |\n",
            "| policy_entropy     | 0.5149934    |\n",
            "| policy_loss        | -0.012385283 |\n",
            "| serial_timesteps   | 71680        |\n",
            "| time_elapsed       | 598          |\n",
            "| total_timesteps    | 573440       |\n",
            "| value_loss         | 0.16515587   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002150643   |\n",
            "| clipfrac           | 0.028955078   |\n",
            "| explained_variance | 0.606         |\n",
            "| fps                | 948           |\n",
            "| n_updates          | 150           |\n",
            "| policy_entropy     | 0.43141407    |\n",
            "| policy_loss        | -0.0028547647 |\n",
            "| serial_timesteps   | 76800         |\n",
            "| time_elapsed       | 641           |\n",
            "| total_timesteps    | 614400        |\n",
            "| value_loss         | 0.20133242    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020470044  |\n",
            "| clipfrac           | 0.028979491   |\n",
            "| explained_variance | 0.776         |\n",
            "| fps                | 954           |\n",
            "| n_updates          | 160           |\n",
            "| policy_entropy     | 0.36922365    |\n",
            "| policy_loss        | -0.0012004336 |\n",
            "| serial_timesteps   | 81920         |\n",
            "| time_elapsed       | 684           |\n",
            "| total_timesteps    | 655360        |\n",
            "| value_loss         | 0.17886223    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.004122478   |\n",
            "| clipfrac           | 0.041503906   |\n",
            "| explained_variance | 0.815         |\n",
            "| fps                | 932           |\n",
            "| n_updates          | 170           |\n",
            "| policy_entropy     | 0.33476806    |\n",
            "| policy_loss        | -0.0021843922 |\n",
            "| serial_timesteps   | 87040         |\n",
            "| time_elapsed       | 726           |\n",
            "| total_timesteps    | 696320        |\n",
            "| value_loss         | 0.035206873   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.011107905   |\n",
            "| clipfrac           | 0.117529295   |\n",
            "| explained_variance | 0.703         |\n",
            "| fps                | 937           |\n",
            "| n_updates          | 180           |\n",
            "| policy_entropy     | 0.61866254    |\n",
            "| policy_loss        | -0.0029238954 |\n",
            "| serial_timesteps   | 92160         |\n",
            "| time_elapsed       | 769           |\n",
            "| total_timesteps    | 737280        |\n",
            "| value_loss         | 0.07296918    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.005405198  |\n",
            "| clipfrac           | 0.053222656  |\n",
            "| explained_variance | 0.704        |\n",
            "| fps                | 915          |\n",
            "| n_updates          | 190          |\n",
            "| policy_entropy     | 0.70742273   |\n",
            "| policy_loss        | -0.005292969 |\n",
            "| serial_timesteps   | 97280        |\n",
            "| time_elapsed       | 813          |\n",
            "| total_timesteps    | 778240       |\n",
            "| value_loss         | 0.3174955    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.008984806 |\n",
            "| clipfrac           | 0.085913084 |\n",
            "| explained_variance | 0.497       |\n",
            "| fps                | 895         |\n",
            "| n_updates          | 200         |\n",
            "| policy_entropy     | 0.72925645  |\n",
            "| policy_loss        | -0.00435385 |\n",
            "| serial_timesteps   | 102400      |\n",
            "| time_elapsed       | 857         |\n",
            "| total_timesteps    | 819200      |\n",
            "| value_loss         | 0.086739525 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0041985726  |\n",
            "| clipfrac           | 0.038745116   |\n",
            "| explained_variance | 0.668         |\n",
            "| fps                | 917           |\n",
            "| n_updates          | 210           |\n",
            "| policy_entropy     | 0.37620357    |\n",
            "| policy_loss        | -0.0031640572 |\n",
            "| serial_timesteps   | 107520        |\n",
            "| time_elapsed       | 900           |\n",
            "| total_timesteps    | 860160        |\n",
            "| value_loss         | 0.05571404    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0042237956  |\n",
            "| clipfrac           | 0.04650879    |\n",
            "| explained_variance | 0.515         |\n",
            "| fps                | 969           |\n",
            "| n_updates          | 220           |\n",
            "| policy_entropy     | 0.3514241     |\n",
            "| policy_loss        | -0.0016821111 |\n",
            "| serial_timesteps   | 112640        |\n",
            "| time_elapsed       | 944           |\n",
            "| total_timesteps    | 901120        |\n",
            "| value_loss         | 0.08127447    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008970035   |\n",
            "| clipfrac           | 0.05180664    |\n",
            "| explained_variance | 0.0751        |\n",
            "| fps                | 951           |\n",
            "| n_updates          | 230           |\n",
            "| policy_entropy     | 0.30037883    |\n",
            "| policy_loss        | -0.0075150393 |\n",
            "| serial_timesteps   | 117760        |\n",
            "| time_elapsed       | 987           |\n",
            "| total_timesteps    | 942080        |\n",
            "| value_loss         | 0.006569264   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.010005359  |\n",
            "| clipfrac           | 0.09819336   |\n",
            "| explained_variance | 0.563        |\n",
            "| fps                | 961          |\n",
            "| n_updates          | 240          |\n",
            "| policy_entropy     | 0.4904037    |\n",
            "| policy_loss        | -0.006081513 |\n",
            "| serial_timesteps   | 122880       |\n",
            "| time_elapsed       | 1.03e+03     |\n",
            "| total_timesteps    | 983040       |\n",
            "| value_loss         | 0.004051942  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.008832921   |\n",
            "| clipfrac           | 0.08696289    |\n",
            "| explained_variance | 0.44          |\n",
            "| fps                | 958           |\n",
            "| n_updates          | 250           |\n",
            "| policy_entropy     | 0.6775175     |\n",
            "| policy_loss        | -0.0045065815 |\n",
            "| serial_timesteps   | 128000        |\n",
            "| time_elapsed       | 1.07e+03      |\n",
            "| total_timesteps    | 1024000       |\n",
            "| value_loss         | 0.15799502    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00437107    |\n",
            "| clipfrac           | 0.0421875     |\n",
            "| explained_variance | 0.442         |\n",
            "| fps                | 984           |\n",
            "| n_updates          | 260           |\n",
            "| policy_entropy     | 0.779139      |\n",
            "| policy_loss        | -0.0013245887 |\n",
            "| serial_timesteps   | 133120        |\n",
            "| time_elapsed       | 1.12e+03      |\n",
            "| total_timesteps    | 1064960       |\n",
            "| value_loss         | 0.15823148    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.003252192    |\n",
            "| clipfrac           | 0.04230957     |\n",
            "| explained_variance | 0.447          |\n",
            "| fps                | 965            |\n",
            "| n_updates          | 270            |\n",
            "| policy_entropy     | 0.46450242     |\n",
            "| policy_loss        | -0.00074173545 |\n",
            "| serial_timesteps   | 138240         |\n",
            "| time_elapsed       | 1.16e+03       |\n",
            "| total_timesteps    | 1105920        |\n",
            "| value_loss         | 0.16743574     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0055396105  |\n",
            "| clipfrac           | 0.02216797    |\n",
            "| explained_variance | 0.429         |\n",
            "| fps                | 995           |\n",
            "| n_updates          | 280           |\n",
            "| policy_entropy     | 0.39300027    |\n",
            "| policy_loss        | -0.0018037386 |\n",
            "| serial_timesteps   | 143360        |\n",
            "| time_elapsed       | 1.2e+03       |\n",
            "| total_timesteps    | 1146880       |\n",
            "| value_loss         | 0.2641668     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0023851814   |\n",
            "| clipfrac           | 0.021679688    |\n",
            "| explained_variance | 0.435          |\n",
            "| fps                | 949            |\n",
            "| n_updates          | 290            |\n",
            "| policy_entropy     | 0.36497992     |\n",
            "| policy_loss        | -0.00085982215 |\n",
            "| serial_timesteps   | 148480         |\n",
            "| time_elapsed       | 1.24e+03       |\n",
            "| total_timesteps    | 1187840        |\n",
            "| value_loss         | 0.34785172     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005477259   |\n",
            "| clipfrac           | 0.05053711    |\n",
            "| explained_variance | 0.102         |\n",
            "| fps                | 946           |\n",
            "| n_updates          | 300           |\n",
            "| policy_entropy     | 0.30998826    |\n",
            "| policy_loss        | -0.0034383473 |\n",
            "| serial_timesteps   | 153600        |\n",
            "| time_elapsed       | 1.29e+03      |\n",
            "| total_timesteps    | 1228800       |\n",
            "| value_loss         | 0.00421614    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005826816   |\n",
            "| clipfrac           | 0.05593262    |\n",
            "| explained_variance | 0.37          |\n",
            "| fps                | 964           |\n",
            "| n_updates          | 310           |\n",
            "| policy_entropy     | 0.5109172     |\n",
            "| policy_loss        | -0.0014137656 |\n",
            "| serial_timesteps   | 158720        |\n",
            "| time_elapsed       | 1.33e+03      |\n",
            "| total_timesteps    | 1269760       |\n",
            "| value_loss         | 4.9660106e-05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.009059049   |\n",
            "| clipfrac           | 0.08078613    |\n",
            "| explained_variance | 0.42          |\n",
            "| fps                | 992           |\n",
            "| n_updates          | 320           |\n",
            "| policy_entropy     | 0.4285643     |\n",
            "| policy_loss        | -0.0020725627 |\n",
            "| serial_timesteps   | 163840        |\n",
            "| time_elapsed       | 1.37e+03      |\n",
            "| total_timesteps    | 1310720       |\n",
            "| value_loss         | 8.072256e-05  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.057341903   |\n",
            "| clipfrac           | 0.10769043    |\n",
            "| explained_variance | 0.425         |\n",
            "| fps                | 919           |\n",
            "| n_updates          | 330           |\n",
            "| policy_entropy     | 0.40145797    |\n",
            "| policy_loss        | -0.02115181   |\n",
            "| serial_timesteps   | 168960        |\n",
            "| time_elapsed       | 1.41e+03      |\n",
            "| total_timesteps    | 1351680       |\n",
            "| value_loss         | 0.00010346095 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017005547  |\n",
            "| clipfrac           | 0.01689453    |\n",
            "| explained_variance | -2.43         |\n",
            "| fps                | 969           |\n",
            "| n_updates          | 340           |\n",
            "| policy_entropy     | 0.4043489     |\n",
            "| policy_loss        | -0.0028141136 |\n",
            "| serial_timesteps   | 174080        |\n",
            "| time_elapsed       | 1.46e+03      |\n",
            "| total_timesteps    | 1392640       |\n",
            "| value_loss         | 6.32675e-06   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0067065037  |\n",
            "| clipfrac           | 0.03273926    |\n",
            "| explained_variance | -0.339        |\n",
            "| fps                | 919           |\n",
            "| n_updates          | 350           |\n",
            "| policy_entropy     | 0.43454018    |\n",
            "| policy_loss        | -0.0022481233 |\n",
            "| serial_timesteps   | 179200        |\n",
            "| time_elapsed       | 1.5e+03       |\n",
            "| total_timesteps    | 1433600       |\n",
            "| value_loss         | 1.0437058e-05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002919239   |\n",
            "| clipfrac           | 0.020092774   |\n",
            "| explained_variance | 0.355         |\n",
            "| fps                | 958           |\n",
            "| n_updates          | 360           |\n",
            "| policy_entropy     | 0.12170271    |\n",
            "| policy_loss        | -0.002627722  |\n",
            "| serial_timesteps   | 184320        |\n",
            "| time_elapsed       | 1.54e+03      |\n",
            "| total_timesteps    | 1474560       |\n",
            "| value_loss         | 8.2759805e-05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.005774608   |\n",
            "| clipfrac           | 0.07460938    |\n",
            "| explained_variance | -5.31         |\n",
            "| fps                | 954           |\n",
            "| n_updates          | 370           |\n",
            "| policy_entropy     | 0.41064423    |\n",
            "| policy_loss        | -0.005573852  |\n",
            "| serial_timesteps   | 189440        |\n",
            "| time_elapsed       | 1.58e+03      |\n",
            "| total_timesteps    | 1515520       |\n",
            "| value_loss         | 1.3809991e-05 |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0030182898 |\n",
            "| clipfrac           | 0.013256836  |\n",
            "| explained_variance | -2.19        |\n",
            "| fps                | 928          |\n",
            "| n_updates          | 380          |\n",
            "| policy_entropy     | 0.071932115  |\n",
            "| policy_loss        | -0.00420472  |\n",
            "| serial_timesteps   | 194560       |\n",
            "| time_elapsed       | 1.63e+03     |\n",
            "| total_timesteps    | 1556480      |\n",
            "| value_loss         | 5.061882e-05 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.007987128   |\n",
            "| clipfrac           | 0.07102051    |\n",
            "| explained_variance | -2.95         |\n",
            "| fps                | 968           |\n",
            "| n_updates          | 390           |\n",
            "| policy_entropy     | 0.36984923    |\n",
            "| policy_loss        | -0.0052672448 |\n",
            "| serial_timesteps   | 199680        |\n",
            "| time_elapsed       | 1.67e+03      |\n",
            "| total_timesteps    | 1597440       |\n",
            "| value_loss         | 9.804578e-05  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.02591851    |\n",
            "| clipfrac           | 0.14504394    |\n",
            "| explained_variance | 0.389         |\n",
            "| fps                | 979           |\n",
            "| n_updates          | 400           |\n",
            "| policy_entropy     | 0.40302044    |\n",
            "| policy_loss        | -0.0074488698 |\n",
            "| serial_timesteps   | 204800        |\n",
            "| time_elapsed       | 1.72e+03      |\n",
            "| total_timesteps    | 1638400       |\n",
            "| value_loss         | 7.1945016e-07 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0034058634  |\n",
            "| clipfrac           | 0.039379884   |\n",
            "| explained_variance | 0.548         |\n",
            "| fps                | 905           |\n",
            "| n_updates          | 410           |\n",
            "| policy_entropy     | 0.27639017    |\n",
            "| policy_loss        | -0.002724376  |\n",
            "| serial_timesteps   | 209920        |\n",
            "| time_elapsed       | 1.76e+03      |\n",
            "| total_timesteps    | 1679360       |\n",
            "| value_loss         | 1.6019181e-05 |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.008022394    |\n",
            "| clipfrac           | 0.08505859     |\n",
            "| explained_variance | -0.467         |\n",
            "| fps                | 950            |\n",
            "| n_updates          | 420            |\n",
            "| policy_entropy     | 0.76296264     |\n",
            "| policy_loss        | -0.00057965505 |\n",
            "| serial_timesteps   | 215040         |\n",
            "| time_elapsed       | 1.8e+03        |\n",
            "| total_timesteps    | 1720320        |\n",
            "| value_loss         | 1.701289e-05   |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.011846899  |\n",
            "| clipfrac           | 0.062329102  |\n",
            "| explained_variance | -1.7         |\n",
            "| fps                | 953          |\n",
            "| n_updates          | 430          |\n",
            "| policy_entropy     | 0.6774312    |\n",
            "| policy_loss        | 0.0070704846 |\n",
            "| serial_timesteps   | 220160       |\n",
            "| time_elapsed       | 1.85e+03     |\n",
            "| total_timesteps    | 1761280      |\n",
            "| value_loss         | 8.033438e-06 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.005182723  |\n",
            "| clipfrac           | 0.03720703   |\n",
            "| explained_variance | -0.282       |\n",
            "| fps                | 938          |\n",
            "| n_updates          | 440          |\n",
            "| policy_entropy     | 0.38503242   |\n",
            "| policy_loss        | 0.0016590379 |\n",
            "| serial_timesteps   | 225280       |\n",
            "| time_elapsed       | 1.89e+03     |\n",
            "| total_timesteps    | 1802240      |\n",
            "| value_loss         | 0.0013628998 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0045988234  |\n",
            "| clipfrac           | 0.029541016   |\n",
            "| explained_variance | 0.76          |\n",
            "| fps                | 980           |\n",
            "| n_updates          | 450           |\n",
            "| policy_entropy     | 0.27681413    |\n",
            "| policy_loss        | -0.0013586401 |\n",
            "| serial_timesteps   | 230400        |\n",
            "| time_elapsed       | 1.93e+03      |\n",
            "| total_timesteps    | 1843200       |\n",
            "| value_loss         | 0.0009806192  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0043893047  |\n",
            "| clipfrac           | 0.04133301    |\n",
            "| explained_variance | 0.697         |\n",
            "| fps                | 931           |\n",
            "| n_updates          | 460           |\n",
            "| policy_entropy     | 0.3567335     |\n",
            "| policy_loss        | 0.00046673493 |\n",
            "| serial_timesteps   | 235520        |\n",
            "| time_elapsed       | 1.98e+03      |\n",
            "| total_timesteps    | 1884160       |\n",
            "| value_loss         | 0.11778084    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.008915579  |\n",
            "| clipfrac           | 0.062719725  |\n",
            "| explained_variance | 0.624        |\n",
            "| fps                | 924          |\n",
            "| n_updates          | 470          |\n",
            "| policy_entropy     | 0.3085479    |\n",
            "| policy_loss        | 0.021819307  |\n",
            "| serial_timesteps   | 240640       |\n",
            "| time_elapsed       | 2.02e+03     |\n",
            "| total_timesteps    | 1925120      |\n",
            "| value_loss         | 0.0029791677 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.006089666   |\n",
            "| clipfrac           | 0.04963379    |\n",
            "| explained_variance | 0.714         |\n",
            "| fps                | 913           |\n",
            "| n_updates          | 480           |\n",
            "| policy_entropy     | 0.3271479     |\n",
            "| policy_loss        | -0.0011484807 |\n",
            "| serial_timesteps   | 245760        |\n",
            "| time_elapsed       | 2.06e+03      |\n",
            "| total_timesteps    | 1966080       |\n",
            "| value_loss         | 0.07554729    |\n",
            "--------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcsEVysINz9j"
      },
      "source": [
        "### Evaluar Agente Entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5chx6sxfN76",
        "outputId": "e347e2d6-7c2f-4b45-d2c8-43eacdfaa2b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Trained Agent, after training using property\n",
        "# from stable_baselines.common.evaluation import evaluate_policy\n",
        "max_r, mean_r, min_r = evaluate(agent_model, eval_env, num_episodes=100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max reward: 0 Mean reward: 0.0 Min reward: 0 Num episodes: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP9AbAmzqDYP"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create save dir\n",
        "save_dir = \"/tmp/gym/\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLpmSgmeN6PX"
      },
      "source": [
        "### Extras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIe43E_dSBRy"
      },
      "source": [
        "import gym_minigrid\n",
        "\n",
        "env = gym.make('MiniGrid-SimpleCrossingEnvUmaze-v0')\n",
        "for i_episode in range(3):\n",
        "    obs = env.reset()\n",
        "    for t_steps in range(100):\n",
        "        # env.render()\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        print(obs)\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t_steps+1))\n",
        "            break\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df08vXY_MtHg"
      },
      "source": [
        "!apt update && apt install xvfb\n",
        "!pip install gym-notebook-wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4A_IOZ_9fzW"
      },
      "source": [
        "# Random Agent, before training using function\n",
        "mean_reward_before_train = evaluate(agent_model, eval_env, num_episodes=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgXOU1gvsHyT"
      },
      "source": [
        "!pip install tensorboard==2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuCkk1M-kPGP"
      },
      "source": [
        "# Trained Agent\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(agent_model, train_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA5AjICfkVZl"
      },
      "source": [
        "!tensorboard --logdir ./ppo2_minigriddoorkey/ --host localhost "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}